[{"path":"https://erincr.github.io/ptLasso/articles/ConditionalAverageTreatmentEffect.html","id":"background-cate-estimation-and-pretraining","dir":"Articles","previous_headings":"","what":"Background: CATE estimation and pretraining","title":"Conditional average treatment effect estimation","text":"causal inference, often interested predicting treatment effect individual observations; called conditional average treatment effect (CATE). example, prescribing drug patient, want know whether drug likely work well patient - just whether works well average. One tool model CATE R-learner (Nie Wager (2021)), minimizes R loss: $$ \\hat{L}_n\\{\\tau(\\cdot)\\}=\\arg \\min_\\tau \\frac{1}{n}\\sum\\Bigl[ (y_i- m^*(x_i)) - (W_i-e^*(x_i))\\tau(x_i) \\Bigr]^2. $$ , xix_i yiy_i covariates outcome observation ii, e*(xi)e^*(x_i) treatment propensity WiW_i treatment assignment, m*(xi)m^*(x_i) conditional mean outcome (E[yi∣x=xi]E[y_i \\mid x = x_i]). , τ̂\\hat\\tau estimate heterogeneous treatment effect function. fitted stages: first, R-learner fits m*m^* e*e^* get m̂*\\hat{m}^* ê*\\hat{e}^*; plugs m̂*(xi)\\hat{m}^*(x_i) ê*(xi)\\hat{e}^*(x_i) fit τ\\tau. minor detail cross-fitting (prevalidation) used first stage plugin value e.g. m̂*(xi)\\hat{m}^*(x_i) comes model trained without using xix_i. τ\\tau linear function, second stage fitting straightforward. values m̂*(xi)\\hat{m}^*(x_i) ê*(xi)\\hat{e}^*(x_i) known, can use linear regression model yi−m̂*(xi)y_i - \\hat{m}^*(x_i) function weighted feature vector (Wi−ê*(xi))xi(W_i-\\hat{e}^*(x_i)) x_i. following example. can pretraining useful ? Well, separately fitting models m*m^* (conditional mean) τ\\tau (heterogeneous treatment effect), two functions likely share support: sensible assume features modulate mean treatment effect also modulate heterogeneous treatment effect. can use pretraining (1) training model m*m^* (2) using support model guide fitting τ\\tau. Note offset used case; m*m^* τ\\tau designed predict different outcomes.","code":""},{"path":"https://erincr.github.io/ptLasso/articles/ConditionalAverageTreatmentEffect.html","id":"a-simulated-example","dir":"Articles","previous_headings":"","what":"A simulated example","title":"Conditional average treatment effect estimation","text":"example. simplify problem assuming treatment randomized – true e*(xi)=0.5e^*(x_i) = 0.5 ii. begin model fitting, starting estimate e*e^* (probability receiving treatment). fit τ\\tau, also need record cross-fitted ê*(x)\\hat{e}^*(x). Now, stage 1 pretraining: fit model m*m^* record support. , also record cross-fitted m̂*(x)\\hat{m}^*(x). fit τ\\tau, regress ỹ=yi−m̂*(xi)\\tilde{y} = y_i - \\hat{m}^*(x_i) x̃=(wi−ê*(xi))xi\\tilde{x} = (w_i - \\hat{e}^*(x_i)) x_i; ’ll define : now, pretraining τ\\tau. Loop α=0,0.1,…,1\\alpha = 0, 0.1, \\dots, 1; α\\alpha, fit model τ\\tau using penalty factor defined support m̂\\hat{m} α\\alpha. ’ll keep track CV MSE step can choose α\\alpha minimizes MSE.  plot , value α=1\\alpha = 1 corresponds usual R learner, makes assumption shared support τ\\tau m*m^*. Based plot, choose α=0.2\\alpha = 0.2 best performing model: concretely compare pretrained R-learner usual R-learner, ’ll train usual R-learner : anticipated, pretraining improves prediction squared error relative R learner – designed simulation:","code":"set.seed(1234)  n = 600; ntrain = 300 p = 20       x = matrix(rnorm(n*p), n, p)  # Treatment assignment w = rbinom(n, 1, 0.5)  # m^* m.coefs = c(rep(2,10), rep(0, p-10)) m = x %*% m.coefs  # tau tau.coefs = runif(p, 0.5, 1)*m.coefs  tau = 1.5*m + x%*%tau.coefs  mu = m + w * tau y  = mu + 10 * rnorm(n) cat(\"Signal to noise ratio:\", var(mu)/var(y-mu)) #> Signal to noise ratio: 2.301315  # Split into train/test xtest = x[-(1:ntrain), ] tautest = tau[-(1:ntrain)]  wtest = w[-(1:ntrain)]  x = x[1:ntrain, ] y = y[1:ntrain]  w = w[1:ntrain]  # Define training folds nfolds = 10 foldid = sample(rep(1:10, trunc(nrow(x)/nfolds)+1))[1:nrow(x)] e_fit = cv.glmnet(x, w, foldid = foldid,                   family=\"binomial\", type.measure=\"deviance\",                   keep = TRUE)  e_hat = e_fit$fit.preval[, e_fit$lambda == e_fit$lambda.1se] e_hat = 1/(1 + exp(-e_hat)) m_fit = cv.glmnet(x, y, foldid = foldid, keep = TRUE)  m_hat = m_fit$fit.preval[, m_fit$lambda == m_fit$lambda.1se]  bhat = coef(m_fit, s = m_fit$lambda.1se) support = which(bhat[-1] != 0) y_tilde = y - m_hat x_tilde = cbind(as.numeric(w - e_hat) * cbind(1, x)) cv.error = NULL alphalist = seq(0, 1, length.out = 11)  for(alpha in alphalist){   pf = rep(1/alpha, p)   pf[support] = 1   pf = c(0, pf) # Don't penalize the intercept      tau_fit = cv.glmnet(x_tilde, y_tilde,                        foldid = foldid,                       penalty.factor = pf,                       intercept = FALSE, # already include in x_tilde                       standardize = FALSE)   cv.error = c(cv.error, min(tau_fit$cvm)) }   plot(alphalist, cv.error, type = \"b\",      xlab = expression(alpha),       ylab = \"CV MSE\",       main = bquote(\"CV mean squared error as a function of \" ~ alpha)) abline(v = alphalist[which.min(cv.error)]) best.alpha = alphalist[which.min(cv.error)] cat(\"Chosen alpha:\", best.alpha) #> Chosen alpha: 0.2  pf = rep(1/best.alpha, p) pf[support] = 1 pf = c(0, pf) tau_fit = cv.glmnet(x_tilde, y_tilde, foldid = foldid,                     penalty.factor = pf,                     intercept = FALSE,                     standardize = FALSE) tau_rlearner = cv.glmnet(x_tilde, y_tilde, foldid = foldid,                           penalty.factor = c(0, rep(1, ncol(x))),                          intercept = FALSE,                          standardize = FALSE) rlearner_preds   = predict(tau_rlearner, cbind(1, xtest), s = \"lambda.min\") cat(\"R-learner PSE: \",      round(mean((rlearner_preds - tautest)^2), 2)) #> R-learner PSE:  45.85  pretrained_preds = predict(tau_fit, cbind(1, xtest), s = \"lambda.min\") cat(\"Pretrained R-learner PSE: \",      round(mean((pretrained_preds - tautest)^2), 2)) #> Pretrained R-learner PSE:  37.63"},{"path":"https://erincr.github.io/ptLasso/articles/ConditionalAverageTreatmentEffect.html","id":"what-if-the-pretraining-assumption-is-wrong","dir":"Articles","previous_headings":"","what":"What if the pretraining assumption is wrong?","title":"Conditional average treatment effect estimation","text":", repeat everything , now overlap support m*m^* τ\\tau. Pretraining hurt performance, even though support m*m^* τ\\tau shared. ? Recall defined y=m*(x)+W*τ(x)+ϵy =  m^*(x) + W * \\tau(x) + \\epsilon, relationship yy xx function supports m*m^* τ\\tau. first stage pretraining, fitted m*m^* using y ~ x – support m*m^*include support τ\\tau. result, using pretraining R-learner harm predictive performance.","code":"###################################################### # Simulate data ###################################################### x = matrix(rnorm(n*p), n, p)  # Treatment assignment w = rbinom(n, 1, 0.5)  # m^* m.coefs = c(rep(2,10), rep(0, p-10)) m = x %*% m.coefs  # tau # Note these coefficients have no overlap with m.coefs! tau.coefs = c(rep(0, 10), rep(2, 10), rep(0, p-20)) tau = x%*%tau.coefs  mu = m + w * tau y  = mu + 10 * rnorm(n) cat(\"Signal to noise ratio:\", var(mu)/var(y-mu)) #> Signal to noise ratio: 0.6938152  # Split into train/test xtest = x[-(1:ntrain), ] tautest = tau[-(1:ntrain)]  wtest = w[-(1:ntrain)]  x = x[1:ntrain, ] y = y[1:ntrain]  w = w[1:ntrain]  ###################################################### # Model fitting: e^* ###################################################### e_fit = cv.glmnet(x, w, foldid = foldid,                   family=\"binomial\", type.measure=\"deviance\",                   keep = TRUE) e_hat = e_fit$fit.preval[, e_fit$lambda == e_fit$lambda.1se] e_hat = 1/(1 + exp(-e_hat))  ###################################################### # Model fitting: m^* ###################################################### m_fit = cv.glmnet(x, y, foldid = foldid, keep = TRUE)  m_hat = m_fit$fit.preval[, m_fit$lambda == m_fit$lambda.1se]  bhat = coef(m_fit, s = m_fit$lambda.1se) support = which(bhat[-1] != 0)  ###################################################### # Pretraining: tau ###################################################### y_tilde = y - m_hat x_tilde = cbind(as.numeric(w - e_hat) * cbind(1, x))  cv.error = NULL alphalist = seq(0, 1, length.out = 11)  for(alpha in alphalist){   pf = rep(1/alpha, p)   pf[support] = 1   pf = c(0, pf) # Don't penalize the intercept      tau_fit = cv.glmnet(x_tilde, y_tilde,                        foldid = foldid,                       penalty.factor = pf,                       intercept = FALSE, # already include in x_tilde                       standardize = FALSE)   cv.error = c(cv.error, min(tau_fit$cvm)) }  # Our final model for tau: best.alpha = alphalist[which.min(cv.error)] cat(\"Chosen alpha:\", best.alpha) #> Chosen alpha: 1  pf = rep(1/best.alpha, p) pf[support] = 1 pf = c(0, pf) tau_fit = cv.glmnet(x_tilde, y_tilde, foldid = foldid,                     penalty.factor = pf,                     intercept = FALSE,                     standardize = FALSE)  ###################################################### # Fit the usual R-learner: ###################################################### tau_rlearner = cv.glmnet(x_tilde, y_tilde, foldid = foldid,                           penalty.factor = c(0, rep(1, ncol(x))),                          intercept = FALSE,                          standardize = FALSE)  ###################################################### # Measure performance: ###################################################### rlearner_preds = predict(tau_rlearner, cbind(1, xtest), s = \"lambda.min\") cat(\"R-learner prediction squared error: \",      round(mean((rlearner_preds - tautest)^2), 2)) #> R-learner prediction squared error:  31.11  pretrained_preds = predict(tau_fit, cbind(1, xtest), s = \"lambda.min\") cat(\"Pretrained R-learner prediction squared error: \",      round(mean((pretrained_preds - tautest)^2), 2)) #> Pretrained R-learner prediction squared error:  31.11"},{"path":"https://erincr.github.io/ptLasso/articles/InputGroupedData.html","id":"base-case-input-grouped-data-with-a-binomial-outcome","dir":"Articles","previous_headings":"","what":"Base case: input grouped data with a binomial outcome","title":"Input grouped data","text":"Quick Start, applied ptLasso data continuous response. , ’ll use data binary outcome. creates dataset k=3k = 3 groups (100100 observations), 5 shared coefficients, 5 coefficients specific group. can fit predict . default, predict.ptLasso compute return deviance test set. instead compute AUC specifying type.measure call ptLasso. Note: type.measure specified model fitting prediction used call cv.glmnet. fit overall individual models, can use elasticnet instead lasso defining parameter en.alpha (glmnet described section “Fitting elasticnet ridge models”). Using cross validation Gaussian case:","code":"set.seed(1234)  out = binomial.example.data() x = out$x; y = out$y; groups = out$groups  outtest = binomial.example.data() xtest = outtest$x; ytest = outtest$y; groupstest = outtest$groups fit = ptLasso(x, y, groups, alpha = 0.5, family = \"binomial\")  predict(fit, xtest, groupstest, ytest = ytest) #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest)  #>  #>  #> alpha =  0.5  #>  #> Performance (Deviance): #>  #>            allGroups  mean wtdMean group_1 group_2 group_3 #> Overall        1.359 1.359   1.359   1.334   1.321   1.421 #> Pretrain       1.279 1.279   1.279   1.272   1.169   1.397 #> Individual     1.283 1.283   1.283   1.265   1.186   1.399 #>  #> Support size: #>                                         #> Overall    7                            #> Pretrain   12 (3 common + 9 individual) #> Individual 20 fit = ptLasso(x, y, groups, alpha = 0.5, family = \"binomial\",                type.measure = \"auc\")  predict(fit, xtest, groupstest, ytest = ytest) #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest)  #>  #>  #> alpha =  0.5  #>  #> Performance (AUC): #>  #>            allGroups   mean wtdMean group_1 group_2 group_3 #> Overall       0.6026 0.6039  0.6039  0.6161  0.6877  0.5080 #> Pretrain      0.6407 0.6524  0.6524  0.6936  0.7447  0.5190 #> Individual    0.6442 0.6618  0.6618  0.6936  0.7732  0.5186 #>  #> Support size: #>                                          #> Overall    15                            #> Pretrain   39 (3 common + 36 individual) #> Individual 40 fit = ptLasso(x, y, groups, alpha = 0.5, family = \"binomial\",                type.measure = \"auc\",                en.alpha = .5) predict(fit, xtest, groupstest, ytest = ytest) #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest)  #>  #>  #> alpha =  0.5  #>  #> Performance (AUC): #>  #>            allGroups   mean wtdMean group_1 group_2 group_3 #> Overall       0.6041 0.6018  0.6018  0.5928  0.6704  0.5422 #> Pretrain      0.6270 0.6547  0.6547  0.6781  0.7720  0.5141 #> Individual    0.6387 0.6598  0.6598  0.6756  0.7820  0.5218 #>  #> Support size: #>                                          #> Overall    3                             #> Pretrain   39 (3 common + 36 individual) #> Individual 36 ################################################## # Fit: ################################################## fit = cv.ptLasso(x, y, groups, family = \"binomial\", type.measure = \"auc\")  ################################################## # Predict with a common alpha for all groups: ################################################## predict(fit, xtest, groupstest, ytest = ytest)  ################################################## # Predict with a different alpha for each group: ################################################## predict(fit, xtest, groupstest, ytest = ytest, alphatype = \"varying\")"},{"path":"https://erincr.github.io/ptLasso/articles/InputGroupedData.html","id":"base-case-input-grouped-survival-data","dir":"Articles","previous_headings":"","what":"Base case: input grouped survival data","title":"Input grouped data","text":"Now, simulate survival times 3 groups; three groups overlapping support, 5 shared features 5 individual features. compute survival time, start computing survival=Xβ+ϵ\\text{survival} = X \\beta + \\epsilon, β\\beta specific group ϵ\\epsilon noise. survival times must positive, modify survival=survival+1.1*abs(min(survival))\\text{survival} = \\text{survival} + 1.1 * \\text{abs}(\\text{min}(\\text{survival})). Training ptLasso much continuous binomial cases; difference specify family = \"cox\". default, ptLasso uses partial likelihood model selection. instead use C index. call cv.ptLasso much ; need specify family (“cox”) type.measure (want use C index instead partial likelihood).","code":"require(survival) #> Loading required package: survival set.seed(1234)  n = 600; ntrain = 300 p = 50       x = matrix(rnorm(n*p), n, p) beta1 = c(rnorm(5), rep(0, p-5))  beta2 = runif(p) * beta1 # Shared support beta2 = beta2 + c(rep(0, 5), rnorm(5), rep(0, p-10)) # Individual features  beta3 = runif(p) * beta1 # Shared support beta3 = beta3 + c(rep(0, 10), rnorm(5), rep(0, p-15)) # Individual features  # Randomly split into groups groups = sample(1:3, n, replace = TRUE)  # Compute survival times: survival = x %*% beta1 survival[groups == 2] = x[groups == 2, ] %*% beta2 survival[groups == 3] = x[groups == 3, ] %*% beta3 survival = survival + rnorm(n) survival = survival + 1.1 * abs(min(survival))  # Censoring times from a random uniform distribution: censoring = runif(n, min = 1, max = 10)  # Did we observe surivival or censoring? y = Surv(pmin(survival, censoring), survival <= censoring)  # Split into train and test: xtest = x[-(1:300), ] ytest = y[-(1:300), ] groupstest = groups[-(1:300)]  x = x[1:300, ] y = y[1:300, ] groups = groups[1:300] ############################################################ # Default -- use partial likelihood as the type.measure: ############################################################ fit = ptLasso(x, y, groups, alpha = 0.5, family = \"cox\") predict(fit, xtest, groupstest, ytest = ytest) #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest)  #>  #>  #> alpha =  0.5  #>  #> Performance (Deviance): #>  #>            allGroups  mean wtdMean group_1 group_2 group_3 #> Overall        381.2 87.60   89.36   99.49  106.53   56.79 #> Pretrain       415.1 90.83   91.48  109.50   94.24   68.74 #> Individual     425.2 99.07   99.54  111.68  101.85   83.67 #>  #> Support size: #>                                          #> Overall    10                            #> Pretrain   19 (3 common + 16 individual) #> Individual 24  ############################################################ # Alternatively -- use the C index: ############################################################ fit = ptLasso(x, y, groups, alpha = 0.5, family = \"cox\", type.measure = \"C\") #> Warning: from glmnet C++ code (error code -30075); Numerical error at 75th #> lambda value; solutions for larger values of lambda returned predict(fit, xtest, groupstest, ytest = ytest) #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest)  #>  #>  #> alpha =  0.5  #>  #> Performance (C-index): #>  #>            allGroups   mean wtdMean group_1 group_2 group_3 #> Overall       0.8545 0.8673  0.8608  0.9139  0.7746  0.9133 #> Pretrain      0.8359 0.8396  0.8393  0.9152  0.8173  0.7864 #> Individual    0.7925 0.7985  0.8008  0.9075  0.8007  0.6873 #>  #> Support size: #>                                          #> Overall    6                             #> Pretrain   35 (4 common + 31 individual) #> Individual 37 ################################################## # Fit: ################################################## fit = cv.ptLasso(x, y, groups, family = \"cox\", type.measure = \"C\")  ################################################## # Predict with a common alpha for all groups: ################################################## predict(fit, xtest, groupstest, ytest = ytest)  ################################################## # Predict with a different alpha for each group: ################################################## predict(fit, xtest, groupstest, ytest = ytest, alphatype = \"varying\")"},{"path":"https://erincr.github.io/ptLasso/articles/TargetGroupedData.html","id":"intuition","dir":"Articles","previous_headings":"","what":"Intuition","title":"Target grouped or multinomial reponse data","text":"Now turn target grouped setting, dataset multinomial outcome grouping observations. example, data might look like following: row XX belongs class 1, 2 3, wish predict class membership. fit single multinomial model data: , fit 3 one-vs-rest models; prediction time, assign observations class highest probability. Another alternative pretraining, fits something one model data three separate models. ptLasso , using arguments family = \"multinomial\" use.case = \"targetGroups\". exactly pretraining ? ’ll walk example, pretraining “hand”. steps : Train overall model: multinomial model using penalty coefficients β\\beta coefficient either 0 nonzero classes. Train individual one-vs-rest models using penalty factor offset defined overall model (input grouped setting). train overall model, use cv.glmnet type.multinomial = \"grouped\". puts penalty β\\beta force coefficients model classes. analogous overall model input grouped setting: want first learn shared information. , fit 3 one-vs-rest models using support offset multinomial model. Now everything need train one-vs-rest models. always, pretraining parameter α\\alpha - example, let’s use α=0.5\\alpha = 0.5: ’re done pretraining! predict, assign row class highest prediction: done automatically within ptLasso; now show example using ptLasso functions. example intended show pretraining works multinomial outcomes, technical details omitted. (example, ptLasso takes care crossfitting first second steps.)","code":"set.seed(1234)  n = 500; p = 75; k = 3 X = matrix(rnorm(n * p), nrow = n, ncol = p) y = sample(1:k, n, replace = TRUE)  Xtest = matrix(rnorm(n * p), nrow = n, ncol = p) multinomial = cv.glmnet(X, y, family = \"multinomial\")  multipreds  = predict(multinomial, Xtest, s = \"lambda.min\") multipreds.class = apply(multipreds, 1, which.max) class1 = cv.glmnet(X, y == 1, family = \"binomial\") class2 = cv.glmnet(X, y == 2, family = \"binomial\") class3 = cv.glmnet(X, y == 3, family = \"binomial\")  ovrpreds = cbind(   predict(class1, Xtest, s = \"lambda.min\"),   predict(class2, Xtest, s = \"lambda.min\"),   predict(class3, Xtest, s = \"lambda.min\")) ovrpreds.class = apply(ovrpreds, 1, which.max) fit = ptLasso(X, y, groups = y, alpha = 0.5,               family = \"multinomial\",                use.case = \"targetGroups\") multinomial = cv.glmnet(X, y, family = \"multinomial\",                          type.multinomial = \"grouped\",                         keep = TRUE) # The support of the overall model: nonzero.coefs = which((coef(multinomial, s = \"lambda.1se\")[[1]] != 0)[-1])  # The offsets - one for each class: offset = predict(multinomial, X, s = \"lambda.1se\") offset.class1 = offset[, 1, 1] offset.class2 = offset[, 2, 1] offset.class3 = offset[, 3, 1] alpha = 0.5 penalty.factor = rep(1/alpha, p) penalty.factor[nonzero.coefs] = 1  class1 = cv.glmnet(X, y == 1, family = \"binomial\",                     offset = (1-alpha) * offset.class1,                    penalty.factor = penalty.factor) class2 = cv.glmnet(X, y == 2, family = \"binomial\",                     offset = (1-alpha) * offset.class2,                    penalty.factor = penalty.factor) class3 = cv.glmnet(X, y == 3, family = \"binomial\",                     offset = (1-alpha) * offset.class3,                    penalty.factor = penalty.factor) newoffset = predict(multinomial, X, s = \"lambda.1se\") ovrpreds = cbind(   predict(class1, Xtest, s = \"lambda.min\", newoffset = newoffset[, 1, 1]),   predict(class2, Xtest, s = \"lambda.min\", newoffset = newoffset[, 2, 1]),   predict(class3, Xtest, s = \"lambda.min\", newoffset = newoffset[, 3, 1]) ) ovrpreds.class = apply(ovrpreds, 1, which.max)"},{"path":"https://erincr.github.io/ptLasso/articles/TargetGroupedData.html","id":"example","dir":"Articles","previous_headings":"","what":"Example","title":"Target grouped or multinomial reponse data","text":"First, let’s simulate multinomial data 5 classes. start drawing XX normal distribution (uncorrelated features), shift columns differently group. calls ptLasso cv.ptLasso almost input grouped setting, now specify use.case = \"targetGroups\". call predict require groups argument groups unknown prediction time.","code":"set.seed(1234)  n = 500; p = 50; k = 5 class.sizes = rep(n/k, k) ncommon = 10; nindiv = 5; shift.common = seq(-.2, .2, length.out = k) shift.indiv  = seq(-.1, .1, length.out = k)  x     = matrix(rnorm(n * p), n, p) xtest = matrix(rnorm(n * p), n, p) y = ytest = c(sapply(1:length(class.sizes), function(i) rep(i, class.sizes[i])))  start = ncommon + 1 for (i in 1:k) {   end = start + nindiv - 1   x[y == i, 1:ncommon] = x[y == i, 1:ncommon] + shift.common[i]   x[y == i, start:end] = x[y == i, start:end] + shift.indiv[i]      xtest[ytest == i, 1:ncommon] = xtest[ytest == i, 1:ncommon] + shift.common[i]   xtest[ytest == i, start:end] = xtest[ytest == i, start:end] + shift.indiv[i]   start = end + 1 } ################################################################################ # Fit the pretrained model. # By default, ptLasso uses type.measure = \"deviance\", but for ease of # interpretability, we use type.measure = \"class\" (the misclassification rate). ################################################################################ fit = ptLasso(x = x, y = y,                use.case = \"targetGroups\", type.measure = \"class\")  ################################################################################ # Predict ################################################################################ predict(fit, xtest, ytest = ytest) #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, ytest = ytest)  #>  #>  #>  #> alpha =  0.5  #>  #> Performance (Misclassification error): #>  #>            overall   mean group_1 group_2 group_3 group_4 group_5 #> Overall      0.738                                                #> Pretrain     0.728 0.2000   0.200     0.2     0.2     0.2   0.200 #> Individual   0.736 0.1984   0.196     0.2     0.2     0.2   0.196 #>  #> Support size: #>                                          #> Overall    29                            #> Pretrain   23 (23 common + 0 individual) #> Individual 32  ################################################################################ # Fit with CV to choose the alpha parameter ################################################################################ cvfit = cv.ptLasso(x = x, y = y,               use.case = \"targetGroups\", type.measure = \"class\")  ################################################################################ # Predict using one alpha for all classes ################################################################################ predict(cvfit, xtest, ytest = ytest) #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, ytest = ytest)  #>  #>  #>  #> alpha =  0.9  #>  #> Performance (Misclassification error): #>  #>            overall   mean group_1 group_2 group_3 group_4 group_5 #> Overall      0.738                                                #> Pretrain     0.722 0.1992     0.2     0.2     0.2     0.2   0.196 #> Individual   0.742 0.2000     0.2     0.2     0.2     0.2   0.200 #>  #> Support size: #>                                          #> Overall    39                            #> Pretrain   32 (23 common + 9 individual) #> Individual 36  ################################################################################ # Predict using a separate alpha for each class ################################################################################ predict(cvfit, xtest, ytest = ytest, alphatype = \"varying\") #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, ytest = ytest,   #>     alphatype = \"varying\")  #>  #>  #> alpha =  0.1 0 0.7 0 0.1  #>  #> Performance (Misclassification error): #>  #>            overall   mean group_1 group_2 group_3 group_4 group_5 #> Overall      0.738                                                #> Pretrain     0.742 0.2016   0.208     0.2     0.2   0.202   0.198 #> Individual   0.742 0.2000   0.200     0.2     0.2   0.200   0.200 #>  #> Support size: #>                                           #> Overall    39                             #> Pretrain   36 (23 common + 13 individual) #> Individual 36"},{"path":"https://erincr.github.io/ptLasso/articles/TimeSeriesData.html","id":"example-1-covariates-are-constant-over-time","dir":"Articles","previous_headings":"","what":"Example 1: covariates are constant over time","title":"Time series data","text":"’ll start simulating data – details comments. simulated data, ready call ptLasso; call ptLasso looks much examples, now (1) yy matrix one column time point (2) specify use.case = \"timeSeries\". fitting, call plot shows models fitted time points without using pretraining.  , can predict xtest. example, pretraining helps performance: two time points share support, pretraining discovers leverages . specified alpha = 0 example, cross validation advise us choose α=0.2\\alpha = 0.2. Plotting shows us average performance across two time points. Importantly, time 1, individual model pretrained model ; see advantage pretraining time 2 (use information time 1). Note also treated multireponse problem, ignored time-ordering responses. See section called “Multi-response data Gaussian responses”. (However, time ordering can informative, multi-response approach make use .)","code":"set.seed(1234)  # Define constants n = 600          # Total number of samples ntrain = 300     # Number of training samples p = 100          # Number of features sigma = 3        # Standard deviation of noise  # Generate covariate matrix x = matrix(rnorm(n * p), n, p)  # Define coefficients for time points 1 and 2 beta1 = c(rep(2, 10), rep(0, p - 10))  # Coefs at time 1 beta2 = runif(p, 0.5, 2) * beta1       # Coefs at time 2, shared support with time 1  # Generate response variables for times 1 and 2 y = cbind(   x %*% beta1 + sigma * rnorm(n),   x %*% beta2 + sigma * rnorm(n) )  # Split data into training and testing sets xtest = x[-(1:ntrain), ]  # Test covariates ytest = y[-(1:ntrain), ]  # Test response  x = x[1:ntrain, ]  # Train covariates y = y[1:ntrain, ]  # Train response fit = ptLasso(x, y, use.case = \"timeSeries\", alpha = 0) plot(fit) preds = predict(fit, xtest, ytest = ytest) preds #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, ytest = ytest)  #>  #>  #>  #> alpha =  0  #>  #> Performance (Mean squared error): #>  #>              mean response_1 response_2 #> Pretrain    9.604      10.78      8.428 #> Individual 10.428      10.78     10.076 #>  #> Support size: #>                                           #> Pretrain   26 (10 common + 16 individual) #> Individual 39 cvfit = cv.ptLasso(x, y, use.case = \"timeSeries\") plot(cvfit) predict(cvfit, xtest, ytest = ytest) #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, ytest = ytest)  #>  #>  #>  #> alpha =  0.2  #>  #> Performance (Mean squared error): #>  #>              mean response_1 response_2 #> Pretrain    9.875      10.87      8.884 #> Individual 10.447      10.87     10.027 #>  #> Support size: #>                                           #> Pretrain   28 (10 common + 18 individual) #> Individual 40 fit = ptLasso(x, y, use.case = \"multiresponse\")"},{"path":"https://erincr.github.io/ptLasso/articles/TimeSeriesData.html","id":"example-2-covariates-change-over-time","dir":"Articles","previous_headings":"","what":"Example 2: covariates change over time","title":"Time series data","text":"Now, ’ll repeat , ’ll simulate data xx changes time. setting, ptLasso expects xx list one covariate matrix time. Now, xx list length two: can call ptLasso, cv.ptLasso, plot predict just :","code":"set.seed(1234)  # Set seed for reproducibility  # Define constants n = 600          # Total number of samples ntrain = 300     # Number of training samples p = 100          # Number of features sigma = 3        # Standard deviation of noise  # Covariates for times 1 and 2 x1 = matrix(rnorm(n * p), n, p) x2 = x1 + matrix(0.2 * rnorm(n * p), n, p)  # Perturbed covariates for time 2 x = list(x1, x2)  # Define coefficients for time points 1 and 2 beta1 = c(rep(2, 10), rep(0, p - 10))  # Coefs at time 1 beta2 = runif(p, 0.5, 2) * beta1       # Coefs at time 2, shared support with time 1  # Response variables for times 1 and 2: y = cbind(   x[[1]] %*% beta1 + sigma * rnorm(n),   x[[2]] %*% beta2 + sigma * rnorm(n) )  # Split data into training and testing sets xtest = lapply(x, function(xx) xx[-(1:ntrain), ])  # Test covariates ytest = y[-(1:ntrain), ]  # Test response  x = lapply(x, function(xx) xx[1:ntrain, ])  # Train covariates y = y[1:ntrain, ]  # Train response str(x) #> List of 2 #>  $ : num [1:300, 1:100] -1.207 0.277 1.084 -2.346 0.429 ... #>  $ : num [1:300, 1:100] -1.493 0.303 1.172 -2.316 0.224 ... fit = ptLasso(x, y, use.case = \"timeSeries\", alpha = 0) plot(fit)  # Plot the fitted model predict(fit, xtest, ytest = ytest)  # Predict using the fitted model #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, ytest = ytest)  #>  #>  #>  #> alpha =  0  #>  #> Performance (Mean squared error): #>  #>             mean response_1 response_2 #> Pretrain   11.92       12.1      11.75 #> Individual 11.46       12.1      10.82 #>  #> Support size: #>                                           #> Pretrain   36 (16 common + 20 individual) #> Individual 61  # With cross validation: cvfit = cv.ptLasso(x, y, use.case = \"timeSeries\") plot(cvfit, plot.alphahat = TRUE)  # Plot cross-validated model predict(cvfit, xtest, ytest = ytest)  # Predict using cross-validated model #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, ytest = ytest)  #>  #>  #>  #> alpha =  0.4  #>  #> Performance (Mean squared error): #>  #>             mean response_1 response_2 #> Pretrain   11.55      12.11      11.00 #> Individual 11.53      12.11      10.96 #>  #> Support size: #>                                           #> Pretrain   54 (19 common + 35 individual) #> Individual 65"},{"path":"https://erincr.github.io/ptLasso/articles/UsingNonlinearBases.html","id":"example-1-xgboost-pretraining","dir":"Articles","previous_headings":"","what":"Example 1: xgboost pretraining","title":"Using nonlinear bases","text":"start simulating data (n=1000n = 1000, p=500p = 500) continuous response. coefficients β\\beta sparse; first 200 entries drawn standard univariate normal, remainder 00. define yy y=1(X>0)β+ϵy = 1(X > 0) \\beta + \\epsilon, ϵ\\epsilon noise; hope xgboost learn splits corresponding X>0X > 0. Now, run xgboost get basis functions: ready model fitting cv.glmnet. two baselines (1) linear model pretrain xgboost, (2) xgboost. find glmnet together xgboost outperforms glmnet alone xgboost alone.","code":"require(xgboost) #> Loading required package: xgboost set.seed(1234)  n = 1000; p = 500; noise = 5;  x     = matrix(rnorm(n * p), nrow=n, ncol=p) xtest = matrix(rnorm(n * p), nrow=n, ncol=p)  x.model     = 1*(x > 0)      xtest.model = 1*(xtest > 0)   beta = c(rnorm(200), rep(0, p-200))  y     = x.model %*% beta + noise * rnorm(n) ytest = xtest.model %*% beta + noise * rnorm(n)  train.folds = sample(rep(1:10, n/10)) xgbfit      = xgboost(data=x, label=y, nrounds=200, max_depth=1, verbose=0)  x.boost     = predict(xgbfit, x, predleaf = TRUE) - 1 xtest.boost = predict(xgbfit, xtest, predleaf = TRUE) - 1 cvfit = cv.glmnet(x.boost, y, type.measure = \"mse\", foldid = train.folds) cvfit.noboost = cv.glmnet(x, y, type.measure = \"mse\", foldid = train.folds)  cat(\"Lasso with xgboost pretraining PSE: \",      assess.glmnet(cvfit, newx = xtest.boost, newy = ytest)$mse) #> Lasso with xgboost pretraining PSE:  56.20989  cat(\"Lasso without xgboost pretraining PSE: \",      assess.glmnet(cvfit.noboost, newx = xtest, newy = ytest)$mse) #> Lasso without xgboost pretraining PSE:  62.5489  cat(\"xgboost alone PSE: \",      assess.glmnet(predict(xgbfit, xtest), newy = ytest)$mse) #> xgboost alone PSE:  56.31106"},{"path":"https://erincr.github.io/ptLasso/articles/UsingNonlinearBases.html","id":"example-2-xgboost-pretraining-with-input-groups","dir":"Articles","previous_headings":"","what":"Example 2: xgboost pretraining with input groups","title":"Using nonlinear bases","text":"Now, let’s repeat supposing data input groups. difference use cv.ptLasso model instead cv.glmnet, use group indicators feature fitting xgboost. start simulating data 2 groups (500500 observations group) continuous response. , simulate yy y=1(X>0)β+ϵy = 1(X > 0) \\beta + \\epsilon, now different β\\beta group. coefficients groups Table @ref(tab:nonlinear). example closely follows example , code included run default. Coefficients simulating data use xgboost pretraining dummy variables group indicators; use fit predict xgboost. Now, let’s train xgboost predict get new features. Note now use max_depth = 2: intended allow interactions group indicators features. Finally, ready fit two models trained cv.ptLasso: one uses xgboost features . , find pretraining xgboost improves performance relative (1) model fitting original feature space (2) xgboost alone.","code":"set.seed(1234)  n = 1000; p = 500; k = 2; noise = 5;  groups = groupstest = sort(rep(1:k, n/k))  x     = matrix(rnorm(n * p), nrow=n, ncol=p) xtest = matrix(rnorm(n * p), nrow=n, ncol=p)  x.model     = 1*(x > 0)      xtest.model = 1*(xtest > 0)  common.beta = c(rep(2, 50), rep(0, p-50)) beta.1 = c(rep(0, 50),  rep(1, 50), rep(0, p-100))  beta.2 = c(rep(0, 100), rep(1, 50), rep(0, p-150))  beta.3 = c(rep(0, 150), rep(1, 50), rep(0, p-200))   y = x.model %*% common.beta + noise * rnorm(n) y[groups == 1] = y[groups == 1] + x.model[groups == 1, ] %*% beta.1 y[groups == 2] = y[groups == 2] + x.model[groups == 2, ] %*% beta.2 y[groups == 3] = y[groups == 3] + x.model[groups == 3, ] %*% beta.3  ytest = xtest.model %*% common.beta + noise * rnorm(n) ytest[groups == 1] = ytest[groups == 1] + xtest.model[groups == 1, ] %*% beta.1 ytest[groups == 2] = ytest[groups == 2] + xtest.model[groups == 2, ] %*% beta.2 ytest[groups == 3] = ytest[groups == 3] + xtest.model[groups == 3, ] %*% beta.3 group.ids     = model.matrix(~as.factor(groups) - 1)  grouptest.ids = model.matrix(~as.factor(groupstest) - 1)  colnames(grouptest.ids) = colnames(group.ids) xgbfit      = xgboost(data=cbind(x, group.ids), label=y,                        nrounds=100, max_depth=2, verbose=0)  x.boost     = predict(xgbfit, cbind(x, group.ids), predleaf = TRUE) - 1 xtest.boost = predict(xgbfit, cbind(xtest, grouptest.ids), predleaf = TRUE) - 1 cvfit = cv.ptLasso(x.boost, y, groups=groups, type.measure = \"mse\") preds = predict(cvfit, xtest.boost, groups=groupstest, alphatype = \"varying\") preds = preds$yhatpre  cvfit.noboost = cv.ptLasso(x, y, groups=groups, type.measure = \"mse\") preds.noboost = predict(cvfit.noboost, xtest, groups=groupstest,                          alphatype = \"varying\") preds.noboost = preds.noboost$yhatpre  cat(\"ptLasso with xgboost pretraining PSE: \",      assess.glmnet(preds, newy = ytest)$mse)  cat(\"ptLasso without xgboost pretraining PSE: \",      assess.glmnet(preds.noboost, newy = ytest)$mse)  cat(\"xgboost alone PSE: \",      assess.glmnet(predict(xgbfit, xtest), newy = ytest)$mse)"},{"path":"https://erincr.github.io/ptLasso/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Erin Craig. Author, maintainer, copyright holder. Rob Tibshirani. Author, copyright holder.","code":""},{"path":"https://erincr.github.io/ptLasso/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Craig E, Pilanci M, Le Menestrel T, Narasimhan B, Rivas MA, Gullaksen S-E, Dehghannasiri R, Salzman J, Taylor J, Tibshirani R (2025). Pretraining lasso. Journal Royal Statistical Society: Series B (Statistical Methodology), qkaf050. Craig E, Tibshirani R (2025). ptLasso: Pretraining lasso R. R package version 1.0.1. URL: https://erincr.github.io/ptLasso/. Craig E, Pilanci M, Le Menestrel T, Narasimhan B, Rivas MA, Gullaksen S-E, Dehghannasiri R, Salzman J, Taylor J, Tibshirani R (2025). Pretraining lasso. Journal Royal Statistical Society: Series B (Statistical Methodology), qkaf050. Craig E, Tibshirani R (2025). ptLasso: Pretraining lasso R. R package version 1.0.1. URL: https://erincr.github.io/ptLasso/.","code":"@Article{,   title = {Pretraining and the lasso},   author = {Erin Craig and Mert Pilanci and Thomas {Le Menestrel} and Balasubramanian Narasimhan and Manuel A Rivas and Stein-Erik Gullaksen and Roozbeh Dehghannasiri and Julia Salzman and Jonathan Taylor and Robert Tibshirani},   journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},   year = {2025},   pages = {qkaf050},   doi = {10.1093/jrsssb/qkaf050},   url = {https://doi.org/10.1093/jrsssb/qkaf050}, } @Manual{,   title = {ptLasso: Pretraining and the lasso in R},   author = {Erin Craig and Robert Tibshirani},   year = {2025},   note = {R package version 1.0.1},   url = {https://erincr.github.io/ptLasso/}, } @Article{,   title = {Pretraining and the lasso},   author = {Erin Craig and Mert Pilanci and Thomas {Le Menestrel} and Balasubramanian Narasimhan and Manuel A Rivas and Stein-Erik Gullaksen and Roozbeh Dehghannasiri and Julia Salzman and Jonathan Taylor and Robert Tibshirani},   journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},   year = {2025},   pages = {qkaf050},   doi = {10.1093/jrsssb/qkaf050},   url = {https://doi.org/10.1093/jrsssb/qkaf050}, } @Manual{,   title = {ptLasso: Pretraining and the lasso in R},   author = {Erin Craig and Robert Tibshirani},   year = {2025},   note = {R package version 1.0.1},   url = {https://erincr.github.io/ptLasso/}, }"},{"path":"https://erincr.github.io/ptLasso/index.html","id":"pretraining-and-the-lasso","dir":"","previous_headings":"","what":"Pretraining and the Lasso","title":"Pretraining and the Lasso","text":"package fits pretrained generalized linear models : (1) data grouped observations, (2) data without grouped observations, multinomial responses, (3) data multiple Gaussian responses (4) time series data (data repeated measurements time). Documentation examples available vignettes within package, can accessed “Articles” tab page. vignettes also include examples pretraining settings yet supported package, including conditional average treatment effect estimation unsupervised pretraining. Details pretraining may found Craig et al. (2024). model fitting package done cv.glmnet, syntax closely follows glmnet package (2010).","code":""},{"path":"https://erincr.github.io/ptLasso/index.html","id":"tutorials","dir":"","previous_headings":"","what":"Tutorials","title":"Pretraining and the Lasso","text":"introductory YouTube tutorials R Markdown examples, please visit website lasso pretraining.","code":""},{"path":"https://erincr.github.io/ptLasso/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Pretraining and the Lasso","text":"install package, recommend following instructions.","code":""},{"path":"https://erincr.github.io/ptLasso/index.html","id":"having-trouble","dir":"","previous_headings":"","what":"Having trouble?","title":"Pretraining and the Lasso","text":"find bug feature request, please open new issue.","code":""},{"path":"https://erincr.github.io/ptLasso/index.html","id":"references","dir":"","previous_headings":"","what":"References","title":"Pretraining and the Lasso","text":"Craig, Erin, Mert Pilanci, Thomas Le Menestrel, Balasubramanian Narasimhan, Manuel Rivas, Roozbeh Dehghannasiri, Julia Salzman, Jonathan Taylor, Robert Tibshirani. “Pretraining Lasso.” arXiv preprint arXiv:2401.12911 (2024). Friedman, Jerome, Trevor Hastie, Robert Tibshirani. 2010. “Regularization Paths Generalized Linear Models via Coordinate Descent.” Journal Statistical Software, Articles 33 (1): 1–22. https://doi.org/10.18637/jss.v033.i01.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/binomial.example.data.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulate input grouped data (binomial outcome) for testing with ptLasso. — binomial.example.data","title":"Simulate input grouped data (binomial outcome) for testing with ptLasso. — binomial.example.data","text":"required arguments; used primarily documentation. Simply calls makedata reasonable set features.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/binomial.example.data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulate input grouped data (binomial outcome) for testing with ptLasso. — binomial.example.data","text":"","code":"binomial.example.data(   k = 3,   class.sizes = rep(100, k),   n = sum(class.sizes),   scommon = 5,   sindiv = rep(5, k),   p = 2 * (sum(sindiv) + scommon),   beta.common = list(c(-0.5, 0.5, 0.3, -0.9, 0.1), c(-0.3, 0.9, 0.1, -0.1, 0.2), c(0.1,     0.2, -0.1, 0.2, 0.3)),   beta.indiv = lapply(1:k, function(i) 0.9 * beta.common[[i]]),   intercepts = rep(0, k),   sigma = NULL )"},{"path":"https://erincr.github.io/ptLasso/reference/binomial.example.data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulate input grouped data (binomial outcome) for testing with ptLasso. — binomial.example.data","text":"k Default: 3. class.sizes Default: rep(100, k). n Default: sum(class.sizes). scommon Default: 5. sindiv Default: rep(5, k). p Default: 2*(sum(sindiv) + scommon). beta.common Default: list(c(-.5, .5, .3, -.9, .1), c(-.3, .9, .1, -.1, .2), c(0.1, .2, -.1, .2, .3)). beta.indiv Default: lapply(1:k, function()  0.9 * beta.common[[]]). intercepts Default: rep(0,k). sigma Default: NULL.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/binomial.example.data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simulate input grouped data (binomial outcome) for testing with ptLasso. — binomial.example.data","text":"list data 5 groups binomial outcome, n=300 p=40: x Simulated features, size n x p. y Outcomes y, length n. groups Vector length n, indicating observations belong group. snr Gaussian outcome : signal noise ratio. mu Gaussian outcome : value y noise added.","code":""},{"path":[]},{"path":"https://erincr.github.io/ptLasso/reference/binomial.example.data.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Simulate input grouped data (binomial outcome) for testing with ptLasso. — binomial.example.data","text":"Erin Craig Rob Tibshirani Maintainer: Erin Craig <erincr@stanford.edu>","code":""},{"path":"https://erincr.github.io/ptLasso/reference/binomial.example.data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulate input grouped data (binomial outcome) for testing with ptLasso. — binomial.example.data","text":"","code":"out = binomial.example.data() x = out$x; y=out$y; groups = out$group"},{"path":"https://erincr.github.io/ptLasso/reference/coef.cv.ptLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the coefficients from a fitted cv.ptLasso model. — coef.cv.ptLasso","title":"Get the coefficients from a fitted cv.ptLasso model. — coef.cv.ptLasso","text":"Get coefficients fitted cv.ptLasso model.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/coef.cv.ptLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the coefficients from a fitted cv.ptLasso model. — coef.cv.ptLasso","text":"","code":"# S3 method for class 'cv.ptLasso' coef(   object,   model = c(\"all\", \"individual\", \"overall\", \"pretrain\"),   alpha = NULL,   ... )"},{"path":"https://erincr.github.io/ptLasso/reference/coef.cv.ptLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the coefficients from a fitted cv.ptLasso model. — coef.cv.ptLasso","text":"object fitted \"cv.ptLasso\" object. model string indicating coefficients retrieve. Must one \"\", \"individual\", \"overall\" \"pretrain\". alpha value 0 1, indicating alpha use. NULL, return coefficients models.  impacts results model = \"\" model = \"pretrain\". ... arguments passed \"coef\" function. May e.g. s = \"lambda.min\".","code":""},{"path":[]},{"path":"https://erincr.github.io/ptLasso/reference/coef.cv.ptLasso.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Get the coefficients from a fitted cv.ptLasso model. — coef.cv.ptLasso","text":"Erin Craig Rob Tibshirani Maintainer: Erin Craig <erincr@stanford.edu>","code":""},{"path":"https://erincr.github.io/ptLasso/reference/coef.cv.ptLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get the coefficients from a fitted cv.ptLasso model. — coef.cv.ptLasso","text":"","code":"set.seed(1234) out = gaussian.example.data(k=2, class.sizes = c(50, 50)) x = out$x; y=out$y; groups = out$group;  cvfit = cv.ptLasso(x, y, groups = groups, family = \"gaussian\", type.measure = \"mse\") # Get all model coefficients. names(coef(cvfit)) #> [1] \"individual\" \"pretrain\"   \"overall\"     coef(cvfit, model = \"overall\") # Overall model only #> 62 x 1 sparse Matrix of class \"dgCMatrix\" #>             lambda.1se #> (Intercept) -2.0911194 #> groups2      6.9480612 #>              .         #>              .         #>              .         #>              2.7965296 #>              3.3203621 #>              .         #>              0.5383593 #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         #>              .         length(coef(cvfit, model = \"individual\")) # List of coefficients for each group model #> [1] 2 length(coef(cvfit, model = \"pretrain\", alpha = .5)) # List of coefficients for each group model #> [1] 2"},{"path":"https://erincr.github.io/ptLasso/reference/coef.ptLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the coefficients from a fitted ptLasso model. — coef.ptLasso","title":"Get the coefficients from a fitted ptLasso model. — coef.ptLasso","text":"Get coefficients fitted ptLasso model.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/coef.ptLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the coefficients from a fitted ptLasso model. — coef.ptLasso","text":"","code":"# S3 method for class 'ptLasso' coef(object, model = c(\"all\", \"individual\", \"overall\", \"pretrain\"), ...)"},{"path":"https://erincr.github.io/ptLasso/reference/coef.ptLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the coefficients from a fitted ptLasso model. — coef.ptLasso","text":"object fitted \"ptLasso\" object. model string indicating coefficients retrieve. Must one \"\", \"individual\", \"overall\" \"pretrain\". ... arguments passed \"coef\" function. May e.g. s = \"lambda.min\".","code":""},{"path":"https://erincr.github.io/ptLasso/reference/coef.ptLasso.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the coefficients from a fitted ptLasso model. — coef.ptLasso","text":"Model coefficients. model = \"overall\", function returns output coef. model \"individual\" \"pretrain\", function returns list containing results coef group-specific model. model = \"\", returns list containing (overall, individual pretrain) coefficients.","code":""},{"path":[]},{"path":"https://erincr.github.io/ptLasso/reference/coef.ptLasso.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Get the coefficients from a fitted ptLasso model. — coef.ptLasso","text":"Erin Craig Rob Tibshirani Maintainer: Erin Craig <erincr@stanford.edu>","code":""},{"path":"https://erincr.github.io/ptLasso/reference/coef.ptLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get the coefficients from a fitted ptLasso model. — coef.ptLasso","text":"","code":"# Train data out = gaussian.example.data() x = out$x; y=out$y; groups = out$group;  fit = ptLasso(x, y, groups = groups, family = \"gaussian\", type.measure = \"mse\") # Get all model coefficients. names(coef(fit)) #> [1] \"individual\" \"pretrain\"   \"overall\"     coef(fit, model = \"overall\") # Overall model only #> 125 x 1 sparse Matrix of class \"dgCMatrix\" #>             lambda.1se #> (Intercept)  -2.307774 #> groups2      -1.226325 #> groups3       2.084267 #> groups4       2.421749 #> groups5      -5.413357 #>               6.696258 #>               7.495847 #>               5.994498 #>               6.349846 #>               8.511716 #>               7.547739 #>               7.251902 #>               7.598820 #>               7.724000 #>               7.192878 #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        #>               .        length(coef(fit, model = \"individual\")) # List of coefficients for each group model #> [1] 5 length(coef(fit, model = \"pretrain\")) # List of coefficients for each group model #> [1] 5"},{"path":"https://erincr.github.io/ptLasso/reference/cv.ptLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"Cross-validation for ptLasso — cv.ptLasso","title":"Cross-validation for ptLasso — cv.ptLasso","text":"Cross-validation ptLasso.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/cv.ptLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cross-validation for ptLasso — cv.ptLasso","text":"","code":"cv.ptLasso(   x,   y,   groups = NULL,   alphalist = seq(0, 1, length = 11),   family = c(\"default\", \"gaussian\", \"multinomial\", \"binomial\", \"cox\"),   use.case = c(\"inputGroups\", \"targetGroups\", \"multiresponse\", \"timeSeries\"),   type.measure = c(\"default\", \"mse\", \"mae\", \"auc\", \"deviance\", \"class\", \"C\"),   nfolds = 10,   foldid = NULL,   verbose = FALSE,   fitoverall = NULL,   fitind = NULL,   s = \"lambda.min\",   gamma = \"gamma.min\",   alphahat.choice = \"overall\",   group.intercepts = TRUE,   ... )"},{"path":"https://erincr.github.io/ptLasso/reference/cv.ptLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cross-validation for ptLasso — cv.ptLasso","text":"x x matrix ptLasso. y y vector matrix ptLasso. groups vector length nobs indicating group observation belongs. data k groups, groups coded integers 1 k. 'use.case = \"inputGroups\"'. alphalist vector values pretraining hyperparameter alpha. Defaults seq(0, 1, length.=11). function pretraining choice alpha alphalist return CV performance alpha. family Response type ptLasso. use.case type grouping observed data. Can one \"inputGroups\", \"targetGroups\", \"multiresponse\" \"timeSeries\". type.measure Measure computed cv.glmnet, ptLasso. nfolds Number folds CV (default 10). Although nfoldscan large sample size (leave-one-CV), recommended large datasets. Smallest value allowable nfolds = 3. foldid optional vector values 1 nfolds identifying fold observation . supplied, nfolds can missing. verbose verbose=1, print statement showing model currently fit. fitoverall optional cv.glmnet object specifying overall model. trained full training data, argument keep = TRUE. fitind optional list cv.glmnet objects specifying individual models. trained training data, argumnet keep = TRUE. s choice lambda used models estimating CV performance choice alpha. Defaults \"lambda.min\". May \"lambda.1se\", numeric value. (Use caution supplying numeric value: lambda used models.) gamma use relax = TRUE. choice gamma used models estimating CV performance choice alpha. Defaults \"gamma.min\". May also \"gamma.1se\". alphahat.choice choosing alphahat, may prefer best performance using data (alphahat.choice = \"overall\") best average performance across groups (alphahat.choice = \"mean\"). particularly useful type.measure \"auc\" \"C\", average performance across groups different performance full dataset. default \"overall\". group.intercepts 'use.case = \"inputGroups\"' . `TRUE`, fit overall model separate intercept group. `FALSE`, ignore grouping fit one overall intercept. Default `TRUE`. ... Additional arguments passed `cv.glmnet` function. Notable choices include \"trace.\" \"parallel\". trace.= TRUE, progress bar displayed call cv.glmnet; useful big models take long time fit. parallel = TRUE, use parallel foreach fit fold.  Must register parallel hand, doMC others. Importantly, \"cv.ptLasso\" support arguments \"intercept\", \"offset\", \"fit\" \"check.args\".","code":""},{"path":"https://erincr.github.io/ptLasso/reference/cv.ptLasso.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cross-validation for ptLasso — cv.ptLasso","text":"object class \"cv.ptLasso\", list ingredients cross-validation fit. call call produced object. alphahat Value alpha optimizes CV performance data. varying.alphahat Vector values alpha, kth optimizes performance group k. alphalist Vector alphas compared. errall CV performance overall model. errpre CV performance pretrained models (one alpha tried). errind CV performance individual model. fit List ptLasso objects, one alpha tried. fitoverall fitted overall model used first stage pretraining. fitoverall.lambda value lambda used first stage pretraining. fitind list containing one individual model group. use.case use case: \"inputGroups\" \"targetGroups\". family family used. type.measure type.measure used.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/cv.ptLasso.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Cross-validation for ptLasso — cv.ptLasso","text":"function runs ptLasso requested choice alpha, returns cross validated performance.","code":""},{"path":[]},{"path":"https://erincr.github.io/ptLasso/reference/cv.ptLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cross-validation for ptLasso — cv.ptLasso","text":"","code":"# Getting started. First, we simulate data: we need covariates x, response y and group IDs. set.seed(1234) n = 80 p = 20 x = matrix(rnorm(n*p), n, p) y = rnorm(n) groups = sort(rep(1:5, n/5))  xtest = matrix(rnorm(n*p), n, p) ytest = rnorm(n) groupstest = sort(rep(1:5, n/5))  # Model fitting cvfit = cv.ptLasso(x, y, groups = groups, family = \"gaussian\", nfolds=3,                     type.measure = \"mse\") cvfit #>  #> Call:   #> cv.ptLasso(x = x, y = y, groups = groups, family = \"gaussian\",   #>     type.measure = \"mse\", nfolds = 3, use.case = \"inputGroups\",   #>     group.intercepts = TRUE)  #>  #>  #> type.measure:  mse  #>  #>  #>            alpha overall  mean wtdMean group_1 group_2 group_3 group_4 group_5 #> Overall            1.318 1.318   1.318  1.0289   1.761   1.447   1.519  0.8360 #> Pretrain     0.0   1.555 1.555   1.555  1.2253   1.932   1.635   1.870  1.1151 #> Pretrain     0.1   1.772 1.772   1.772  1.9005   2.454   1.665   1.808  1.0337 #> Pretrain     0.2   1.586 1.586   1.586  1.2015   1.946   1.973   1.803  1.0073 #> Pretrain     0.3   1.630 1.630   1.630  1.7202   1.939   1.502   1.770  1.2186 #> Pretrain     0.4   1.419 1.419   1.419  1.1071   1.895   1.634   1.378  1.0786 #> Pretrain     0.5   1.506 1.506   1.506  0.9974   1.527   1.842   2.219  0.9469 #> Pretrain     0.6   1.639 1.639   1.639  1.3574   2.295   1.654   1.578  1.3111 #> Pretrain     0.7   1.360 1.360   1.360  0.9112   1.505   1.690   1.493  1.2029 #> Pretrain     0.8   1.387 1.387   1.387  0.9263   1.642   1.556   1.960  0.8495 #> Pretrain     0.9   1.305 1.305   1.305  1.0910   1.354   1.546   1.419  1.1155 #> Pretrain     1.0   1.252 1.252   1.252  1.0629   1.162   1.526   1.387  1.1244 #> Individual         1.252 1.252   1.252  1.0629   1.162   1.526   1.387  1.1244 #>  #> alphahat (fixed) = 1 #> alphahat (varying): #> group_1 group_2 group_3 group_4 group_5  #>     0.7     1.0     0.3     0.4     0.8  plot(cvfit) # to see CV performance as a function of alpha  predict(cvfit, xtest, groupstest, s=\"lambda.min\") # to predict with held out data #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, groupstest = groupstest,   #>     s = \"lambda.min\")  #>  #>  #> alpha =  1  #>  #> Support size: #>                                        #> Overall    0                           #> Pretrain   1 (0 common + 1 individual) #> Individual 1                           predict(cvfit, xtest, groupstest, s=\"lambda.min\", ytest=ytest) # to also measure performance #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest, s = \"lambda.min\")  #>  #>  #> alpha =  1  #>  #> Performance (Mean squared error): #>  #>            allGroups  mean group_1 group_2 group_3 group_4 group_5      r^2 #> Overall        1.223 1.223   1.131  0.7117  0.7803   1.809   1.682 -0.02313 #> Pretrain       1.223 1.223   1.131  0.7104  0.7803   1.809   1.682 -0.02292 #> Individual     1.223 1.223   1.131  0.7104  0.7803   1.809   1.682 -0.02292 #>  #> Support size: #>                                        #> Overall    0                           #> Pretrain   1 (0 common + 1 individual) #> Individual 1                            # By default, we used s = \"lambda.min\" to compute CV performance. # We could instead use s = \"lambda.1se\": cvfit = cv.ptLasso(x, y, groups = groups, family = \"gaussian\", nfolds=3,                     type.measure = \"mse\", s = \"lambda.1se\")  # \\donttest{ # We could have used the glmnet option relax = TRUE: cvfit = cv.ptLasso(x, y, groups = groups, family = \"gaussian\", nfolds=3,                     type.measure = \"mse\", relax = TRUE) # And, as we did with lambda, we may want to specify the choice of gamma to compute CV performance: cvfit = cv.ptLasso(x, y, groups = groups, family = \"gaussian\", nfolds=3,                     type.measure = \"mse\", relax = TRUE, gamma = \"gamma.1se\") # } # Note that the first stage of pretraining uses \"lambda.1se\" and \"gamma.1se\" by default. # This behavior can be modified by specifying overall.lambda and overall.gamma; # see the documentation for ptLasso for more information.  # \\donttest{ # Now, we are ready to simulate slightly more realistic data. # This continuous outcome example has k = 5 groups, where each group has 200 observations. # There are scommon = 10 features shared across all groups, and # sindiv = 10 features unique to each group. # n = 1000 and p = 120 (60 informative features and 60 noise features). # The coefficients of the common features differ across groups (beta.common). # In group 1, these coefficients are rep(1, 10); in group 2 they are rep(2, 10), etc. # Each group has 10 unique features, the coefficients of which are all 3 (beta.indiv). # The intercept in all groups is 0. # The variable sigma = 20 indicates that we add noise to y according to 20 * rnorm(n).  set.seed(1234) k=5 class.sizes=rep(200, k) scommon=10; sindiv=rep(10, k) n=sum(class.sizes); p=2*(sum(sindiv) + scommon) beta.common=3*(1:k); beta.indiv=rep(3, k) intercepts=rep(0, k) sigma=20 out = gaussian.example.data(k=k, class.sizes=class.sizes,                             scommon=scommon, sindiv=sindiv,                             n=n, p=p,                             beta.common=beta.common, beta.indiv=beta.indiv,                             intercepts=intercepts, sigma=20) x = out$x; y=out$y; groups = out$group  outtest = gaussian.example.data(k=k, class.sizes=class.sizes,                                 scommon=scommon, sindiv=sindiv,                                 n=n, p=p,                                 beta.common=beta.common, beta.indiv=beta.indiv,                                 intercepts=intercepts, sigma=20) xtest=outtest$x; ytest=outtest$y; groupstest=outtest$groups  cvfit = cv.ptLasso(x, y, groups = groups, family = \"gaussian\", type.measure = \"mse\") cvfit #>  #> Call:   #> cv.ptLasso(x = x, y = y, groups = groups, family = \"gaussian\",   #>     type.measure = \"mse\", use.case = \"inputGroups\", group.intercepts = TRUE)  #>  #>  #>  #> type.measure:  mse  #>  #>  #>            alpha overall  mean wtdMean group_1 group_2 group_3 group_4 group_5 #> Overall            699.7 699.7   699.7   748.4   501.9   575.6   663.0  1009.9 #> Pretrain     0.0   518.6 518.6   518.6   470.1   471.5   547.0   540.7   563.7 #> Pretrain     0.1   506.0 506.0   506.0   429.7   452.1   538.7   551.1   558.3 #> Pretrain     0.2   495.3 495.3   495.3   393.6   460.6   565.5   530.9   526.1 #> Pretrain     0.3   490.4 490.4   490.4   390.4   436.5   546.3   511.6   567.4 #> Pretrain     0.4   487.5 487.5   487.5   383.7   438.8   545.6   509.4   560.3 #> Pretrain     0.5   481.2 481.2   481.2   364.9   429.7   548.5   513.4   549.7 #> Pretrain     0.6   504.1 504.1   504.1   393.1   460.0   586.4   531.9   549.0 #> Pretrain     0.7   511.5 511.5   511.5   393.2   462.7   584.3   492.9   624.3 #> Pretrain     0.8   509.1 509.1   509.1   382.4   496.2   597.9   503.4   565.6 #> Pretrain     0.9   501.5 501.5   501.5   404.0   481.6   581.9   488.3   552.0 #> Pretrain     1.0   517.1 517.1   517.1   409.1   488.9   612.7   484.7   590.1 #> Individual         517.1 517.1   517.1   409.1   488.9   612.7   484.7   590.1 #>  #> alphahat (fixed) = 0.5 #> alphahat (varying): #> group_1 group_2 group_3 group_4 group_5  #>     0.5     0.5     0.1     1.0     0.2  # plot(cvfit) # to see CV performance as a function of alpha  predict(cvfit, xtest, groupstest, ytest=ytest, s=\"lambda.min\") #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest, s = \"lambda.min\")  #>  #>  #> alpha =  0.5  #>  #> Performance (Mean squared error): #>  #>            allGroups  mean group_1 group_2 group_3 group_4 group_5    r^2 #> Overall        755.7 755.7   836.0   554.9   565.4   777.9  1044.0 0.5371 #> Pretrain       500.2 500.2   539.0   443.8   553.5   502.5   462.4 0.6936 #> Individual     532.8 532.8   584.1   443.2   567.2   550.5   518.9 0.6736 #>  #> Support size: #>                                           #> Overall    64                             #> Pretrain   92 (21 common + 71 individual) #> Individual 109                            # }  # \\donttest{ # Now, we repeat with a binomial outcome. # This example has k = 3 groups, where each group has 100 observations. # There are scommon = 5 features shared across all groups, and # sindiv = 5 features unique to each group. # n = 300 and p = 40 (20 informative features and 20 noise features). # The coefficients of the common features differ across groups (beta.common), # as do the coefficients specific to each group (beta.indiv). set.seed(1234) k=3 class.sizes=rep(100, k) scommon=5; sindiv=rep(5, k) n=sum(class.sizes); p=2*(sum(sindiv) + scommon) beta.common=list(c(-.5, .5, .3, -.9, .1), c(-.3, .9, .1, -.1, .2), c(0.1, .2, -.1, .2, .3)) beta.indiv = lapply(1:k, function(i)  0.9 * beta.common[[i]])  out = binomial.example.data(k=k, class.sizes=class.sizes,                             scommon=scommon, sindiv=sindiv,                             n=n, p=p,                             beta.common=beta.common, beta.indiv=beta.indiv) x = out$x; y=out$y; groups = out$group  outtest = binomial.example.data(k=k, class.sizes=class.sizes,                                 scommon=scommon, sindiv=sindiv,                                 n=n, p=p,                                 beta.common=beta.common, beta.indiv=beta.indiv) xtest=outtest$x; ytest=outtest$y; groupstest=outtest$groups  cvfit = cv.ptLasso(x, y, groups = groups, family = \"binomial\",                    type.measure = \"auc\", nfolds=3, verbose=TRUE,                     alpha = c(0, .5, 1),                    alphahat.choice=\"mean\") #>  #> alpha= 0 #> Fitting overall model #> Fitting individual models #> \tFitting individual model 1 / 3 #> \tFitting individual model 2 / 3 #> \tFitting individual model 3 / 3 #> Fitting pretrained lasso models #> \tFitting pretrained model 1 / 3 #> \tFitting pretrained model 2 / 3 #> \tFitting pretrained model 3 / 3 #>  #> alpha= 0.5 #> Fitting pretrained lasso models #> \tFitting pretrained model 1 / 3 #> \tFitting pretrained model 2 / 3 #> \tFitting pretrained model 3 / 3 #>  #> alpha= 1 #> Fitting pretrained lasso models cvfit #>  #> Call:   #> cv.ptLasso(x = x, y = y, groups = groups, alphalist = c(0, 0.5,   #>     1), family = \"binomial\", type.measure = \"auc\", nfolds = 3,   #>     verbose = TRUE, alphahat.choice = \"mean\", use.case = \"inputGroups\",   #>     group.intercepts = TRUE)  #>  #>  #> type.measure:  auc  #>  #>  #>            alpha overall   mean wtdMean group_1 group_2 group_3 #> Overall           0.5715 0.5717  0.5717  0.6933  0.5994  0.4223 #> Pretrain     0.0  0.6272 0.6242  0.6242  0.8001  0.6819  0.3905 #> Pretrain     0.5  0.7284 0.7623  0.7623  0.8472  0.7636  0.6762 #> Pretrain     1.0  0.5882 0.6894  0.6894  0.7896  0.7112  0.5673 #> Individual        0.5882 0.6894  0.6894  0.7896  0.7112  0.5673 #>  #> alphahat (fixed) = 0.5 #> alphahat (varying): #> group_1 group_2 group_3  #>     0.5     0.5     0.5  # plot(cvfit) # to see CV performance as a function of alpha  predict(cvfit, xtest, groupstest, ytest=ytest, s=\"lambda.1se\") #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest, s = \"lambda.1se\")  #>  #>  #> alpha =  0.5  #>  #> Performance (AUC): #>  #>            allGroups   mean wtdMean group_1 group_2 group_3 #> Overall       0.6070 0.6072  0.6072  0.5985  0.6764  0.5467 #> Pretrain      0.6391 0.6440  0.6440  0.6720  0.7166  0.5435 #> Individual    0.6762 0.6435  0.6435  0.6663  0.7644  0.5000 #>  #> Support size: #>                                          #> Overall    3                             #> Pretrain   21 (3 common + 18 individual) #> Individual 7                             # }  if (FALSE) { # \\dontrun{ ### Model fitting with parallel = TRUE require(doMC) registerDoMC(cores = 4) cvfit = cv.ptLasso(x, y, groups = groups, family = \"binomial\",                    type.measure = \"auc\", parallel=TRUE) } # } # \\donttest{ # Multiresponse pretraining # Now let's consider the case of a multiresponse outcome. We'll start by simulating data: set.seed(1234) n = 1000; ntrain = 500; p = 500 sigma = 2       x = matrix(rnorm(n*p), n, p) beta1 = c(rep(1, 5), rep(0.5, 5), rep(0, p - 10)) beta2 = c(rep(1, 5), rep(0, 5), rep(0.5, 5), rep(0, p - 15))  mu = cbind(x %*% beta1, x %*% beta2) y  = cbind(mu[, 1] + sigma * rnorm(n),             mu[, 2] + sigma * rnorm(n)) cat(\"SNR for the two tasks:\", round(diag(var(mu)/var(y-mu)), 2), fill=TRUE) #> SNR for the two tasks: 1.6 1.44 cat(\"Correlation between two tasks:\", cor(y[, 1], y[, 2]), fill=TRUE) #> Correlation between two tasks: 0.5164748  xtest = x[-(1:ntrain), ] ytest = y[-(1:ntrain), ]  x = x[1:ntrain, ] y = y[1:ntrain, ]  # Now, we can fit a ptLasso model: fit = cv.ptLasso(x, y, type.measure = \"mse\", use.case = \"multiresponse\") plot(fit) # to see the cv curve. predict(fit, xtest) # to predict with new data #>  #> Call:   #> predict.cv.ptLasso(object = fit, xtest = xtest)  #>  #>  #>  #> alpha =  0.2  #>  #> Support size: #>                                          #> Overall    57                            #> Pretrain   23 (19 common + 4 individual) #> Individual 80                            predict(fit, xtest, ytest=ytest) # if ytest is included, we also measure performance #>  #> Call:   #> predict.cv.ptLasso(object = fit, xtest = xtest, ytest = ytest)  #>  #>  #>  #> alpha =  0.2  #>  #> Performance (Mean squared error): #>  #>            allGroups  mean response_1 response_2 #> Overall        9.394 4.697      4.227      5.168 #> Pretrain       8.907 4.453      4.186      4.721 #> Individual     9.465 4.733      4.243      5.222 #>  #> Support size: #>                                          #> Overall    57                            #> Pretrain   23 (19 common + 4 individual) #> Individual 80                            # By default, we used s = \"lambda.min\" to compute CV performance. # We could instead use s = \"lambda.1se\": cvfit = cv.ptLasso(x, y, type.measure = \"mse\", s = \"lambda.1se\", use.case = \"multiresponse\")  # We could also use the glmnet option relax = TRUE: cvfit = cv.ptLasso(x, y, type.measure = \"mse\", relax = TRUE, use.case = \"multiresponse\") # And, as we did with lambda, we may want to specify the choice of gamma to compute CV performance: cvfit = cv.ptLasso(x, y, type.measure = \"mse\", relax = TRUE, gamma = \"gamma.1se\",                    use.case = \"multiresponse\") # }  # \\donttest{ # Time series pretraining # Now suppose we have time series data with a binomial outcome measured at 3 different time points. set.seed(1234) n = 600; ntrain = 300; p = 50 x = matrix(rnorm(n*p), n, p)  beta1 = c(rep(0.25, 10), rep(0, p-10)) beta2 = beta1 + c(rep(0.1, 10), runif(5, min = -0.25, max = 0), rep(0, p-15)) beta3 = beta1 + c(rep(0.2, 10), runif(5, min = -0.25, max = 0),                   runif(5, min = 0, max = 0.1), rep(0, p-20))  y1 = rbinom(n, 1, prob = 1/(1 + exp(-x %*% beta1))) y2 = rbinom(n, 1, prob = 1/(1 + exp(-x %*% beta2))) y3 = rbinom(n, 1, prob = 1/(1 + exp(-x %*% beta3))) y = cbind(y1, y2, y3)  xtest = x[-(1:ntrain), ] ytest = y[-(1:ntrain), ]  x = x[1:ntrain, ] y = y[1:ntrain, ]  cvfit =  cv.ptLasso(x, y, use.case=\"timeSeries\", family=\"binomial\",                     type.measure=\"auc\") plot(cvfit, plot.alphahat = TRUE) predict(cvfit, xtest, ytest=ytest) #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, ytest = ytest)  #>  #>  #>  #> alpha =  0.8  #>  #> Performance (AUC): #>  #>              mean response_1 response_2 response_3 #> Pretrain   0.7093     0.6731     0.7165     0.7383 #> Individual 0.7042     0.6731     0.7116     0.7279 #>  #> Support size: #>                                           #> Pretrain   39 (21 common + 18 individual) #> Individual 39                              # The glmnet option relax = TRUE: cvfit = cv.ptLasso(x, y, type.measure = \"auc\", family = \"binomial\", relax = TRUE,                    use.case = \"timeSeries\") # }"},{"path":"https://erincr.github.io/ptLasso/reference/gaussian.example.data.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulate input grouped data (gaussian outcome) for testing with ptLasso. — gaussian.example.data","title":"Simulate input grouped data (gaussian outcome) for testing with ptLasso. — gaussian.example.data","text":"required arguments; used primarily documentation. Simply calls makedata reasonable set features.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/gaussian.example.data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulate input grouped data (gaussian outcome) for testing with ptLasso. — gaussian.example.data","text":"","code":"gaussian.example.data(   k = 5,   class.sizes = rep(200, k),   n = sum(class.sizes),   scommon = 10,   sindiv = rep(10, k),   p = 2 * (sum(sindiv) + scommon),   beta.common = 3 * (1:k),   beta.indiv = rep(3, k),   intercepts = rep(0, k),   sigma = 20 )"},{"path":"https://erincr.github.io/ptLasso/reference/gaussian.example.data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulate input grouped data (gaussian outcome) for testing with ptLasso. — gaussian.example.data","text":"k Default: 5. class.sizes Default: rep(200, k). n Default: sum(class.sizes). scommon Default: 10. sindiv Default: rep(10, k). p Default: 2*(sum(sindiv) + scommon). beta.common Default: 3*(1:k). beta.indiv Default: rep(3, k). intercepts Default: rep(0, k). sigma Default: 20.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/gaussian.example.data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simulate input grouped data (gaussian outcome) for testing with ptLasso. — gaussian.example.data","text":"list data 5 groups gaussian outcome, n=1000 p=120: x Simulated features, size n x p. y Outcomes y, length n. groups Vector length n, indicating observations belong group. snr Gaussian outcome : signal noise ratio. mu Gaussian outcome : value y noise added.","code":""},{"path":[]},{"path":"https://erincr.github.io/ptLasso/reference/gaussian.example.data.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Simulate input grouped data (gaussian outcome) for testing with ptLasso. — gaussian.example.data","text":"Erin Craig Rob Tibshirani Maintainer: Erin Craig <erincr@stanford.edu>","code":""},{"path":"https://erincr.github.io/ptLasso/reference/gaussian.example.data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulate input grouped data (gaussian outcome) for testing with ptLasso. — gaussian.example.data","text":"","code":"out = gaussian.example.data() x = out$x; y=out$y; groups = out$group"},{"path":"https://erincr.github.io/ptLasso/reference/get.individual.support.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the support for individual models — get.individual.support","title":"Get the support for individual models — get.individual.support","text":"Get indices nonzero coefficients individual models fitted ptLasso cv.ptLasso object, excluding intercept.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/get.individual.support.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the support for individual models — get.individual.support","text":"","code":"get.individual.support(   fit,   s = \"lambda.min\",   gamma = \"gamma.min\",   commonOnly = FALSE,   groups = 1:length(fit$fitind) )"},{"path":"https://erincr.github.io/ptLasso/reference/get.individual.support.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the support for individual models — get.individual.support","text":"fit fitted \"ptLasso\" \"cv.ptLasso\" object. s choice lambda use. May \"lambda.min\", \"lambda.1se\" numeric value. Default \"lambda.min\". gamma use 'relax = TRUE' specified training. choice 'gamma' use. May \"gamma.min\" \"gamma.1se\". Default \"gamma.min\". commonOnly whether return features chosen half group- response-specific models (TRUE) features chosen group-specific models (FALSE). Default FALSE. groups groups responses include computing support. Default include groups/responses.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/get.individual.support.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the support for individual models — get.individual.support","text":"returns vector containing indices nonzero coefficients (excluding intercept).","code":""},{"path":[]},{"path":"https://erincr.github.io/ptLasso/reference/get.individual.support.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Get the support for individual models — get.individual.support","text":"Erin Craig Rob Tibshirani Maintainer: Erin Craig <erincr@stanford.edu>","code":""},{"path":"https://erincr.github.io/ptLasso/reference/get.individual.support.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get the support for individual models — get.individual.support","text":"","code":"# Train data set.seed(1234) out = gaussian.example.data() x = out$x; y=out$y; groups = out$group;  fit = ptLasso(x, y, groups = groups, family = \"gaussian\", type.measure = \"mse\")  get.individual.support(fit)  #>   [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18 #>  [19]  19  20  21  22  23  25  27  28  29  30  31  32  34  35  36  37  38  39 #>  [37]  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57 #>  [55]  58  59  60  62  63  64  65  66  67  68  69  70  72  73  75  76  77  78 #>  [73]  79  80  82  83  84  87  88  89  90  91  92  93  94  95  98  99 100 101 #>  [91] 102 103 104 105 106 107 108 109 110 111 112 113 114 115 117 118 119 120  # only return features common to all groups  get.individual.support(fit, commonOnly = TRUE)  #>  [1]   1   2   3   4   5   6   7   8   9  10  11  13  14  37  38  48  51  54  60 #> [20]  65  67  73  80  82  83 108 118  # group 1 only get.individual.support(fit, groups = 1)  #>  [1]   1   2   3   4   5   6   7   8   9  10  11  13  14  15  16  18  20  22  29 #> [20]  31  32  34  37  38  47  48  50  54  57  58  59  60  62  64  66  67  70  72 #> [39]  73  77  79  80  83  88  89  91  93  94  95 101 103 104 106 108 111 113 114 #> [58] 115 117 118 119  cvfit = cv.ptLasso(x, y, groups = groups, alphalist = c(0, .5, 1),                     family = \"gaussian\", type.measure = \"mse\")  get.individual.support(cvfit) #>   [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18 #>  [19]  19  20  21  22  23  25  27  28  29  30  31  32  34  35  36  37  38  39 #>  [37]  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57 #>  [55]  58  59  60  61  62  63  64  65  66  67  68  69  70  72  73  75  76  77 #>  [73]  78  79  80  82  83  84  87  88  89  90  91  92  93  94  95  98  99 100 #>  [91] 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 117 118 119 #> [109] 120  # group 1 only get.individual.support(cvfit, groups = 1)  #>  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  18  19  20 #> [20]  22  29  31  32  34  37  38  43  47  48  50  54  56  57  58  59  60  62  64 #> [39]  65  66  67  70  72  73  75  77  79  80  83  88  89  90  91  93  94  95 101 #> [58] 103 104 106 108 111 113 114 115 117 118 119"},{"path":"https://erincr.github.io/ptLasso/reference/get.overall.support.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the support for the overall model — get.overall.support","title":"Get the support for the overall model — get.overall.support","text":"Get indices nonzero coefficients overall model fitted ptLasso cv.ptLasso object, excluding intercept.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/get.overall.support.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the support for the overall model — get.overall.support","text":"","code":"get.overall.support(fit, s = \"lambda.min\", gamma = \"gamma.min\")"},{"path":"https://erincr.github.io/ptLasso/reference/get.overall.support.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the support for the overall model — get.overall.support","text":"fit fitted \"ptLasso\" \"cv.ptLasso\" object. s choice lambda use. May \"lambda.min\", \"lambda.1se\" numeric value. Default \"lambda.min\". gamma use 'relax = TRUE' specified training. choice 'gamma' use. May \"gamma.min\" \"gamma.1se\". Default \"gamma.min\".","code":""},{"path":"https://erincr.github.io/ptLasso/reference/get.overall.support.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the support for the overall model — get.overall.support","text":"returns vector containing indices nonzero coefficients (excluding intercept).","code":""},{"path":[]},{"path":"https://erincr.github.io/ptLasso/reference/get.overall.support.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Get the support for the overall model — get.overall.support","text":"Erin Craig Rob Tibshirani Maintainer: Erin Craig <erincr@stanford.edu>","code":""},{"path":"https://erincr.github.io/ptLasso/reference/get.overall.support.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get the support for the overall model — get.overall.support","text":"","code":"# Train data set.seed(1234) out = gaussian.example.data(k=2, class.sizes = c(50, 50)) x = out$x; y=out$y; groups = out$group;  fit = ptLasso(x, y, groups = groups, family = \"gaussian\", type.measure = \"mse\")  get.overall.support(fit, s=\"lambda.min\")  #>  [1]  1  2  4  5  6  7  8  9 10 11 12 13 26 29 31 36 40 45 46 58 59 get.overall.support(fit, s=\"lambda.1se\")  #> [1] 4 5 7  cvfit = cv.ptLasso(x, y, groups = groups, family = \"gaussian\", type.measure = \"mse\")  get.overall.support(cvfit, s=\"lambda.min\")  #>  [1]  1  2  4  5  6  7 10 11 31 45 46 58 59 get.overall.support(cvfit, s=\"lambda.1se\")  #> [1] 4 5 7"},{"path":"https://erincr.github.io/ptLasso/reference/get.pretrain.support.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the support for pretrained models — get.pretrain.support","title":"Get the support for pretrained models — get.pretrain.support","text":"Get indices nonzero coefficients pretrained models fitted ptLasso cv.ptLasso object, excluding intercept.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/get.pretrain.support.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the support for pretrained models — get.pretrain.support","text":"","code":"get.pretrain.support(   fit,   s = \"lambda.min\",   gamma = \"gamma.min\",   commonOnly = FALSE,   includeOverall = TRUE,   groups = 1:length(fit$fitind) )"},{"path":"https://erincr.github.io/ptLasso/reference/get.pretrain.support.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the support for pretrained models — get.pretrain.support","text":"fit fitted \"ptLasso\" \"cv.ptLasso\" object. s choice lambda use. May \"lambda.min\", \"lambda.1se\" numeric value. Default \"lambda.min\". gamma use 'relax = TRUE' specified training. choice 'gamma' use. May \"gamma.min\" \"gamma.1se\". Default \"gamma.min\". commonOnly whether return features chosen half group- response-specific models (TRUE) features chosen group-specific models (FALSE). Default FALSE. includeOverall whether return features chosen overall model group-specific models (TRUE) features chosen overall model group-specific models (FALSE). Default TRUE. used 'use.case = \"timeSeries\"'. groups groups responses include computing support. Default include groups/responses.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/get.pretrain.support.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the support for pretrained models — get.pretrain.support","text":"ptLasso object supplied, returns vector containing indices nonzero coefficients (excluding intercept). cv.ptLasso object supplied, returns list results - one value alpha.","code":""},{"path":[]},{"path":"https://erincr.github.io/ptLasso/reference/get.pretrain.support.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get the support for pretrained models — get.pretrain.support","text":"","code":"# Train data set.seed(1234) out = gaussian.example.data(k=2, class.sizes = c(50, 50)) x = out$x; y=out$y; groups = out$group;  fit = ptLasso(x, y, groups = groups, family = \"gaussian\", type.measure = \"mse\")  get.pretrain.support(fit)  #>  [1]  1  4  5  6  7 11 40 45 46 58  # only return features common to all groups  get.pretrain.support(fit, commonOnly = TRUE)  #> [1] 4 5 7  # group 1 only, don't include the overall model support get.pretrain.support(fit, groups = 1, includeOverall = FALSE)  #> integer(0)  # group 1 only, include the overall model support get.pretrain.support(fit, groups = 1, includeOverall = TRUE)  #> [1] 4 5 7  cvfit = cv.ptLasso(x, y, groups = groups, family = \"gaussian\", type.measure = \"mse\")  get.pretrain.support(cvfit) #> [[1]] #> [1] 4 5 7 #>  #> [[2]] #> [1]  4  5  6  7 11 45 58 #>  #> [[3]] #>  [1]  1  2  4  5  6  7  8 11 26 40 45 46 58 #>  #> [[4]] #>  [1]  1  2  4  5  6  7 11 26 40 45 46 58 #>  #> [[5]] #>  [1]  1  4  5  6  7 11 40 45 46 58 #>  #> [[6]] #>  [1]  1  4  5  6  7 11 40 45 46 58 #>  #> [[7]] #>  [1]  1  2  4  5  6  7  8 11 26 40 45 46 58 #>  #> [[8]] #>  [1]  1  2  4  5  6  7  8 11 26 40 45 46 58 #>  #> [[9]] #>  [1]  1  2  4  5  6  7  8 11 26 40 45 46 58 #>  #> [[10]] #>  [1]  1  2  4  5  6  7  8  9 10 11 13 22 25 26 28 31 32 34 40 42 45 46 51 58 #>  #> [[11]] #>  [1]  1  2  4  5  6  7  8  9 10 11 12 13 22 25 26 28 31 32 34 40 41 42 45 46 51 #> [26] 54 58 #>  get.pretrain.support(cvfit, groups = 1)  #> [[1]] #> [1] 4 5 7 #>  #> [[2]] #> [1] 4 5 7 #>  #> [[3]] #> [1] 4 5 7 #>  #> [[4]] #> [1] 4 5 7 #>  #> [[5]] #> [1] 4 5 7 #>  #> [[6]] #> [1] 4 5 7 #>  #> [[7]] #> [1] 4 5 7 #>  #> [[8]] #> [1] 4 5 7 #>  #> [[9]] #> [1] 4 5 7 #>  #> [[10]] #> [1] 4 5 7 #>  #> [[11]] #> [1]  6 32 54 #>"},{"path":"https://erincr.github.io/ptLasso/reference/makedata.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulate input grouped data for testing with ptLasso. — makedata","title":"Simulate input grouped data for testing with ptLasso. — makedata","text":"Simulate input grouped data testing ptLasso.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/makedata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulate input grouped data for testing with ptLasso. — makedata","text":"","code":"makedata(   n,   p,   k,   scommon,   sindiv,   class.sizes,   beta.common,   beta.indiv,   intercepts = rep(0, k),   sigma = 0,   outcome = c(\"gaussian\", \"binomial\", \"multinomial\"),   mult.classes = 3 )"},{"path":"https://erincr.github.io/ptLasso/reference/makedata.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulate input grouped data for testing with ptLasso. — makedata","text":"n Total number observations simulate. p Total number features simulate. k Number groups. scommon Number features shared groups. sindiv Vector length k. ^th entry indicates number features specific group . class.sizes Vector length k. ^th entry indicates number observations group . beta.common coefficients common features. can vector length k, case, ^th entry coefficient scommon features group . can alternatively list length k (one group). entry list vector length scommon, containing coefficients scommon features. beta.indiv coefficients individual features, form beta.common. intercepts vector length k, indicating intercept group. Default 0. sigma used Gaussian outcome. number greater equal 0, used modify amount noise added. Default 0. outcome May '\"gaussian\"', '\"binomial\"' '\"multinomial\"'. mult.classes Number classes simulate multinomial setting.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/makedata.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simulate input grouped data for testing with ptLasso. — makedata","text":"list: x Simulated features, size n x p. y Outcomes y, length n. groups Vector length n, indicating observations belong group. snr Gaussian outcome : signal noise ratio. mu Gaussian outcome : value y noise added.","code":""},{"path":[]},{"path":"https://erincr.github.io/ptLasso/reference/makedata.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Simulate input grouped data for testing with ptLasso. — makedata","text":"Erin Craig Rob Tibshirani Maintainer: Erin Craig <erincr@stanford.edu>","code":""},{"path":"https://erincr.github.io/ptLasso/reference/makedata.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulate input grouped data for testing with ptLasso. — makedata","text":"","code":"# Data with a binary outcome: k = 3 class.sizes = rep(100, k) n = sum(class.sizes) scommon = 5 sindiv = rep(5, k)  p = 2*(sum(sindiv) + scommon) beta.common = lapply(1:k, function(i)  c(-.5, .5, .3, -.9, .1)) beta.indiv = lapply(1:k, function(i)  0.9 * beta.common[[i]])  out = makedata(n=n, p=p, k=k, scommon=scommon, sindiv=sindiv,                beta.common=beta.common, beta.indiv=beta.indiv,                class.sizes=class.sizes, outcome=\"binomial\") x = out$x; y=out$y; groups = out$group"},{"path":"https://erincr.github.io/ptLasso/reference/makedata.targetgroups.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulate target grouped data for testing with ptLasso. — makedata.targetgroups","title":"Simulate target grouped data for testing with ptLasso. — makedata.targetgroups","text":"Simulate target grouped data testing ptLasso.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/makedata.targetgroups.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulate target grouped data for testing with ptLasso. — makedata.targetgroups","text":"","code":"makedata.targetgroups(   n,   p,   scommon,   sindiv,   class.sizes,   shift.common,   shift.indiv )"},{"path":"https://erincr.github.io/ptLasso/reference/makedata.targetgroups.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulate target grouped data for testing with ptLasso. — makedata.targetgroups","text":"n Total number observations simulate. p Total number features simulate. scommon Number features shared groups. sindiv Vector length k. ^th entry indicates number features specific group . class.sizes Vector length k. ^th entry indicates number observations group . shift.common list length k (one group). entry list vector length scommon, containing shifts scommon features. ^th entry list added first scommon columns x observations group . shift.indiv shifts individual features, form shift.common.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/makedata.targetgroups.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simulate target grouped data for testing with ptLasso. — makedata.targetgroups","text":"list: x Simulated features, size n x p. y Outcomes y, length n.","code":""},{"path":[]},{"path":"https://erincr.github.io/ptLasso/reference/makedata.targetgroups.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Simulate target grouped data for testing with ptLasso. — makedata.targetgroups","text":"Erin Craig Rob Tibshirani Maintainer: Erin Craig <erincr@stanford.edu>","code":""},{"path":"https://erincr.github.io/ptLasso/reference/makedata.targetgroups.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulate target grouped data for testing with ptLasso. — makedata.targetgroups","text":"","code":"k = 5 class.sizes = rep(50, k) n = sum(class.sizes) scommon = 3 sindiv = rep(3, k) p = 3*(sum(sindiv) + scommon) shift.common  = lapply(seq(-.1, .1, length.out = k), function(i) rep(i, scommon)) shift.indiv = lapply(1:k, function(i) -shift.common[[i]])  out = makedata.targetgroups(n=n, p=p, scommon=scommon,                             sindiv=sindiv, class.sizes=class.sizes,                             shift.common=shift.common, shift.indiv=shift.indiv) x = out$x; y=out$y"},{"path":"https://erincr.github.io/ptLasso/reference/plot.cv.ptLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot the cross-validation curve produced by cv.ptLasso, as a function of the alpha values used. — plot.cv.ptLasso","title":"Plot the cross-validation curve produced by cv.ptLasso, as a function of the alpha values used. — plot.cv.ptLasso","text":"plot produced, nothing returned.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/plot.cv.ptLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot the cross-validation curve produced by cv.ptLasso, as a function of the alpha values used. — plot.cv.ptLasso","text":"","code":"# S3 method for class 'cv.ptLasso' plot(x, plot.alphahat = FALSE, ...)"},{"path":"https://erincr.github.io/ptLasso/reference/plot.cv.ptLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot the cross-validation curve produced by cv.ptLasso, as a function of the alpha values used. — plot.cv.ptLasso","text":"x Fitted \"cv.ptLasso\" object. plot.alphahat TRUE, show dashed vertical line indicating single value alpha maximized overall cross-validated performance. ... graphical parameters plot.","code":""},{"path":[]},{"path":"https://erincr.github.io/ptLasso/reference/plot.cv.ptLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot the cross-validation curve produced by cv.ptLasso, as a function of the alpha values used. — plot.cv.ptLasso","text":"","code":"set.seed(1234) out = gaussian.example.data(k=2, class.sizes = c(50, 50)) x = out$x; y=out$y; groups = out$group  cvfit = cv.ptLasso(x, y, groups = groups, family = \"gaussian\", type.measure = \"mse\") plot(cvfit)"},{"path":"https://erincr.github.io/ptLasso/reference/plot.ptLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot the models trained by a ptLasso object — plot.ptLasso","title":"Plot the models trained by a ptLasso object — plot.ptLasso","text":"plot produced, nothing returned.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/plot.ptLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot the models trained by a ptLasso object — plot.ptLasso","text":"","code":"# S3 method for class 'ptLasso' plot(x, ...)"},{"path":"https://erincr.github.io/ptLasso/reference/plot.ptLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot the models trained by a ptLasso object — plot.ptLasso","text":"x Fitted \"ptLasso\" object. ... parameters pass plot function.","code":""},{"path":[]},{"path":"https://erincr.github.io/ptLasso/reference/plot.ptLasso.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plot the models trained by a ptLasso object — plot.ptLasso","text":"Erin Craig Rob Tibshirani Maintainer: Erin Craig <erincr@stanford.edu>","code":""},{"path":"https://erincr.github.io/ptLasso/reference/plot.ptLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot the models trained by a ptLasso object — plot.ptLasso","text":"","code":"set.seed(1234) out = gaussian.example.data() x = out$x; y=out$y; groups = out$group  fit = ptLasso(x, y, groups = groups, alpha = 0.5, family = \"gaussian\", type.measure = \"mse\") plot(fit)"},{"path":"https://erincr.github.io/ptLasso/reference/predict.cv.ptLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict using a cv.ptLasso object. — predict.cv.ptLasso","title":"Predict using a cv.ptLasso object. — predict.cv.ptLasso","text":"Return predictions performance measures test set.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/predict.cv.ptLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict using a cv.ptLasso object. — predict.cv.ptLasso","text":"","code":"# S3 method for class 'cv.ptLasso' predict(   object,   xtest,   groupstest = NULL,   ytest = NULL,   alpha = NULL,   alphatype = c(\"fixed\", \"varying\"),   type = c(\"link\", \"response\", \"class\"),   s = \"lambda.min\",   gamma = \"gamma.min\",   return.link = FALSE,   ... )"},{"path":"https://erincr.github.io/ptLasso/reference/predict.cv.ptLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict using a cv.ptLasso object. — predict.cv.ptLasso","text":"object Fitted \"cv.ptLasso\" object. xtest Input matrix, matching form used \"cv.ptLasso\" model training. groupstest vector indicating group observation belongs. Coding match used model training. NULL target grouped data. ytest Response variable. Optional. included, \"predict\" compute performance measures xtest using \"type.measure\" cvfit object. alpha chosen alpha use prediction. May vector containing one value alpha group. NULL, rely choice \"alphatype\". alphatype Choice '\"fixed\"' '\"varying\"'. '\"fixed\"', use alpha achieved best cross-validated performance. '\"varying\"', group uses alpha optimized group-specific cross-validated performance. type Type prediction required. Type '\"link\"' gives linear predictors '\"binomial\", '\"multinomial\"' '\"cox\"' models; '\"gaussian\"' models gives fitted values. Type '\"response\"' gives fitted probabilities '\"binomial\"' '\"multinomial\"', fitted relative-risk '\"cox\"'; '\"gaussian\"' type '\"response\"' equivalent type '\"link\"'. Note '\"binomial\"' models, results returned class corresponding second level factor response. Type '\"class\"' applies '\"binomial\"' '\"multinomial\"' models, produces class label corresponding maximum probability. s Value penalty parameter 'lambda' predictions required. use lambda models; can numeric value, '\"lambda.min\"' '\"lambda.1se\"'. Default '\"lambda.min\"'. gamma use 'relax = TRUE' specified training. Value penalty parameter 'gamma' predictions required. use gamma models; can numeric value, '\"gamma.min\"' '\"gamma.1se\"'. Default '\"gamma.min\"'. return.link TRUE, additionally return linear link overall, pretrained individual models: linkoverall, linkpre linkind. ... arguments passed \"predict\" function.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/predict.cv.ptLasso.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict using a cv.ptLasso object. — predict.cv.ptLasso","text":"list containing requested predictions. ytest included, also return error measures. call call produced object. alpha value(s) alpha used generate predictions. yhatoverall Predictions overall model. yhatind Predictions individual models. yhatpre Predictions pretrained models. supoverall Indices features selected overall model. supind Union indices features selected individual models. suppre.common Features selected first stage pretraining. suppre.individual Union indices features selected pretrained models, without features selected first stage. type.measure ytest supplied, performance measure computed. erroverall ytest supplied, performance overall model. named vector containing performance (1) entire dataset, (2) average performance across groups, (3) average performance across groups weighted group size (4) group-specific performance. errind ytest supplied, performance overall model. described erroverall. errpre ytest supplied, performance overall model. described erroverall. linkoverall return.link TRUE, return linear link overall model. linkind return.link TRUE, return linear link individual models. linkpre return.link TRUE, return linear link pretrained models.","code":""},{"path":[]},{"path":"https://erincr.github.io/ptLasso/reference/predict.cv.ptLasso.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Predict using a cv.ptLasso object. — predict.cv.ptLasso","text":"Erin Craig Rob Tibshirani Maintainer: Erin Craig <erincr@stanford.edu>","code":""},{"path":"https://erincr.github.io/ptLasso/reference/predict.cv.ptLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict using a cv.ptLasso object. — predict.cv.ptLasso","text":"","code":"#### Gaussian example set.seed(1234) out = gaussian.example.data(k=2, class.sizes = c(50, 50)) x = out$x; y=out$y; groups = out$group; outtest = gaussian.example.data(k=2, class.sizes = c(50, 50)) xtest=outtest$x; ytest=outtest$y; groupstest=outtest$groups;  # Model fitting # By default, use the single value of alpha that had the best CV performance on the entire dataset: cvfit = cv.ptLasso(x, y, groups = groups, family = \"gaussian\", type.measure = \"mse\") pred = predict(cvfit, xtest, groupstest, ytest=ytest, s=\"lambda.min\") pred #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest, s = \"lambda.min\")  #>  #>  #> alpha =  0.5  #>  #> Performance (Mean squared error): #>  #>            allGroups  mean group_1 group_2    r^2 #> Overall        609.0 609.0   548.3   669.7 0.2330 #> Pretrain       650.8 650.8   601.1   700.6 0.1803 #> Individual     675.2 675.2   667.6   682.8 0.1496 #>  #> Support size: #>                                          #> Overall    21                            #> Pretrain   18 (6 common + 12 individual) #> Individual 26                             # For each group, use the value of alpha that had the best CV performance for that group: pred = predict(cvfit, xtest, groupstest, ytest=ytest, s=\"lambda.min\", alphatype = \"varying\") pred #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest, alphatype = \"varying\", s = \"lambda.min\")  #>  #>  #>  #> alpha: #> [1] 0.1 1.0 #>  #>  #> Performance (Mean squared error): #>            overall  mean wtdMean group_1 group_2 #> Overall      609.0 609.0   609.0   548.3   669.7 #> Pretrain     629.7 629.7   629.7   576.6   682.8 #> Individual   675.2 675.2   675.2   667.6   682.8 #>  #>  #> Support size: #>                                          #> Overall    21                            #> Pretrain   26 (6 common + 20 individual) #> Individual 26                             # Specify a single value of alpha and use lambda.1se. pred = predict(cvfit, xtest, groupstest, ytest=ytest, s=\"lambda.1se\",                alphatype = \"varying\", alpha = .3) pred #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest, alpha = 0.3, alphatype = \"varying\", s = \"lambda.1se\")  #>  #>  #>  #> alpha =  0.3  #>  #> Performance (Mean squared error): #>  #>            allGroups  mean group_1 group_2     r^2 #> Overall        709.1 709.1   573.8   844.5 0.10690 #> Pretrain       660.6 660.6   585.7   735.5 0.16800 #> Individual     743.8 743.8   667.6   820.0 0.06322 #>  #> Support size: #>                                        #> Overall    6                           #> Pretrain   7 (6 common + 1 individual) #> Individual 10                           # Specify a vector of choices for alpha:  pred = predict(cvfit, xtest, groupstest, ytest=ytest, s=\"lambda.min\",                alphatype = \"varying\", alpha = c(.1, .5)) pred #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest, alpha = c(0.1, 0.5), alphatype = \"varying\",   #>     s = \"lambda.min\")  #>  #>  #> alpha: #> [1] 0.1 0.5 #>  #>  #> Performance (Mean squared error): #>            overall  mean wtdMean group_1 group_2 #> Overall      609.0 609.0   609.0   548.3   669.7 #> Pretrain     638.6 638.6   638.6   576.6   700.6 #> Individual   675.2 675.2   675.2   667.6   682.8 #>  #>  #> Support size: #>                                          #> Overall    21                            #> Pretrain   18 (6 common + 12 individual) #> Individual 26"},{"path":"https://erincr.github.io/ptLasso/reference/predict.ptLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict using a ptLasso object. — predict.ptLasso","title":"Predict using a ptLasso object. — predict.ptLasso","text":"Return predictions performance measures test set.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/predict.ptLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict using a ptLasso object. — predict.ptLasso","text":"","code":"# S3 method for class 'ptLasso' predict(   object,   xtest,   groupstest = NULL,   ytest = NULL,   type = c(\"link\", \"response\", \"class\"),   s = \"lambda.min\",   gamma = \"gamma.min\",   return.link = FALSE,   ... )"},{"path":"https://erincr.github.io/ptLasso/reference/predict.ptLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict using a ptLasso object. — predict.ptLasso","text":"object Fitted \"ptLasso\" object. xtest Input matrix, matching form used \"ptLasso\" model training. groupstest vector indicating group observation belongs. Coding match used model training. NULL target grouped data. ytest Response variable. Optional. included, \"predict\" compute performance measures xtest using \"type.measure\" cvfit object. type Type prediction required. Type '\"link\"' gives linear predictors '\"binomial\", '\"multinomial\"' '\"cox\"' models; '\"gaussian\"' models gives fitted values. Type '\"response\"' gives fitted probabilities '\"binomial\"' '\"multinomial\"', fitted relative-risk '\"cox\"'; '\"gaussian\"' type '\"response\"' equivalent type '\"link\"'. Note '\"binomial\"' models, results returned class corresponding second level factor response. Type '\"class\"' applies '\"binomial\"' '\"multinomial\"' models, produces class label corresponding maximum probability. s Value penalty parameter 'lambda' predictions required. use lambda models; can numeric value, '\"lambda.min\"' '\"lambda.1se\"'. Default '\"lambda.min\"'. gamma use 'relax = TRUE' specified training. Value penalty parameter 'gamma' predictions required. use gamma models; can numeric value, '\"gamma.min\"' '\"gamma.1se\"'. Default '\"gamma.min\"'. return.link TRUE, additionally return linear link overall, pretrained individual models: linkoverall, linkpre linkind. ... arguments passed \"predict\" function.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/predict.ptLasso.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict using a ptLasso object. — predict.ptLasso","text":"list containing requested predictions. ytest included, also return error measures. call call produced object. alpha value(s) alpha used generate predictions. alpha used model training. yhatoverall Predictions overall model. yhatind Predictions individual models. yhatpre Predictions pretrained models. supoverall Indices features selected overall model. supind Union indices features selected individual models. suppre.common Features selected first stage pretraining. suppre.individual Union indices features selected pretrained models, without features selected first stage. type.measure ytest supplied, string name computed performance measure. erroverall ytest supplied, performance overall model. named vector containing performance (1) entire dataset, (2) average performance across groups, (3) average performance across groups weighted group size (4) group-specific performance. errind ytest supplied, performance overall model. described erroverall. errpre ytest supplied, performance overall model. described erroverall. linkoverall Ifreturn.link TRUE, return linear link overall model. linkind Ifreturn.link TRUE, return linear link individual models. linkpre Ifreturn.link TRUE, return linear link pretrained models.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/predict.ptLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict using a ptLasso object. — predict.ptLasso","text":"","code":"# Gaussian example set.seed(1234) out = gaussian.example.data() x = out$x; y=out$y; groups = out$group  outtest = gaussian.example.data() xtest=outtest$x; ytest=outtest$y; groupstest=outtest$groups  fit = ptLasso(x, y, groups = groups, alpha = 0.5, family = \"gaussian\", type.measure = \"mse\") pred = predict(fit, xtest, groupstest, ytest=ytest) pred #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest)  #>  #>  #> alpha =  0.5  #>  #> Performance (Mean squared error): #>  #>            allGroups  mean group_1 group_2 group_3 group_4 group_5    r^2 #> Overall        755.7 755.7   836.0   554.9   565.4   777.9  1044.0 0.5371 #> Pretrain       503.2 503.2   550.6   443.3   553.5   505.6   462.9 0.6918 #> Individual     532.8 532.8   584.1   443.2   567.2   550.5   518.9 0.6736 #>  #> Support size: #>                                           #> Overall    64                             #> Pretrain   94 (21 common + 73 individual) #> Individual 109"},{"path":"https://erincr.github.io/ptLasso/reference/print.cv.ptLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"Print the cv.ptLasso object. — print.cv.ptLasso","title":"Print the cv.ptLasso object. — print.cv.ptLasso","text":"Print cv.ptLasso object.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/print.cv.ptLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print the cv.ptLasso object. — print.cv.ptLasso","text":"","code":"# S3 method for class 'cv.ptLasso' print(x, ...)"},{"path":"https://erincr.github.io/ptLasso/reference/print.cv.ptLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print the cv.ptLasso object. — print.cv.ptLasso","text":"x fitted \"cv.ptLasso\" object. ... arguments pass print function.","code":""},{"path":[]},{"path":"https://erincr.github.io/ptLasso/reference/print.cv.ptLasso.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Print the cv.ptLasso object. — print.cv.ptLasso","text":"Erin Craig Rob Tibshirani Maintainer: Erin Craig <erincr@stanford.edu>","code":""},{"path":"https://erincr.github.io/ptLasso/reference/print.cv.ptLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print the cv.ptLasso object. — print.cv.ptLasso","text":"","code":"out = gaussian.example.data(k=2, class.sizes = c(50, 50)) x = out$x; y=out$y; groups = out$group;  cvfit = cv.ptLasso(x, y, groups = groups, family = \"gaussian\", type.measure = \"mse\") print(cvfit) #>  #> Call:   #> cv.ptLasso(x = x, y = y, groups = groups, family = \"gaussian\",   #>     type.measure = \"mse\", use.case = \"inputGroups\", group.intercepts = TRUE)  #>  #>  #>  #> type.measure:  mse  #>  #>  #>            alpha overall  mean wtdMean group_1 group_2 #> Overall            543.1 543.1   543.1   477.2   609.0 #> Pretrain     0.0   520.0 520.0   520.0   507.3   532.6 #> Pretrain     0.1   503.9 503.9   503.9   511.5   496.2 #> Pretrain     0.2   468.4 468.4   468.4   449.4   487.4 #> Pretrain     0.3   500.0 500.0   500.0   543.7   456.3 #> Pretrain     0.4   476.7 476.7   476.7   427.1   526.3 #> Pretrain     0.5   456.6 456.6   456.6   439.6   473.6 #> Pretrain     0.6   436.9 436.9   436.9   420.5   453.3 #> Pretrain     0.7   434.7 434.7   434.7   386.6   482.8 #> Pretrain     0.8   474.2 474.2   474.2   421.3   527.1 #> Pretrain     0.9   478.9 478.9   478.9   406.4   551.4 #> Pretrain     1.0   469.7 469.7   469.7   411.4   528.0 #> Individual         469.7 469.7   469.7   411.4   528.0 #>  #> alphahat (fixed) = 0.7 #> alphahat (varying): #> group_1 group_2  #>     0.7     0.6"},{"path":"https://erincr.github.io/ptLasso/reference/print.predict.cv.ptLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"Print the predict.cv.ptLasso object. — print.predict.cv.ptLasso","title":"Print the predict.cv.ptLasso object. — print.predict.cv.ptLasso","text":"Print predict.cv.ptLasso object.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/print.predict.cv.ptLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print the predict.cv.ptLasso object. — print.predict.cv.ptLasso","text":"","code":"# S3 method for class 'predict.cv.ptLasso' print(x, ...)"},{"path":"https://erincr.github.io/ptLasso/reference/print.predict.cv.ptLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print the predict.cv.ptLasso object. — print.predict.cv.ptLasso","text":"x output predict called ptLasso object. ... arguments pass print function.","code":""},{"path":[]},{"path":"https://erincr.github.io/ptLasso/reference/print.predict.cv.ptLasso.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Print the predict.cv.ptLasso object. — print.predict.cv.ptLasso","text":"Erin Craig Rob Tibshirani Maintainer: Erin Craig <erincr@stanford.edu>","code":""},{"path":"https://erincr.github.io/ptLasso/reference/print.predict.cv.ptLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print the predict.cv.ptLasso object. — print.predict.cv.ptLasso","text":"","code":"# Train data out = gaussian.example.data(k=2, class.sizes = c(50, 50)) x = out$x; y=out$y; groups = out$group;  # Test data outtest = gaussian.example.data(k=2, class.sizes = c(50, 50)) xtest=outtest$x; ytest=outtest$y; groupstest=outtest$groups  cvfit = cv.ptLasso(x, y, groups = groups, nfolds = 3, family = \"gaussian\", type.measure = \"mse\") pred = predict(cvfit, xtest, groupstest, ytest=ytest, s=\"lambda.min\") print(pred) #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest, s = \"lambda.min\")  #>  #>  #> alpha =  0.8  #>  #> Performance (Mean squared error): #>  #>            allGroups  mean group_1 group_2     r^2 #> Overall        540.5 540.5   528.3   552.8 0.17213 #> Pretrain       579.7 579.7   568.4   590.9 0.11219 #> Individual     598.8 598.8   578.3   619.3 0.08281 #>  #> Support size: #>                                          #> Overall    23                            #> Pretrain   11 (0 common + 11 individual) #> Individual 9                              # If ytest is not supplied, just prints the pretrained predictions. pred = predict(cvfit, xtest, groupstest, s=\"lambda.min\") print(pred) #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, groupstest = groupstest,   #>     s = \"lambda.min\")  #>  #>  #> alpha =  0.8  #>  #> Support size: #>                                          #> Overall    23                            #> Pretrain   11 (0 common + 11 individual) #> Individual 9"},{"path":"https://erincr.github.io/ptLasso/reference/print.predict.ptLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"Print the predict.ptLasso object. — print.predict.ptLasso","title":"Print the predict.ptLasso object. — print.predict.ptLasso","text":"Print predict.ptLasso object.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/print.predict.ptLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print the predict.ptLasso object. — print.predict.ptLasso","text":"","code":"# S3 method for class 'predict.ptLasso' print(x, ...)"},{"path":"https://erincr.github.io/ptLasso/reference/print.predict.ptLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print the predict.ptLasso object. — print.predict.ptLasso","text":"x output predict called ptLasso object. ... arguments pass print function.","code":""},{"path":[]},{"path":"https://erincr.github.io/ptLasso/reference/print.predict.ptLasso.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Print the predict.ptLasso object. — print.predict.ptLasso","text":"Erin Craig Rob Tibshirani Maintainer: Erin Craig <erincr@stanford.edu>","code":""},{"path":"https://erincr.github.io/ptLasso/reference/print.predict.ptLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print the predict.ptLasso object. — print.predict.ptLasso","text":"","code":"# Train data out = gaussian.example.data(k=2, class.sizes = c(50, 50)) x = out$x; y=out$y; groups = out$group;  # Test data outtest = gaussian.example.data(k=2, class.sizes = c(50, 50)) xtest=outtest$x; ytest=outtest$y; groupstest=outtest$groups  fit = ptLasso(x, y, groups = groups, nfolds = 3, family = \"gaussian\", type.measure = \"mse\") pred = predict(fit, xtest, groupstest, ytest=ytest, s=\"lambda.min\") print(pred) #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest, s = \"lambda.min\")  #>  #>  #> alpha =  0.5  #>  #> Performance (Mean squared error): #>  #>            allGroups  mean group_1 group_2     r^2 #> Overall        710.8 710.8   487.6   934.0 -0.1502 #> Pretrain       700.7 700.7   451.1   950.3 -0.1338 #> Individual     700.3 700.3   450.5   950.1 -0.1331 #>  #> Support size: #>                                          #> Overall    19                            #> Pretrain   13 (0 common + 13 individual) #> Individual 13                             # If ytest is not supplied, just prints the pretrained predictions. pred = predict(fit, xtest, groupstest, s=\"lambda.min\") print(pred) #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, groupstest = groupstest,   #>     s = \"lambda.min\")  #>  #>  #> alpha =  0.5  #>  #> Support size: #>                                          #> Overall    19                            #> Pretrain   13 (0 common + 13 individual) #> Individual 13"},{"path":"https://erincr.github.io/ptLasso/reference/print.ptLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"Print the ptLasso object. — print.ptLasso","title":"Print the ptLasso object. — print.ptLasso","text":"Print ptLasso object.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/print.ptLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print the ptLasso object. — print.ptLasso","text":"","code":"# S3 method for class 'ptLasso' print(x, ...)"},{"path":"https://erincr.github.io/ptLasso/reference/print.ptLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print the ptLasso object. — print.ptLasso","text":"x fitted \"ptLasso\" object. ... arguments pass print function.","code":""},{"path":[]},{"path":"https://erincr.github.io/ptLasso/reference/print.ptLasso.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Print the ptLasso object. — print.ptLasso","text":"Erin Craig Rob Tibshirani Maintainer: Erin Craig <erincr@stanford.edu>","code":""},{"path":"https://erincr.github.io/ptLasso/reference/print.ptLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print the ptLasso object. — print.ptLasso","text":"","code":"out = gaussian.example.data() x = out$x; y=out$y; groups = out$group;  fit = ptLasso(x, y, groups = groups, family = \"gaussian\", type.measure = \"mse\") print(fit) #>  #> Call:  ptLasso(x = x, y = y, groups = groups, family = \"gaussian\", type.measure = \"mse\",      use.case = \"inputGroups\", group.intercepts = TRUE)  #>"},{"path":"https://erincr.github.io/ptLasso/reference/ptLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit a pretrained lasso model using glmnet. — ptLasso","title":"Fit a pretrained lasso model using glmnet. — ptLasso","text":"Fits pretrained lasso model using glmnet package, fixed choice pretraining hyperparameter alpha. Additionally fits \"overall\" model (using data) \"individual\" models (use individual group). Can fit input-grouped data Gaussian, multinomial, binomial Cox outcomes, target-grouped data, necessarily multinomial outcome. Many ptLasso arguments passed directly glmnet, therefore glmnet documentation another good reference ptLasso.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/ptLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit a pretrained lasso model using glmnet. — ptLasso","text":"","code":"ptLasso(   x,   y,   groups = NULL,   alpha = 0.5,   family = c(\"default\", \"gaussian\", \"multinomial\", \"binomial\", \"cox\"),   type.measure = c(\"default\", \"mse\", \"mae\", \"auc\", \"deviance\", \"class\", \"C\"),   use.case = c(\"inputGroups\", \"targetGroups\", \"multiresponse\", \"timeSeries\"),   overall.lambda = c(\"lambda.1se\", \"lambda.min\"),   overall.gamma = \"gamma.1se\",   foldid = NULL,   nfolds = 10,   standardize = TRUE,   verbose = FALSE,   weights = NULL,   penalty.factor = rep(1, p),   fitoverall = NULL,   fitind = NULL,   en.alpha = 1,   group.intercepts = TRUE,   ... )"},{"path":"https://erincr.github.io/ptLasso/reference/ptLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit a pretrained lasso model using glmnet. — ptLasso","text":"x input matrix, dimension nobs x nvars; row observation vector. Can sparse matrix format (inherit class '\"sparseMatrix\"' package 'Matrix'). Requirement: 'nvars >1'; words, 'x' 2 columns. 'use.case = \"timeSeries\"', x may list matrices identical dimensions, one point time. y response variable. Quantitative 'family=\"gaussian\"'. 'family=\"binomial\"' either factor two levels, two-column matrix counts proportions (second column treated target class; factor, last level alphabetical order target class). 'family=\"multinomial\"', can 'nc>=2' level factor, matrix 'nc' columns counts proportions. either '\"binomial\"' '\"multinomial\"', 'y' presented vector, coerced factor. 'family=\"cox\"', preferably 'Surv' object survival package: see Detail section information. 'use.case = \"multiresponse\"' 'use.case = \"timeSeries\"', 'y' matrix responses. groups vector length nobs indicating group observation belongs. 'use.case = \"inputGroups\"'. alpha pretrained lasso hyperparameter, \\(0\\le\\alpha\\le 1\\). range alpha 0 (fits overall model fine tuning) 1 (individual models). default value 0.5, chosen mostly random. choose appropriate value data, please either run ptLasso choices alpha evaluate validation set, use cv.ptLasso, recommends value alpha using cross validation. family Either character string representing one built-families, else 'glm()' family object. information, see Details section documentation response type (see ). type.measure loss use cross-validation within individual, overall, pretrained lasso model. Currently five options, available models. default 'type.measure=\"deviance\"', uses squared-error gaussian models (.k.'type.measure=\"mse\"' ), deviance logistic poisson regression, partial-likelihood Cox model. 'type.measure=\"class\"' applies binomial multinomial logistic regression , gives misclassification error. 'type.measure=\"auc\"' two-class logistic regression , gives area ROC curve. 'type.measure=\"mse\"' 'type.measure=\"mae\"' (mean absolute error) can used models except '\"cox\"'; measure deviation fitted mean response. 'type.measure=\"C\"' Harrel's concordance measure, available 'cox' models. use.case type grouping observed data. Can one \"inputGroups\", \"targetGroups\", \"multiresponse\" \"timeSeries\". overall.lambda choice lambda used overall model define offset penalty factor pretrained lasso. Defaults \"lambda.1se\", alternatively \"lambda.min\". choice lambda used compute offset penalty factor (1) model training (2) prediction. predict function, another lambda must specified individual models, second stage pretraining overall model. overall.gamma use option relax = TRUE specified. choice gamma used overall model define offset penalty factor pretrained lasso. Defaults \"gamma.1se\", \"gamma.min\" also good option. choice gamma used compute offset penalty factor (1) model training (2) prediction. predict function, another gamma must specified individual models, second stage pretraining overall model. foldid optional vector values 1 nfolds identifying fold observation . supplied, nfold can missing. nfolds Number folds CV (default 10). Although nfolds can large sample size (leave-one-CV), recommended large datasets. Smallest value allowable nfolds = 3. standardize predictors standardized fitting (default TRUE). verbose verbose=TRUE, print statement showing model currently fit cv.glmnet. weights observation weights. Default 1 observation. penalty.factor Separate penalty factors can applied coefficient. number multiplies 'lambda' allow differential shrinkage. Can 0 variables,  implies shrinkage, variable always included model. Default 1 variables (implicitly infinity variables listed 'exclude'). information, see ?glmnet. pretraining, user-supplied penalty.factor multiplied computed overall model. fitoverall optional cv.glmnet object specifying overall model. trained full training data, argument 'keep = TRUE'. fitind optional list cv.glmnet objects specifying individual models. trained original training data, argument 'keep = TRUE'. en.alpha elasticnet mixing parameter, 0 <= en.alpha <= 1. penalty defined (1-alpha)/2||beta||_2^2+alpha||beta||_1. 'alpha=1' lasso penalty, 'alpha=0' ridge penalty. Default `en.alpha = 1` (lasso). group.intercepts 'use.case = \"inputGroups\"' . `TRUE`, fit overall model separate intercept group. `FALSE`, ignore grouping fit one overall intercept. Default `TRUE`. ... Additional arguments passed cv.glmnet functions. Notable choices include \"trace.\" \"parallel\". trace.= TRUE, progress bar displayed call \"cv.glmnet\"; useful big models take long time fit. parallel = TRUE, use parallel foreach fit fold.  Must register parallel hand, doMC others. ptLasso support arguments intercept, offset, fit check.args.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/ptLasso.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit a pretrained lasso model using glmnet. — ptLasso","text":"object class \"ptLasso\", list ingredients fitted models. call call produced object. k number groups ('use.case = \"inputGroups\"'). nresps number responses ('use.case = \"multiresponse\"' 'use.case = \"timeseries\"'). alpha value alpha used pretraining. group.levels IDs groups used training ('use.case = \"inputGroups\"'). group.legend Mapping user-supplied group ids numeric group ids. internal use (e.g. predict). ('use.case = \"inputGroups\"') fitoverall fitted cv.glmnet object trained using full data. available 'use.case = \"timeseries\"'. fitpre list fitted (pretrained) cv.glmnet objects, one trained data group response. fitind list fitted cv.glmnet objects, one trained group response. fitoverall.lambda Lambda used fitoverall, compute offset pretraining. fitoverall.gamma Gamma used fitoverall 'relax = TRUE', compute offset pretraining.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/ptLasso.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Fit a pretrained lasso model using glmnet. — ptLasso","text":"Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization paths generalized linear models via coordinate descent. Journal Statistical Software, 33(1), 1-22.","code":""},{"path":[]},{"path":"https://erincr.github.io/ptLasso/reference/ptLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit a pretrained lasso model using glmnet. — ptLasso","text":"","code":"# Getting started. First, we simulate data: we need covariates x, response y and group IDs. set.seed(1234) n=100 p=10 n.groups=2 x = matrix(rnorm(n*p), n, p) y = rnorm(n) groups = sort(rep(1:n.groups, n/n.groups))  xtest = matrix(rnorm(n*p), n, p) ytest = rnorm(n) groupstest = sort(rep(1:n.groups, n/n.groups))  # Now, we can fit a ptLasso model: fit = ptLasso(x, y, groups = groups, alpha = 0.5, family = \"gaussian\",                nfolds = 3, type.measure = \"mse\") plot(fit) # to see all of the cv.glmnet models trained  predict(fit, xtest, groupstest) # to predict on new data #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, groupstest = groupstest)  #>  #>  #>  #> alpha =  0.5  #>  #> Support size: #>                                        #> Overall    1                           #> Pretrain   1 (0 common + 1 individual) #> Individual 1                           predict(fit, xtest, groupstest, ytest=ytest) # if ytest is included, we also measure performance #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest)  #>  #>  #> alpha =  0.5  #>  #> Performance (Mean squared error): #>  #>            allGroups  mean group_1 group_2      r^2 #> Overall        1.162 1.162   1.423  0.9020 0.009235 #> Pretrain       1.167 1.167   1.422  0.9110 0.005515 #> Individual     1.166 1.166   1.421  0.9115 0.005927 #>  #> Support size: #>                                        #> Overall    1                           #> Pretrain   1 (0 common + 1 individual) #> Individual 1                            # When we trained our model, we used \"lambda.1se\" in the first stage of pretraining by default. # This is a necessary choice to make during model training; we need to select the model # we want to use to define the offset and penalty factor for the second stage of pretraining. # We could instead have used \"lambda.min\": fit = ptLasso(x, y, groups = groups, alpha = 0.5, family = \"gaussian\",                type.measure = \"mse\", nfolds = 3, overall.lambda = \"lambda.min\")  # We can use the 'relax' option to fit relaxed lasso models: fit = ptLasso(x, y, groups = groups, alpha = 0.5,               family = \"gaussian\", type.measure = \"mse\",               relax = TRUE)  # As we did for lambda, we may want to specify the choice of gamma for stage one # of pretraining. (The default is \"gamma.1se\".) fit = ptLasso(x, y, groups = groups, alpha = 0.5,               family = \"gaussian\", type.measure = \"mse\",               relax = TRUE, overall.gamma = \"gamma.min\")  # In practice, we may want to try many values of alpha. # alpha may range from 0 (the overall model with fine tuning) to 1 (the individual models). # To choose alpha, you may either (1) run ptLasso with different values of alpha # and measure performance with a validation set, or (2) use cv.ptLasso.   # \\donttest{ # Now, we are ready to simulate slightly more realistic data. # This continuous outcome example has k = 5 groups, where each group has 200 observations. # There are scommon = 10 features shared across all groups, and # sindiv = 10 features unique to each group. # n = 1000 and p = 120 (60 informative features and 60 noise features). # The coefficients of the common features differ across groups (beta.common). # In group 1, these coefficients are rep(1, 10); in group 2 they are rep(2, 10), etc. # Each group has 10 unique features, the coefficients of which are all 3 (beta.indiv). # The intercept in all groups is 0. # The variable sigma = 20 indicates that we add noise to y according to 20 * rnorm(n).  set.seed(1234) k=5 class.sizes=rep(200, k) scommon=10; sindiv=rep(10, k) n=sum(class.sizes); p=2*(sum(sindiv) + scommon) beta.common=3*(1:k); beta.indiv=rep(3, k) intercepts=rep(0, k) sigma=20 out = gaussian.example.data(k=k, class.sizes=class.sizes,                             scommon=scommon, sindiv=sindiv,                             n=n, p=p,                             beta.common=beta.common, beta.indiv=beta.indiv,                             intercepts=intercepts, sigma=20) x = out$x; y=out$y; groups = out$group  outtest = gaussian.example.data(k=k, class.sizes=class.sizes,                                 scommon=scommon, sindiv=sindiv,                                 n=n, p=p,                                 beta.common=beta.common, beta.indiv=beta.indiv,                                 intercepts=intercepts, sigma=20) xtest=outtest$x; ytest=outtest$y; groupstest=outtest$groups  fit = ptLasso(x, y, groups = groups, alpha = 0.5, family = \"gaussian\", type.measure = \"mse\") plot(fit) # to see all of the cv.glmnet models trained  predict(fit, xtest, groupstest, ytest=ytest) #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest)  #>  #>  #> alpha =  0.5  #>  #> Performance (Mean squared error): #>  #>            allGroups  mean group_1 group_2 group_3 group_4 group_5    r^2 #> Overall        755.7 755.7   836.0   554.9   565.4   777.9  1044.0 0.5371 #> Pretrain       503.2 503.2   550.6   443.3   553.5   505.6   462.9 0.6918 #> Individual     532.8 532.8   584.1   443.2   567.2   550.5   518.9 0.6736 #>  #> Support size: #>                                           #> Overall    64                             #> Pretrain   94 (21 common + 73 individual) #> Individual 109                            # }  # \\donttest{ # Now, we repeat with a binomial outcome. # This example has k = 3 groups, where each group has 100 observations. # There are scommon = 5 features shared across all groups, and # sindiv = 5 features unique to each group. # n = 300 and p = 40 (20 informative features and 20 noise features). # The coefficients of the common features differ across groups (beta.common), # as do the coefficients specific to each group (beta.indiv). set.seed(1234) k=3 class.sizes=rep(100, k) scommon=5; sindiv=rep(5, k) n=sum(class.sizes); p=2*(sum(sindiv) + scommon) beta.common=list(c(-.5, .5, .3, -.9, .1), c(-.3, .9, .1, -.1, .2), c(0.1, .2, -.1, .2, .3)) beta.indiv = lapply(1:k, function(i)  0.9 * beta.common[[i]])  out = binomial.example.data(k=k, class.sizes=class.sizes,                             scommon=scommon, sindiv=sindiv,                             n=n, p=p,                             beta.common=beta.common, beta.indiv=beta.indiv) x = out$x; y=out$y; groups = out$group  outtest = binomial.example.data(k=k, class.sizes=class.sizes,                                 scommon=scommon, sindiv=sindiv,                                 n=n, p=p,                                 beta.common=beta.common, beta.indiv=beta.indiv) xtest=outtest$x; ytest=outtest$y; groupstest=outtest$groups  fit = ptLasso(x, y, groups = groups, alpha = 0.5, family = \"binomial\", type.measure = \"auc\") plot(fit) # to see all of the cv.glmnet models trained  predict(fit, xtest, groupstest, ytest=ytest) #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest)  #>  #>  #> alpha =  0.5  #>  #> Performance (AUC): #>  #>            allGroups   mean wtdMean group_1 group_2 group_3 #> Overall       0.5990 0.5963  0.5963  0.6006  0.6772  0.5112 #> Pretrain      0.6442 0.6631  0.6631  0.6965  0.7752  0.5177 #> Individual    0.6429 0.6580  0.6580  0.6948  0.7607  0.5186 #>  #> Support size: #>                                          #> Overall    8                             #> Pretrain   40 (3 common + 37 individual) #> Individual 40                            # }  if (FALSE) { # \\dontrun{ ### Model fitting with parallel = TRUE require(doMC) registerDoMC(cores = 4) fit = ptLasso(x, y, groups = groups, family = \"gaussian\", type.measure = \"mse\", parallel=TRUE) } # }  # \\donttest{ # Multiresponse pretraining: # Now let's consider the case of a multiresponse outcome. We'll start by simulating data: set.seed(1234) n = 1000; ntrain = 500; p = 500 sigma = 2       x = matrix(rnorm(n*p), n, p) beta1 = c(rep(1, 5), rep(0.5, 5), rep(0, p - 10)) beta2 = c(rep(1, 5), rep(0, 5), rep(0.5, 5), rep(0, p - 15))  mu = cbind(x %*% beta1, x %*% beta2) y  = cbind(mu[, 1] + sigma * rnorm(n),             mu[, 2] + sigma * rnorm(n)) cat(\"SNR for the two tasks:\", round(diag(var(mu)/var(y-mu)), 2), fill=TRUE) #> SNR for the two tasks: 1.6 1.44 cat(\"Correlation between two tasks:\", cor(y[, 1], y[, 2]), fill=TRUE) #> Correlation between two tasks: 0.5164748  xtest = x[-(1:ntrain), ] ytest = y[-(1:ntrain), ]  x = x[1:ntrain, ] y = y[1:ntrain, ]  fit = ptLasso(x, y, type.measure = \"mse\", use.case = \"multiresponse\") plot(fit)  # to see all of the cv.glmnet models trained  predict(fit, xtest) # to predict with new data #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest)  #>  #>  #> alpha =  0.5  #>  #> Support size: #>                                           #> Overall    57                             #> Pretrain   29 (19 common + 10 individual) #> Individual 80                             predict(fit, xtest, ytest=ytest) # if ytest is included, we also measure performance #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, ytest = ytest)  #>  #>  #>  #> alpha =  0.5  #>  #> Performance (Mean squared error): #>  #>            allGroups  mean response_1 response_2 #> Overall        9.394 4.697      4.227      5.168 #> Pretrain       9.022 4.511      4.144      4.878 #> Individual     9.465 4.733      4.243      5.222 #>  #> Support size: #>                                           #> Overall    57                             #> Pretrain   29 (19 common + 10 individual) #> Individual 80                             # By default, we used lambda = \"lambda.min\" to measure performance. # We could instead use lambda = \"lambda.1se\": predict(fit, xtest, ytest=ytest, s=\"lambda.1se\") #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, ytest = ytest, s = \"lambda.1se\")  #>  #>  #>  #> alpha =  0.5  #>  #> Performance (Mean squared error): #>  #>            allGroups  mean response_1 response_2 #> Overall        9.879 4.940      4.387      5.492 #> Pretrain       9.481 4.740      4.210      5.271 #> Individual     9.965 4.983      4.341      5.624 #>  #> Support size: #>                                          #> Overall    19                            #> Pretrain   19 (19 common + 0 individual) #> Individual 27                             # We could also use the glmnet option relax = TRUE: fit = ptLasso(x, y, type.measure = \"mse\", relax = TRUE, use.case = \"multiresponse\") # }  # \\donttest{ # Time series pretraining # Now suppose we have time series data with a binomial outcome measured at 3 different time points. set.seed(1234) n = 600; ntrain = 300; p = 50 x = matrix(rnorm(n*p), n, p)  beta1 = c(rep(0.5, 10), rep(0, p-10)) beta2 = beta1 + c(rep(0, 10), runif(5, min = 0, max = 0.5), rep(0, p-15)) beta3 = beta1 + c(rep(0, 10), runif(5, min = 0, max = 0.5), rep(.5, 5), rep(0, p-20))  y1 = rbinom(n, 1, prob = 1/(1 + exp(-x %*% beta1))) y2 = rbinom(n, 1, prob = 1/(1 + exp(-x %*% beta2))) y3 = rbinom(n, 1, prob = 1/(1 + exp(-x %*% beta3))) y = cbind(y1, y2, y3)  xtest = x[-(1:ntrain), ] ytest = y[-(1:ntrain), ]  x = x[1:ntrain, ] y = y[1:ntrain, ]  fit =  ptLasso(x, y, use.case=\"timeSeries\", family=\"binomial\", type.measure = \"auc\") plot(fit)  predict(fit, xtest, ytest=ytest) #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, ytest = ytest)  #>  #>  #>  #> alpha =  0.5  #>  #> Performance (AUC): #>  #>              mean response_1 response_2 response_3 #> Pretrain   0.8194     0.8194     0.8035     0.8354 #> Individual 0.7997     0.8194     0.7691     0.8106 #>  #> Support size: #>                                           #> Pretrain   43 (26 common + 17 individual) #> Individual 47                              # The glmnet option relax = TRUE: fit = ptLasso(x, y, type.measure = \"auc\", family = \"binomial\", relax = TRUE,               use.case = \"timeSeries\") plot(fit)  predict(fit, xtest, ytest=ytest) #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, ytest = ytest)  #>  #>  #>  #> alpha =  0.5  #>  #> Performance (AUC): #>  #>              mean response_1 response_2 response_3 #> Pretrain   0.8123     0.8298     0.7818     0.8252 #> Individual 0.8018     0.8298     0.7647     0.8110 #>  #> Support size: #>                                           #> Pretrain   26 (12 common + 14 individual) #> Individual 29                             # }"},{"path":"https://erincr.github.io/ptLasso/news/index.html","id":"ptlasso-101","dir":"Changelog","previous_headings":"","what":"ptLasso 1.0.1","title":"ptLasso 1.0.1","text":"Improved timings tests vignettes.","code":""},{"path":"https://erincr.github.io/ptLasso/news/index.html","id":"ptlasso-100","dir":"Changelog","previous_headings":"","what":"ptLasso 1.0.0","title":"ptLasso 1.0.0","text":"Initial CRAN submission.","code":""}]
