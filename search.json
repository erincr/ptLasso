[{"path":"https://erincr.github.io/ptLasso/articles/ConditionalAverageTreatmentEffect.html","id":"background-cate-estimation-and-pretraining","dir":"Articles","previous_headings":"","what":"Background: CATE estimation and pretraining","title":"Conditional average treatment effect estimation","text":"causal inference, often interested predicting treatment effect individual observations; called conditional average treatment effect (CATE). example, prescribing drug patient, want know whether drug likely work well patient - just whether works well average. One tool model CATE R-learner (Nie Wager (2021)), minimizes R loss: $$ \\hat{L}_n\\{\\tau(\\cdot)\\}=\\arg \\min_\\tau \\frac{1}{n}\\sum\\Bigl[ (y_i- m^*(x_i)) - (W_i-e^*(x_i))\\tau(x_i) \\Bigr]^2. $$ , xix_i yiy_i covariates outcome observation ii, e*(xi)e^*(x_i) treatment propensity WiW_i treatment assignment, m*(xi)m^*(x_i) conditional mean outcome (E[yi∣x=xi]E[y_i \\mid x = x_i]). , τ̂\\hat\\tau estimate heterogeneous treatment effect function. fitted stages: first, R-learner fits m*m^* e*e^* get m̂*\\hat{m}^* ê*\\hat{e}^*; plugs m̂*(xi)\\hat{m}^*(x_i) ê*(xi)\\hat{e}^*(x_i) fit τ\\tau. minor detail cross-fitting (prevalidation) used first stage plugin value e.g. m̂*(xi)\\hat{m}^*(x_i) comes model trained without using xix_i. τ\\tau linear function, second stage fitting straightforward. values m̂*(xi)\\hat{m}^*(x_i) ê*(xi)\\hat{e}^*(x_i) known, can use linear regression model yi−m̂*(xi)y_i - \\hat{m}^*(x_i) function weighted feature vector (Wi−ê*(xi))xi(W_i-\\hat{e}^*(x_i)) x_i. following example. can pretraining useful ? Well, separately fitting models m*m^* (conditional mean) τ\\tau (heterogeneous treatment effect), two functions likely share support: sensible assume features modulate mean treatment effect also modulate heterogeneous treatment effect. can use pretraining (1) training model m*m^* (2) using support model guide fitting τ\\tau. Note offset used case; m*m^* τ\\tau designed predict different outcomes.","code":""},{"path":"https://erincr.github.io/ptLasso/articles/ConditionalAverageTreatmentEffect.html","id":"a-simulated-example","dir":"Articles","previous_headings":"","what":"A simulated example","title":"Conditional average treatment effect estimation","text":"example. simplify problem assuming treatment randomized – true e*(xi)=0.5e^*(x_i) = 0.5 ii. begin model fitting, starting estimate e*e^* (probability receiving treatment). fit τ\\tau, also need record cross-fitted ê*(x)\\hat{e}^*(x). Now, stage 1 pretraining: fit model m*m^* record support. , also record cross-fitted m̂*(x)\\hat{m}^*(x). fit τ\\tau, regress ỹ=yi−m̂*(xi)\\tilde{y} = y_i - \\hat{m}^*(x_i) x̃=(wi−ê*(xi))xi\\tilde{x} = (w_i - \\hat{e}^*(x_i)) x_i; ’ll define : now, pretraining τ\\tau. Loop α=0,0.1,…,1\\alpha = 0, 0.1, \\dots, 1; α\\alpha, fit model τ\\tau using penalty factor defined support m̂\\hat{m} α\\alpha. ’ll keep track CV MSE step can choose α\\alpha minimizes MSE.  plot , value α=1\\alpha = 1 corresponds usual R learner, makes assumption shared support τ\\tau m*m^*. Based plot, choose α=0.2\\alpha = 0.2 best performing model: concretely compare pretrained R-learner usual R-learner, ’ll train usual R-learner : anticipated, pretraining improves prediction squared error relative R learner – designed simulation:","code":"set.seed(1234)  n = 600; ntrain = 300 p = 20       x = matrix(rnorm(n*p), n, p)  # Treatment assignment w = rbinom(n, 1, 0.5)  # m^* m.coefs = c(rep(2,10), rep(0, p-10)) m = x %*% m.coefs  # tau tau.coefs = runif(p, 0.5, 1)*m.coefs  tau = 1.5*m + x%*%tau.coefs  mu = m + w * tau y  = mu + 10 * rnorm(n) cat(\"Signal to noise ratio:\", var(mu)/var(y-mu)) #> Signal to noise ratio: 2.301315  # Split into train/test xtest = x[-(1:ntrain), ] tautest = tau[-(1:ntrain)]  wtest = w[-(1:ntrain)]  x = x[1:ntrain, ] y = y[1:ntrain]  w = w[1:ntrain]  # Define training folds nfolds = 10 foldid = sample(rep(1:10, trunc(nrow(x)/nfolds)+1))[1:nrow(x)] e_fit = cv.glmnet(x, w, foldid = foldid,                   family=\"binomial\", type.measure=\"deviance\",                   keep = TRUE)  e_hat = e_fit$fit.preval[, e_fit$lambda == e_fit$lambda.1se] e_hat = 1/(1 + exp(-e_hat)) m_fit = cv.glmnet(x, y, foldid = foldid, keep = TRUE)  m_hat = m_fit$fit.preval[, m_fit$lambda == m_fit$lambda.1se]  bhat = coef(m_fit, s = m_fit$lambda.1se) support = which(bhat[-1] != 0) y_tilde = y - m_hat x_tilde = cbind(as.numeric(w - e_hat) * cbind(1, x)) cv.error = NULL alphalist = seq(0, 1, length.out = 11)  for(alpha in alphalist){   pf = rep(1/alpha, p)   pf[support] = 1   pf = c(0, pf) # Don't penalize the intercept      tau_fit = cv.glmnet(x_tilde, y_tilde,                        foldid = foldid,                       penalty.factor = pf,                       intercept = FALSE, # already include in x_tilde                       standardize = FALSE)   cv.error = c(cv.error, min(tau_fit$cvm)) }   plot(alphalist, cv.error, type = \"b\",      xlab = expression(alpha),       ylab = \"CV MSE\",       main = bquote(\"CV mean squared error as a function of \" ~ alpha)) abline(v = alphalist[which.min(cv.error)]) best.alpha = alphalist[which.min(cv.error)] cat(\"Chosen alpha:\", best.alpha) #> Chosen alpha: 0.2  pf = rep(1/best.alpha, p) pf[support] = 1 pf = c(0, pf) tau_fit = cv.glmnet(x_tilde, y_tilde, foldid = foldid,                     penalty.factor = pf,                     intercept = FALSE,                     standardize = FALSE) tau_rlearner = cv.glmnet(x_tilde, y_tilde, foldid = foldid,                           penalty.factor = c(0, rep(1, ncol(x))),                          intercept = FALSE,                          standardize = FALSE) rlearner_preds   = predict(tau_rlearner, cbind(1, xtest), s = \"lambda.min\") cat(\"R-learner PSE: \",      round(mean((rlearner_preds - tautest)^2), 2)) #> R-learner PSE:  45.85  pretrained_preds = predict(tau_fit, cbind(1, xtest), s = \"lambda.min\") cat(\"Pretrained R-learner PSE: \",      round(mean((pretrained_preds - tautest)^2), 2)) #> Pretrained R-learner PSE:  37.63"},{"path":"https://erincr.github.io/ptLasso/articles/ConditionalAverageTreatmentEffect.html","id":"what-if-the-pretraining-assumption-is-wrong","dir":"Articles","previous_headings":"","what":"What if the pretraining assumption is wrong?","title":"Conditional average treatment effect estimation","text":", repeat everything , now overlap support m*m^* τ\\tau. Pretraining hurt performance, even though support m*m^* τ\\tau shared. ? Recall defined y=m*(x)+W*τ(x)+ϵy =  m^*(x) + W * \\tau(x) + \\epsilon, relationship yy xx function supports m*m^* τ\\tau. first stage pretraining, fitted m*m^* using y ~ x – support m*m^*include support τ\\tau. result, using pretraining R-learner harm predictive performance.","code":"###################################################### # Simulate data ###################################################### x = matrix(rnorm(n*p), n, p)  # Treatment assignment w = rbinom(n, 1, 0.5)  # m^* m.coefs = c(rep(2,10), rep(0, p-10)) m = x %*% m.coefs  # tau # Note these coefficients have no overlap with m.coefs! tau.coefs = c(rep(0, 10), rep(2, 10), rep(0, p-20)) tau = x%*%tau.coefs  mu = m + w * tau y  = mu + 10 * rnorm(n) cat(\"Signal to noise ratio:\", var(mu)/var(y-mu)) #> Signal to noise ratio: 0.6938152  # Split into train/test xtest = x[-(1:ntrain), ] tautest = tau[-(1:ntrain)]  wtest = w[-(1:ntrain)]  x = x[1:ntrain, ] y = y[1:ntrain]  w = w[1:ntrain]  ###################################################### # Model fitting: e^* ###################################################### e_fit = cv.glmnet(x, w, foldid = foldid,                   family=\"binomial\", type.measure=\"deviance\",                   keep = TRUE) e_hat = e_fit$fit.preval[, e_fit$lambda == e_fit$lambda.1se] e_hat = 1/(1 + exp(-e_hat))  ###################################################### # Model fitting: m^* ###################################################### m_fit = cv.glmnet(x, y, foldid = foldid, keep = TRUE)  m_hat = m_fit$fit.preval[, m_fit$lambda == m_fit$lambda.1se]  bhat = coef(m_fit, s = m_fit$lambda.1se) support = which(bhat[-1] != 0)  ###################################################### # Pretraining: tau ###################################################### y_tilde = y - m_hat x_tilde = cbind(as.numeric(w - e_hat) * cbind(1, x))  cv.error = NULL alphalist = seq(0, 1, length.out = 11)  for(alpha in alphalist){   pf = rep(1/alpha, p)   pf[support] = 1   pf = c(0, pf) # Don't penalize the intercept      tau_fit = cv.glmnet(x_tilde, y_tilde,                        foldid = foldid,                       penalty.factor = pf,                       intercept = FALSE, # already include in x_tilde                       standardize = FALSE)   cv.error = c(cv.error, min(tau_fit$cvm)) }  # Our final model for tau: best.alpha = alphalist[which.min(cv.error)] cat(\"Chosen alpha:\", best.alpha) #> Chosen alpha: 1  pf = rep(1/best.alpha, p) pf[support] = 1 pf = c(0, pf) tau_fit = cv.glmnet(x_tilde, y_tilde, foldid = foldid,                     penalty.factor = pf,                     intercept = FALSE,                     standardize = FALSE)  ###################################################### # Fit the usual R-learner: ###################################################### tau_rlearner = cv.glmnet(x_tilde, y_tilde, foldid = foldid,                           penalty.factor = c(0, rep(1, ncol(x))),                          intercept = FALSE,                          standardize = FALSE)  ###################################################### # Measure performance: ###################################################### rlearner_preds = predict(tau_rlearner, cbind(1, xtest), s = \"lambda.min\") cat(\"R-learner prediction squared error: \",      round(mean((rlearner_preds - tautest)^2), 2)) #> R-learner prediction squared error:  31.11  pretrained_preds = predict(tau_fit, cbind(1, xtest), s = \"lambda.min\") cat(\"Pretrained R-learner prediction squared error: \",      round(mean((pretrained_preds - tautest)^2), 2)) #> Pretrained R-learner prediction squared error:  31.11"},{"path":"https://erincr.github.io/ptLasso/articles/InputGroupedData.html","id":"base-case-input-grouped-data-with-a-binomial-outcome","dir":"Articles","previous_headings":"","what":"Base case: input grouped data with a binomial outcome","title":"Input grouped data","text":"Quick Start, applied ptLasso data continuous response. , ’ll use data binary outcome. creates dataset k=3k = 3 groups (100100 observations), 5 shared coefficients, 5 coefficients specific group. can fit predict . default, predict.ptLasso compute return deviance test set. instead compute AUC specifying type.measure call ptLasso. Note: type.measure specified model fitting prediction used call cv.glmnet. fit overall individual models, can use elasticnet instead lasso defining parameter en.alpha (glmnet described section “Fitting elasticnet ridge models”). Using cross validation Gaussian case:","code":"set.seed(1234)  out = binomial.example.data() x = out$x; y = out$y; groups = out$groups  outtest = binomial.example.data() xtest = outtest$x; ytest = outtest$y; groupstest = outtest$groups fit = ptLasso(x, y, groups, alpha = 0.5, family = \"binomial\")  predict(fit, xtest, groupstest, ytest = ytest) #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest)  #>  #>  #> alpha =  0.5  #>  #> Performance (Deviance): #>  #>            allGroups  mean wtdMean group_1 group_2 group_3 #> Overall        1.359 1.359   1.359   1.334   1.321   1.421 #> Pretrain       1.279 1.279   1.279   1.272   1.169   1.397 #> Individual     1.283 1.283   1.283   1.265   1.186   1.399 #>  #> Support size: #>                                         #> Overall    7                            #> Pretrain   12 (3 common + 9 individual) #> Individual 20 fit = ptLasso(x, y, groups, alpha = 0.5, family = \"binomial\",                type.measure = \"auc\")  predict(fit, xtest, groupstest, ytest = ytest) #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest)  #>  #>  #> alpha =  0.5  #>  #> Performance (AUC): #>  #>            allGroups   mean wtdMean group_1 group_2 group_3 #> Overall       0.6026 0.6039  0.6039  0.6161  0.6877  0.5080 #> Pretrain      0.6407 0.6524  0.6524  0.6936  0.7447  0.5190 #> Individual    0.6442 0.6618  0.6618  0.6936  0.7732  0.5186 #>  #> Support size: #>                                          #> Overall    15                            #> Pretrain   39 (3 common + 36 individual) #> Individual 40 fit = ptLasso(x, y, groups, alpha = 0.5, family = \"binomial\",                type.measure = \"auc\",                en.alpha = .5) predict(fit, xtest, groupstest, ytest = ytest) #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest)  #>  #>  #> alpha =  0.5  #>  #> Performance (AUC): #>  #>            allGroups   mean wtdMean group_1 group_2 group_3 #> Overall       0.6041 0.6018  0.6018  0.5928  0.6704  0.5422 #> Pretrain      0.6270 0.6547  0.6547  0.6781  0.7720  0.5141 #> Individual    0.6387 0.6598  0.6598  0.6756  0.7820  0.5218 #>  #> Support size: #>                                          #> Overall    3                             #> Pretrain   39 (3 common + 36 individual) #> Individual 36 ################################################## # Fit: ################################################## fit = cv.ptLasso(x, y, groups, family = \"binomial\", type.measure = \"auc\") #> Warning: from glmnet C++ code (error code -100); Convergence for 100th lambda #> value not reached after maxit=100000 iterations; solutions for larger lambdas #> returned #> Warning: from glmnet C++ code (error code -100); Convergence for 100th lambda #> value not reached after maxit=100000 iterations; solutions for larger lambdas #> returned #> Warning: from glmnet C++ code (error code -92); Convergence for 92th lambda #> value not reached after maxit=100000 iterations; solutions for larger lambdas #> returned #> Warning: from glmnet C++ code (error code -90); Convergence for 90th lambda #> value not reached after maxit=100000 iterations; solutions for larger lambdas #> returned  ################################################## # Predict with a common alpha for all groups: ################################################## predict(fit, xtest, groupstest, ytest = ytest) #>  #> Call:   #> predict.cv.ptLasso(object = fit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest)  #>  #>  #> alpha =  0.7  #>  #> Performance (AUC): #>  #>            allGroups   mean wtdMean group_1 group_2 group_3 #> Overall       0.5990 0.5960  0.5960  0.6030  0.6644  0.5206 #> Pretrain      0.6401 0.6640  0.6640  0.6965  0.7732  0.5222 #> Individual    0.6559 0.6707  0.6707  0.6936  0.7808  0.5377 #>  #> Support size: #>                                          #> Overall    7                             #> Pretrain   40 (3 common + 37 individual) #> Individual 37  ################################################## # Predict with a different alpha for each group: ################################################## predict(fit, xtest, groupstest, ytest = ytest, alphatype = \"varying\") #>  #> Call:   #> predict.cv.ptLasso(object = fit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest, alphatype = \"varying\")  #>  #>  #> alpha: #> [1] 0.2 0.5 0.2 #>  #>  #> Performance (AUC): #>            overall   mean wtdMean group_1 group_2 group_3 #> Overall     0.5990 0.5960  0.5960  0.6030  0.6644  0.5206 #> Pretrain    0.6359 0.6573  0.6573  0.6838  0.7736  0.5145 #> Individual  0.6559 0.6707  0.6707  0.6936  0.7808  0.5377 #>  #>  #> Support size: #>                                          #> Overall    7                             #> Pretrain   40 (3 common + 37 individual) #> Individual 37"},{"path":"https://erincr.github.io/ptLasso/articles/InputGroupedData.html","id":"base-case-input-grouped-survival-data","dir":"Articles","previous_headings":"","what":"Base case: input grouped survival data","title":"Input grouped data","text":"Now, simulate survival times 3 groups; three groups overlapping support, 5 shared features 5 individual features. compute survival time, start computing survival=Xβ+ϵ\\text{survival} = X \\beta + \\epsilon, β\\beta specific group ϵ\\epsilon noise. survival times must positive, modify survival=survival+1.1*abs(min(survival))\\text{survival} = \\text{survival} + 1.1 * \\text{abs}(\\text{min}(\\text{survival})). Training ptLasso much continuous binomial cases; difference specify family = \"cox\". default, ptLasso uses partial likelihood model selection. instead use C index. call cv.ptLasso much ; need specify family (“cox”) type.measure (want use C index instead partial likelihood).","code":"require(survival) #> Loading required package: survival set.seed(1234)  n = 600; ntrain = 300 p = 50       x = matrix(rnorm(n*p), n, p) beta1 = c(rnorm(5), rep(0, p-5))  beta2 = runif(p) * beta1 # Shared support beta2 = beta2 + c(rep(0, 5), rnorm(5), rep(0, p-10)) # Individual features  beta3 = runif(p) * beta1 # Shared support beta3 = beta3 + c(rep(0, 10), rnorm(5), rep(0, p-15)) # Individual features  # Randomly split into groups groups = sample(1:3, n, replace = TRUE)  # Compute survival times: survival = x %*% beta1 survival[groups == 2] = x[groups == 2, ] %*% beta2 survival[groups == 3] = x[groups == 3, ] %*% beta3 survival = survival + rnorm(n) survival = survival + 1.1 * abs(min(survival))  # Censoring times from a random uniform distribution: censoring = runif(n, min = 1, max = 10)  # Did we observe surivival or censoring? y = Surv(pmin(survival, censoring), survival <= censoring)  # Split into train and test: xtest = x[-(1:300), ] ytest = y[-(1:300), ] groupstest = groups[-(1:300)]  x = x[1:300, ] y = y[1:300, ] groups = groups[1:300] ############################################################ # Default -- use partial likelihood as the type.measure: ############################################################ fit = ptLasso(x, y, groups, alpha = 0.5, family = \"cox\") predict(fit, xtest, groupstest, ytest = ytest) #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest)  #>  #>  #> alpha =  0.5  #>  #> Performance (Deviance): #>  #>            allGroups  mean wtdMean group_1 group_2 group_3 #> Overall        381.2 87.60   89.36   99.49  106.53   56.79 #> Pretrain       396.3 87.86   88.66   93.31   96.54   73.72 #> Individual     425.2 99.07   99.54  111.68  101.85   83.67 #>  #> Support size: #>                                          #> Overall    10                            #> Pretrain   20 (4 common + 16 individual) #> Individual 24  ############################################################ # Alternatively -- use the C index: ############################################################ fit = ptLasso(x, y, groups, alpha = 0.5, family = \"cox\", type.measure = \"C\") #> Warning: from glmnet C++ code (error code -30075); Numerical error at 75th #> lambda value; solutions for larger values of lambda returned predict(fit, xtest, groupstest, ytest = ytest) #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest)  #>  #>  #> alpha =  0.5  #>  #> Performance (C-index): #>  #>            allGroups   mean wtdMean group_1 group_2 group_3 #> Overall       0.8545 0.8673  0.8608  0.9139  0.7746  0.9133 #> Pretrain      0.8359 0.8396  0.8393  0.9152  0.8173  0.7864 #> Individual    0.7925 0.7985  0.8008  0.9075  0.8007  0.6873 #>  #> Support size: #>                                          #> Overall    6                             #> Pretrain   35 (4 common + 31 individual) #> Individual 37 ################################################## # Fit: ################################################## fit = cv.ptLasso(x, y, groups, family = \"cox\", type.measure = \"C\")  ################################################## # Predict with a common alpha for all groups: ################################################## predict(fit, xtest, groupstest, ytest = ytest) #>  #> Call:   #> predict.cv.ptLasso(object = fit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest)  #>  #>  #> alpha =  0.2  #>  #> Performance (C-index): #>  #>            allGroups   mean wtdMean group_1 group_2 group_3 #> Overall       0.8527 0.8652  0.8586  0.9113  0.7711  0.9133 #> Pretrain      0.8501 0.8795  0.8742  0.9177  0.8043  0.9164 #> Individual    0.7865 0.8005  0.8033  0.9126  0.8078  0.6811 #>  #> Support size: #>                                         #> Overall    8                            #> Pretrain   13 (4 common + 9 individual) #> Individual 31  ################################################## # Predict with a different alpha for each group: ################################################## predict(fit, xtest, groupstest, ytest = ytest, alphatype = \"varying\") #>  #> Call:   #> predict.cv.ptLasso(object = fit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest, alphatype = \"varying\")  #>  #>  #> alpha: #> [1] 0.3 0.4 0.4 #>  #>  #> Performance (C-index): #>            overall   mean wtdMean group_1 group_2 group_3 #> Overall     0.8527 0.8652  0.8586  0.9113  0.7711  0.9133 #> Pretrain    0.8081 0.8493  0.8475  0.9229  0.8078  0.8173 #> Individual  0.7865 0.8005  0.8033  0.9126  0.8078  0.6811 #>  #>  #> Support size: #>                                          #> Overall    8                             #> Pretrain   28 (4 common + 24 individual) #> Individual 31"},{"path":"https://erincr.github.io/ptLasso/articles/TargetGroupedData.html","id":"intuition","dir":"Articles","previous_headings":"","what":"Intuition","title":"Target grouped or multinomial reponse data","text":"Now turn target grouped setting, dataset multinomial outcome grouping observations. example, data might look like following: row XX belongs class 1, 2 3, wish predict class membership. fit single multinomial model data: , fit 3 one-vs-rest models; prediction time, assign observations class highest probability. Another alternative pretraining, fits something one model data three separate models. ptLasso , using arguments family = \"multinomial\" use.case = \"targetGroups\". exactly pretraining ? ’ll walk example, pretraining “hand”. steps : Train overall model: multinomial model using penalty coefficients β\\beta coefficient either 0 nonzero classes. Train individual one-vs-rest models using penalty factor offset defined overall model (input grouped setting). train overall model, use cv.glmnet type.multinomial = \"grouped\". puts penalty β\\beta force coefficients model classes. analogous overall model input grouped setting: want first learn shared information. , fit 3 one-vs-rest models using support offset multinomial model. Now everything need train one-vs-rest models. always, pretraining parameter α\\alpha - example, let’s use α=0.5\\alpha = 0.5: ’re done pretraining! predict, assign row class highest prediction: done automatically within ptLasso; now show example using ptLasso functions. example intended show pretraining works multinomial outcomes, technical details omitted. (example, ptLasso takes care crossfitting first second steps.)","code":"set.seed(1234)  n = 500; p = 75; k = 3 X = matrix(rnorm(n * p), nrow = n, ncol = p) y = sample(1:k, n, replace = TRUE)  Xtest = matrix(rnorm(n * p), nrow = n, ncol = p) multinomial = cv.glmnet(X, y, family = \"multinomial\")  multipreds  = predict(multinomial, Xtest, s = \"lambda.min\") multipreds.class = apply(multipreds, 1, which.max) class1 = cv.glmnet(X, y == 1, family = \"binomial\") class2 = cv.glmnet(X, y == 2, family = \"binomial\") class3 = cv.glmnet(X, y == 3, family = \"binomial\")  ovrpreds = cbind(   predict(class1, Xtest, s = \"lambda.min\"),   predict(class2, Xtest, s = \"lambda.min\"),   predict(class3, Xtest, s = \"lambda.min\")) ovrpreds.class = apply(ovrpreds, 1, which.max) fit = ptLasso(X, y, groups = y, alpha = 0.5,               family = \"multinomial\",                use.case = \"targetGroups\") multinomial = cv.glmnet(X, y, family = \"multinomial\",                          type.multinomial = \"grouped\",                         keep = TRUE) # The support of the overall model: nonzero.coefs = which((coef(multinomial, s = \"lambda.1se\")[[1]] != 0)[-1])  # The offsets - one for each class: offset = predict(multinomial, X, s = \"lambda.1se\") offset.class1 = offset[, 1, 1] offset.class2 = offset[, 2, 1] offset.class3 = offset[, 3, 1] alpha = 0.5 penalty.factor = rep(1/alpha, p) penalty.factor[nonzero.coefs] = 1  class1 = cv.glmnet(X, y == 1, family = \"binomial\",                     offset = (1-alpha) * offset.class1,                    penalty.factor = penalty.factor) class2 = cv.glmnet(X, y == 2, family = \"binomial\",                     offset = (1-alpha) * offset.class2,                    penalty.factor = penalty.factor) class3 = cv.glmnet(X, y == 3, family = \"binomial\",                     offset = (1-alpha) * offset.class3,                    penalty.factor = penalty.factor) newoffset = predict(multinomial, X, s = \"lambda.1se\") ovrpreds = cbind(   predict(class1, Xtest, s = \"lambda.min\", newoffset = newoffset[, 1, 1]),   predict(class2, Xtest, s = \"lambda.min\", newoffset = newoffset[, 2, 1]),   predict(class3, Xtest, s = \"lambda.min\", newoffset = newoffset[, 3, 1]) ) ovrpreds.class = apply(ovrpreds, 1, which.max)"},{"path":"https://erincr.github.io/ptLasso/articles/TargetGroupedData.html","id":"example","dir":"Articles","previous_headings":"","what":"Example","title":"Target grouped or multinomial reponse data","text":"First, let’s simulate multinomial data 5 classes. start drawing XX normal distribution (uncorrelated features), shift columns differently group. calls ptLasso cv.ptLasso almost input grouped setting, now specify use.case = \"targetGroups\". call predict require groups argument groups unknown prediction time.","code":"set.seed(1234)  n = 500; p = 50; k = 5 class.sizes = rep(n/k, k) ncommon = 10; nindiv = 5; shift.common = seq(-.2, .2, length.out = k) shift.indiv  = seq(-.1, .1, length.out = k)  x     = matrix(rnorm(n * p), n, p) xtest = matrix(rnorm(n * p), n, p) y = ytest = c(sapply(1:length(class.sizes), function(i) rep(i, class.sizes[i])))  start = ncommon + 1 for (i in 1:k) {   end = start + nindiv - 1   x[y == i, 1:ncommon] = x[y == i, 1:ncommon] + shift.common[i]   x[y == i, start:end] = x[y == i, start:end] + shift.indiv[i]      xtest[ytest == i, 1:ncommon] = xtest[ytest == i, 1:ncommon] + shift.common[i]   xtest[ytest == i, start:end] = xtest[ytest == i, start:end] + shift.indiv[i]   start = end + 1 } ################################################################################ # Fit the pretrained model. # By default, ptLasso uses type.measure = \"deviance\", but for ease of # interpretability, we use type.measure = \"class\" (the misclassification rate). ################################################################################ fit = ptLasso(x = x, y = y,                use.case = \"targetGroups\", type.measure = \"class\")  ################################################################################ # Predict ################################################################################ predict(fit, xtest, ytest = ytest) #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, ytest = ytest)  #>  #>  #>  #> alpha =  0.5  #>  #> Performance (Misclassification error): #>  #>            overall   mean group_1 group_2 group_3 group_4 group_5 #> Overall      0.738                                                #> Pretrain     0.728 0.2000   0.200     0.2     0.2     0.2   0.200 #> Individual   0.736 0.1984   0.196     0.2     0.2     0.2   0.196 #>  #> Support size: #>                                          #> Overall    29                            #> Pretrain   23 (23 common + 0 individual) #> Individual 32  ################################################################################ # Fit with CV to choose the alpha parameter ################################################################################ cvfit = cv.ptLasso(x = x, y = y,               use.case = \"targetGroups\", type.measure = \"class\")  ################################################################################ # Predict using one alpha for all classes ################################################################################ predict(cvfit, xtest, ytest = ytest) #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, ytest = ytest)  #>  #>  #>  #> alpha =  0.9  #>  #> Performance (Misclassification error): #>  #>            overall   mean group_1 group_2 group_3 group_4 group_5 #> Overall      0.738                                                #> Pretrain     0.722 0.1992     0.2     0.2     0.2     0.2   0.196 #> Individual   0.742 0.2000     0.2     0.2     0.2     0.2   0.200 #>  #> Support size: #>                                          #> Overall    39                            #> Pretrain   32 (23 common + 9 individual) #> Individual 36  ################################################################################ # Predict using a separate alpha for each class ################################################################################ predict(cvfit, xtest, ytest = ytest, alphatype = \"varying\") #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, ytest = ytest,   #>     alphatype = \"varying\")  #>  #>  #> alpha =  0.1 0 0.7 0 0.1  #>  #> Performance (Misclassification error): #>  #>            overall   mean group_1 group_2 group_3 group_4 group_5 #> Overall      0.738                                                #> Pretrain     0.742 0.2016   0.208     0.2     0.2   0.202   0.198 #> Individual   0.742 0.2000   0.200     0.2     0.2   0.200   0.200 #>  #> Support size: #>                                           #> Overall    39                             #> Pretrain   36 (23 common + 13 individual) #> Individual 36"},{"path":"https://erincr.github.io/ptLasso/articles/TimeSeriesData.html","id":"example-1-covariates-are-constant-over-time","dir":"Articles","previous_headings":"","what":"Example 1: covariates are constant over time","title":"Time series data","text":"’ll start simulating data – details comments. simulated data, ready call ptLasso; call ptLasso looks much examples, now (1) yy matrix one column time point (2) specify use.case = \"timeSeries\". fitting, call plot shows models fitted time points without using pretraining.  , can predict xtest. example, pretraining helps performance: two time points share support, pretraining discovers leverages . specified alpha = 0 example, cross validation advise us choose α=0.2\\alpha = 0.2. Plotting shows us average performance across two time points. Importantly, time 1, individual model pretrained model ; see advantage pretraining time 2 (use information time 1).  Note also treated multireponse problem, ignored time-ordering responses. See section called “Multi-response data Gaussian responses”. (However, time ordering can informative, multi-response approach make use .)","code":"set.seed(1234)  # Define constants n = 600          # Total number of samples ntrain = 300     # Number of training samples p = 100          # Number of features sigma = 3        # Standard deviation of noise  # Generate covariate matrix x = matrix(rnorm(n * p), n, p)  # Define coefficients for time points 1 and 2 beta1 = c(rep(2, 10), rep(0, p - 10))  # Coefs at time 1 beta2 = runif(p, 0.5, 2) * beta1       # Coefs at time 2, shared support with time 1  # Generate response variables for times 1 and 2 y = cbind(   x %*% beta1 + sigma * rnorm(n),   x %*% beta2 + sigma * rnorm(n) )  # Split data into training and testing sets xtest = x[-(1:ntrain), ]  # Test covariates ytest = y[-(1:ntrain), ]  # Test response  x = x[1:ntrain, ]  # Train covariates y = y[1:ntrain, ]  # Train response fit = ptLasso(x, y, use.case = \"timeSeries\", alpha = 0) plot(fit) preds = predict(fit, xtest, ytest = ytest) preds #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, ytest = ytest)  #>  #>  #>  #> alpha =  0  #>  #> Performance (Mean squared error): #>  #>              mean response_1 response_2 #> Pretrain    9.604      10.78      8.428 #> Individual 10.428      10.78     10.076 #>  #> Support size: #>                                           #> Pretrain   26 (10 common + 16 individual) #> Individual 39 cvfit = cv.ptLasso(x, y, use.case = \"timeSeries\") plot(cvfit) predict(cvfit, xtest, ytest = ytest) #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, ytest = ytest)  #>  #>  #>  #> alpha =  0.2  #>  #> Performance (Mean squared error): #>  #>             mean response_1 response_2 #> Pretrain   10.62      10.87      10.37 #> Individual 10.45      10.87      10.03 #>  #> Support size: #>                                           #> Pretrain   28 (10 common + 18 individual) #> Individual 40 fit = ptLasso(x, y, use.case = \"multiresponse\")"},{"path":"https://erincr.github.io/ptLasso/articles/TimeSeriesData.html","id":"example-2-covariates-change-over-time","dir":"Articles","previous_headings":"","what":"Example 2: covariates change over time","title":"Time series data","text":"Now, ’ll repeat , ’ll simulate data xx changes time. setting, ptLasso expects xx list one covariate matrix time. Now, xx list length two: can call ptLasso, cv.ptLasso, plot predict just :","code":"set.seed(1234)  # Set seed for reproducibility  # Define constants n = 600          # Total number of samples ntrain = 300     # Number of training samples p = 100          # Number of features sigma = 3        # Standard deviation of noise  # Covariates for times 1 and 2 x1 = matrix(rnorm(n * p), n, p) x2 = x1 + matrix(0.2 * rnorm(n * p), n, p)  # Perturbed covariates for time 2 x = list(x1, x2)  # Define coefficients for time points 1 and 2 beta1 = c(rep(2, 10), rep(0, p - 10))  # Coefs at time 1 beta2 = runif(p, 0.5, 2) * beta1       # Coefs at time 2, shared support with time 1  # Response variables for times 1 and 2: y = cbind(   x[[1]] %*% beta1 + sigma * rnorm(n),   x[[2]] %*% beta2 + sigma * rnorm(n) )  # Split data into training and testing sets xtest = lapply(x, function(xx) xx[-(1:ntrain), ])  # Test covariates ytest = y[-(1:ntrain), ]  # Test response  x = lapply(x, function(xx) xx[1:ntrain, ])  # Train covariates y = y[1:ntrain, ]  # Train response str(x) #> List of 2 #>  $ : num [1:300, 1:100] -1.207 0.277 1.084 -2.346 0.429 ... #>  $ : num [1:300, 1:100] -1.493 0.303 1.172 -2.316 0.224 ... fit = ptLasso(x, y, use.case = \"timeSeries\", alpha = 0) plot(fit)  # Plot the fitted model predict(fit, xtest, ytest = ytest)  # Predict using the fitted model #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, ytest = ytest)  #>  #>  #>  #> alpha =  0  #>  #> Performance (Mean squared error): #>  #>             mean response_1 response_2 #> Pretrain   11.92       12.1      11.75 #> Individual 11.46       12.1      10.82 #>  #> Support size: #>                                           #> Pretrain   36 (16 common + 20 individual) #> Individual 61  # With cross validation: cvfit = cv.ptLasso(x, y, use.case = \"timeSeries\") plot(cvfit, plot.alphahat = TRUE)  # Plot cross-validated model predict(cvfit, xtest, ytest = ytest)  # Predict using cross-validated model #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, ytest = ytest)  #>  #>  #>  #> alpha =  0.4  #>  #> Performance (Mean squared error): #>  #>             mean response_1 response_2 #> Pretrain   15.73      12.11      19.35 #> Individual 11.53      12.11      10.96 #>  #> Support size: #>                                           #> Pretrain   54 (19 common + 35 individual) #> Individual 65"},{"path":"https://erincr.github.io/ptLasso/articles/UsingNonlinearBases.html","id":"example-1-xgboost-pretraining","dir":"Articles","previous_headings":"","what":"Example 1: xgboost pretraining","title":"Using nonlinear bases","text":"start simulating data (n=1800n = 1800, p=1000p = 1000) continuous response. coefficients β\\beta sparse; first 200 entries drawn standard univariate normal, remainder 00. define yy y=1(X>0)β+ϵy = 1(X > 0) \\beta + \\epsilon, ϵ\\epsilon noise; hope xgboost learn splits corresponding X>0X > 0. Now, run xgboost get basis functions: ready model fitting cv.glmnet. two baselines (1) linear model pretrain xgboost, (2) xgboost. find glmnet together xgboost outperforms glmnet alone xgboost alone.","code":"require(xgboost) #> Loading required package: xgboost set.seed(1234)  n = 1800; p = 1000; noise = 5;  x     = matrix(rnorm(n * p), nrow=n, ncol=p) xtest = matrix(rnorm(n * p), nrow=n, ncol=p)  x.model     = 1*(x > 0)      xtest.model = 1*(xtest > 0)   beta = c(rnorm(200), rep(0, p-200))  y     = x.model %*% beta + noise * rnorm(n) ytest = xtest.model %*% beta + noise * rnorm(n)  train.folds = sample(rep(1:10, n/10)) xgbfit      = xgboost(data=x, label=y, nrounds=200, max_depth=1, verbose=0)  x.boost     = predict(xgbfit, x, predleaf = TRUE) - 1 xtest.boost = predict(xgbfit, xtest, predleaf = TRUE) - 1 cvfit = cv.glmnet(x.boost, y, type.measure = \"mse\", foldid = train.folds) cvfit.noboost = cv.glmnet(x, y, type.measure = \"mse\", foldid = train.folds)  cat(\"Lasso with xgboost pretraining PSE: \",      assess.glmnet(cvfit, newx = xtest.boost, newy = ytest)$mse) #> Lasso with xgboost pretraining PSE:  46.23225  cat(\"Lasso without xgboost pretraining PSE: \",      assess.glmnet(cvfit.noboost, newx = xtest, newy = ytest)$mse) #> Lasso without xgboost pretraining PSE:  60.68818  cat(\"xgboost alone PSE: \",      assess.glmnet(predict(xgbfit, xtest), newy = ytest)$mse) #> xgboost alone PSE:  49.47738"},{"path":"https://erincr.github.io/ptLasso/articles/UsingNonlinearBases.html","id":"example-2-xgboost-pretraining-with-input-groups","dir":"Articles","previous_headings":"","what":"Example 2: xgboost pretraining with input groups","title":"Using nonlinear bases","text":"Now, let’s repeat supposing data input groups. difference use cv.ptLasso model instead cv.glmnet, use group indicators feature fitting xgboost. start simulating data 3 groups (600600 observations group) continuous response. , simulate yy y=1(X>0)β+ϵy = 1(X > 0) \\beta + \\epsilon, now different β\\beta group. coefficients groups Table @ref(tab:nonlinear). Coefficients simulating data use xgboost pretraining dummy variables group indicators; use fit predict xgboost. Now, let’s train xgboost predict get new features. Note now use max_depth = 2: intended allow interactions group indicators features. Finally, ready fit two models trained cv.ptLasso: one uses xgboost features . , find pretraining xgboost improves performance relative (1) model fitting original feature space (2) xgboost alone.","code":"set.seed(1234)  n = 1800; p = 500; k = 3; noise = 5;  groups = groupstest = sort(rep(1:k, n/k))  x     = matrix(rnorm(n * p), nrow=n, ncol=p) xtest = matrix(rnorm(n * p), nrow=n, ncol=p)  x.model     = 1*(x > 0)      xtest.model = 1*(xtest > 0)  common.beta = c(rep(2, 50), rep(0, p-50)) beta.1 = c(rep(0, 50),  rep(1, 50), rep(0, p-100))  beta.2 = c(rep(0, 100), rep(1, 50), rep(0, p-150))  beta.3 = c(rep(0, 150), rep(1, 50), rep(0, p-200))   y = x.model %*% common.beta + noise * rnorm(n) y[groups == 1] = y[groups == 1] + x.model[groups == 1, ] %*% beta.1 y[groups == 2] = y[groups == 2] + x.model[groups == 2, ] %*% beta.2 y[groups == 3] = y[groups == 3] + x.model[groups == 3, ] %*% beta.3  ytest = xtest.model %*% common.beta + noise * rnorm(n) ytest[groups == 1] = ytest[groups == 1] + xtest.model[groups == 1, ] %*% beta.1 ytest[groups == 2] = ytest[groups == 2] + xtest.model[groups == 2, ] %*% beta.2 ytest[groups == 3] = ytest[groups == 3] + xtest.model[groups == 3, ] %*% beta.3 group.ids     = model.matrix(~as.factor(groups) - 1)  grouptest.ids = model.matrix(~as.factor(groupstest) - 1)  colnames(grouptest.ids) = colnames(group.ids) xgbfit      = xgboost(data=cbind(x, group.ids), label=y,                        nrounds=200, max_depth=2, verbose=0)  x.boost     = predict(xgbfit, cbind(x, group.ids), predleaf = TRUE) - 1 xtest.boost = predict(xgbfit, cbind(xtest, grouptest.ids), predleaf = TRUE) - 1 cvfit = cv.ptLasso(x.boost, y, groups=groups, type.measure = \"mse\") preds = predict(cvfit, xtest.boost, groups=groupstest, alphatype = \"varying\") preds = preds$yhatpre  cvfit.noboost = cv.ptLasso(x, y, groups=groups, type.measure = \"mse\") preds.noboost = predict(cvfit.noboost, xtest, groups=groupstest,                          alphatype = \"varying\") preds.noboost = preds.noboost$yhatpre  cat(\"ptLasso with xgboost pretraining PSE: \",      assess.glmnet(preds, newy = ytest)$mse) #> ptLasso with xgboost pretraining PSE:  55.1535  cat(\"ptLasso without xgboost pretraining PSE: \",      assess.glmnet(preds.noboost, newy = ytest)$mse) #> ptLasso without xgboost pretraining PSE:  66.37259  cat(\"xgboost alone PSE: \",      assess.glmnet(predict(xgbfit, xtest), newy = ytest)$mse) #> xgboost alone PSE:  59.63781"},{"path":"https://erincr.github.io/ptLasso/articles/ptLasso.html","id":"introduction-to-pretraining","dir":"Articles","previous_headings":"","what":"Introduction to pretraining","title":"ptLasso Vignette and Manual","text":"Suppose dataset spanning ten cancers want fit lasso penalized Cox model predict survival time. cancer classes dataset large (e.g. breast, lung) small (e.g. head neck). two obvious approaches: (1) fit “pancancer model” entire training set use make predictions cancer classes (2) fit separate (class specific) model cancer use make predictions class . Pretraining (Craig et al. (2024)) method bridges two options; parameter allows fit pancancer model, class specific models, everything . ptLasso package fits pretrained models using glmnet package (Friedman, Tibshirani, Hastie (2010)), including lasso, elasticnet ridge models. example dataset consisting ten different cancers called input grouped. grouping rows XX row belongs one cancer classes. want fit specific model ten cancers, also want share information across cancers. Importantly, pretraining general method pass information one model another – many uses beyond already discussed , including time series data, multi-response data conditional average treatment effect estimation. modeling tasks supported ptLasso package, final section vignette shows pretraining using glmnet package. remainder introduction describes input grouped setting. describe pretraining detail, first give quick review lasso.","code":""},{"path":"https://erincr.github.io/ptLasso/articles/ptLasso.html","id":"review-of-the-lasso","dir":"Articles","previous_headings":"Introduction to pretraining","what":"Review of the lasso","title":"ptLasso Vignette and Manual","text":"Gaussian family data (xi,yi),=1,2,…n(x_i,y_i), =1,2,\\ldots n, lasso form $$\\begin{equation} {\\rm argmin}_{\\beta_0, \\beta} \\frac{1}{2} \\sum_{=1}^n(y_i- \\beta_0 -\\sum_{j=1}^p x_{ij}\\beta_j)^2 + \\lambda \\sum_{j=1}^p |\\beta_j |. \\end{equation}$$ Varying regularization parameter λ≥0\\lambda \\ge 0 yields path solutions: optimal value λ̂\\hat\\lambda usually chosen cross-validation, using example cv.glmnet function package glmnet. GLMs ℓ1\\ell_1-regularized GLMs, one can include offset: pre-specified nn-vector included additional column feature matrix, whose weight βj\\beta_j fixed 1. Secondly, one can generalize ℓ1\\ell_1 norm weighted norm, taking form $$\\begin{equation} \\sum_j {\\rm pf}_j |\\beta_j | \\end{equation}$$ ${\\rm pf}_j \\ge 0$ penalty factor feature jj. extremes, penalty factor zero implies penalty means feature always included model; penalty factor +∞+\\infty leads feature discarded (.e., never entered model).","code":""},{"path":"https://erincr.github.io/ptLasso/articles/ptLasso.html","id":"details-of-pretraining","dir":"Articles","previous_headings":"Introduction to pretraining","what":"Details of pretraining","title":"ptLasso Vignette and Manual","text":"input grouped setting, pretraining model fitting happens two steps. First, train model using full data: μ̂0,θ̂1,…,θ̂k,β̂0=argminμ0,θ1,…,θk,β012∑k=1K∥yk−(μ0𝟏+θk𝟏+Xkβ0)∥22+λ||β||1,\\begin{equation}     \\hat{\\mu}_0, \\hat{\\theta}_1, \\dots, \\hat{\\theta}_k, \\hat{\\beta}_0 = \\arg \\min_{\\mu_0, \\theta_1, \\dots, \\theta_k, \\beta_0} \\frac{1}{2} \\sum_{k=1}^K \\| y_k - \\left(\\mu_0 \\mathbf{1} + \\theta_k \\mathbf{1} + X_k \\beta_0\\right) \\|_2^2 + \\lambda ||\\beta||_1, \\end{equation} : Xk,ykX_k, y_k observations group kk, θk\\theta_k group specific intercept group kk (convention, θ̂1=0\\hat{\\theta}_1 = 0), μ,β\\mu, \\beta overall intercept coefficients, λ\\lambda parameter chosen (perhaps value minimizing CV error). Define S(β̂0)S(\\hat\\beta_0) support set (nonzero coefficients) β̂0\\hat{\\beta}_0. , group kk, fit individual model: find β̂k\\hat{\\beta}_k μ̂k\\hat{\\mu}_k $$\\begin{eqnarray} && \\hat{\\mu}_k, \\hat{\\beta}_k = \\arg \\min_{\\mu_k, \\beta_k} \\frac{1}{2}  \\| y_k - (1-\\alpha) \\left(\\hat{\\mu}_0 \\mathbf{1} + \\hat{\\theta}_k \\mathbf{1} + X_k \\hat{\\beta}_0\\right) - (\\mu_k \\mathbf{1} + X_k \\beta_k) \\|_2^2 + \\cr && \\phantom{\\hat{\\mu}_k, \\hat{\\beta}_k} \\lambda_2 \\sum_{j=1}^p \\Bigl[ (j \\S(\\hat{\\beta}_0))+ \\frac{1}{\\alpha} (j \\notin S(\\hat{\\beta}_0))  \\Bigr] |\\beta_{kj}|, \\label{eq:model} \\end{eqnarray}$$ λ2>0\\lambda_2 > 0 α∈[0,1]\\alpha\\[0,1] hyperparameters may chosen cross validation. lasso linear regression model two additional components: offset (1−α)(μ̂0𝟏+θ̂k𝟏+Xkβ̂0)(1-\\alpha) \\left(\\hat{\\mu}_0 \\mathbf{1} + \\hat{\\theta}_k \\mathbf{1} + X_k \\hat{\\beta}_0\\right), penalty factor coefficient jj 1 j∈S(β̂0)j \\S(\\hat{\\beta}_0) 1α\\frac{1}{\\alpha} otherwise. Notice α=0\\alpha=0, returns overall model fine tuned group: second stage model allowed fit residual yk−(μ̂0𝟏+θ̂k𝟏+Xkβ̂0)y_k - \\left(\\hat{\\mu}_0 \\mathbf{1} + \\hat{\\theta}_k \\mathbf{1} + X_k \\hat{\\beta}_0\\right), penalty factor allows use βj\\beta_j already selected overall model. extreme, α=1\\alpha=1, equivalent fitting separate model class. offset, lasso penalty 1 features (usual lasso penalty).","code":""},{"path":"https://erincr.github.io/ptLasso/articles/ptLasso.html","id":"ptlasso-under-the-hood","dir":"Articles","previous_headings":"Introduction to pretraining","what":"ptLasso under the hood","title":"ptLasso Vignette and Manual","text":"model fitting ptLasso done cv.glmnet. first step pretraining straightforward call cv.glmnet; second step done calling cv.glmnet : offset (1−α)(μ0̂𝟏+θ̂k𝟏+Xkβ0̂)(1-\\alpha) \\left(\\hat{\\mu_0} \\mathbf{1} + \\hat{\\theta}_k \\mathbf{1} + X_k \\hat{\\beta_0}\\right) penalty.factor, jthj^\\text{th} entry 11 j∈S(β0̂)j \\S(\\hat{\\beta_0}) 1α\\frac{1}{\\alpha} otherwise. ptLasso uses cv.glmnet, inherits virtues glmnet package: example, handles sparse input-matrix formats, well range constraints coefficients. Additionally, one call ptLasso fits overall model, pretrained class specific models, class specific models group (without pretraining). ptLasso package also includes methods prediction plotting, function performs K-fold cross-validation.","code":""},{"path":[]},{"path":"https://erincr.github.io/ptLasso/articles/ptLasso.html","id":"ptlasso-uses-the-same-syntax-as-glmnet","dir":"Articles","previous_headings":"Quick start","what":"ptLasso uses the same syntax as glmnet","title":"ptLasso Vignette and Manual","text":"familiar glmnet, ptLasso similar structure: ptLasso functions train, plot predict, follows syntax glmnet. Additionally, ptLasso parameter α\\alpha analogous elasticnet parameter also called α\\alpha. avoid confusion, refer elasticnet parameter αen\\alpha_{\\text{en}}. αen\\alpha_{\\text{en}} glmnet, must specify value α\\alpha want use calling ptLasso; default α=0.5\\alpha = 0.5. big differences ptLasso glmnet: ptLasso calls cv.glmnet hood: cross validation λ\\lambda done automatically, ptLasso package includes cv.ptLasso: function cross validation α\\alpha. cross validation, typical ptLasso pipeline looks like: predict function uses value α\\alpha achieved best average CV performance across groups. possible instead use different α\\alpha group (specifically α\\alpha achieved best CV performance group). example end section.","code":"# The typical glmnet pipeline: train, plot and predict, # using elasticnet parameter 0.2. fit = glmnet(X, y, alpha = 0.2) plot(fit) test.preds = predict(fit, Xtest)  # The typical ptLasso pipeline: train, plot and predict, # using pretraining parameter 0.5. fit = ptLasso(X, y, groups, alpha = 0.5) plot(fit) test.preds = predict(fit, Xtest, groupstest) fit = cv.ptLasso(X, y, groups, alpha = 0.5) plot(fit) test.preds = predict(fit, Xtest, groupstest)"},{"path":"https://erincr.github.io/ptLasso/articles/ptLasso.html","id":"an-example","dir":"Articles","previous_headings":"Quick start","what":"An example","title":"ptLasso Vignette and Manual","text":"First, load ptLasso package: show use ptLasso, ’ll simulate data 5 groups continuous response using helper function gaussian.example.data. n=200n = 200 observations group p=120p = 120 features. groups share 10 informative features; though features shared, different coefficient values. group 10 additional features specific group, features uninformative. Now ready fit model using ptLasso. ’ll use pretraining parameter α=0.5\\alpha = 0.5 (randomly chosen). function ptLasso used cv.glmnet fit 11 models: overall model (using 5 groups), 5 pretrained models (one group) 5 individual models (one group). call plot displays cross validation curves model. top row shows overall model, middle row pretrained models, bottom row individual models.  predict makes predictions 1111 models. returns list containing: yhatoverall (predictions overall model), yhatpre (predictions pretrained models) yhatind (predictions individual models). default, predict uses lambda.min 1111cv.glmnet models; instead specify s = lambda.1se use numeric value. Whatever value λ\\lambda choose used models (overall, pretrained individual). also provide ytest (model validation), predict additionally compute performance measures. access coefficients fitted models, use coef usual. returns list coefficients individual models, pretrained models overall models, returned glmnet. entries individual pretrained models lists one entry group. 5 groups, ’ll 5 sets coefficients. first coefficients group 1 pretrained model : used ptLasso fit model, chose α=0.5\\alpha = 0.5. practice recommend choosing α\\alpha thoughtfully using (1) validation set measure performance different choices α\\alpha (e.g. 0,0.25,0.5,0.75,1.00, 0.25, 0.5, 0.75, 1.0) (2) function cv.ptLasso. call cv.ptLasso nearly identical ptLasso. default, cv.ptLasso try α=0,0.1,0.2,…,1\\alpha = 0, 0.1, 0.2, \\dots, 1, can changed argument alphalist. fitting, printing cv.ptLasso object shows cross validated mean squared error models. Plotting cv.ptLasso object visualizes performance function α\\alpha.  , ptLasso, can predict. default, predict uses α\\alpha minimized cross validated MSE. instead use argument alphatype = \"varying\" use different α\\alpha group – choose α\\alpha minimizes CV MSE group:","code":"require(ptLasso) #> Loading required package: ptLasso #> Loading required package: ggplot2 #> Loading required package: glmnet #> Loading required package: Matrix #> Loaded glmnet 4.1-8 #> Loading required package: gridExtra set.seed(1234)  out = gaussian.example.data() x = out$x; y = out$y; groups = out$groups  outtest = gaussian.example.data() xtest = outtest$x; ytest = outtest$y; groupstest = outtest$groups fit <- ptLasso(x, y, groups, alpha = 0.5) plot(fit) preds = predict(fit, xtest, groupstest=groupstest) preds = predict(fit, xtest, groupstest=groupstest, ytest=ytest) preds #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest)  #>  #>  #> alpha =  0.5  #>  #> Performance (Mean squared error): #>  #>            allGroups  mean group_1 group_2 group_3 group_4 group_5    r^2 #> Overall        755.7 755.7   836.0   554.9   565.4   777.9  1044.0 0.5371 #> Pretrain       503.2 503.2   550.6   443.3   553.5   505.6   462.9 0.6918 #> Individual     532.8 532.8   584.1   443.2   567.2   550.5   518.9 0.6736 #>  #> Support size: #>                                           #> Overall    64                             #> Pretrain   94 (21 common + 73 individual) #> Individual 109 all.coefs = coef(fit, s= \"lambda.min\") names(all.coefs) #> [1] \"individual\" \"pretrain\"   \"overall\" length(all.coefs$pretrain) #> [1] 5 head(all.coefs$pretrain[[1]]) #> 6 x 1 sparse Matrix of class \"dgCMatrix\" #>                     s1 #> (Intercept)  0.5088629 #> V1          -4.0203684 #> V2           .         #> V3           .         #> V4          -0.1923623 #> V5          -0.6581933 cvfit <- cv.ptLasso(x, y, groups) cvfit #>  #> Call:   #> cv.ptLasso(x = x, y = y, groups = groups, family = \"gaussian\",   #>     type.measure = \"mse\", use.case = \"inputGroups\", group.intercepts = TRUE)  #>  #>  #>  #> type.measure:  mse  #>  #>  #>            alpha overall  mean wtdMean group_1 group_2 group_3 group_4 group_5 #> Overall            696.4 696.4   696.4   739.5   500.8   566.4   669.4  1005.9 #> Pretrain     0.0   523.2 523.2   523.2   511.2   475.1   513.8   521.5   594.3 #> Pretrain     0.1   512.9 512.9   512.9   417.7   471.3   554.8   537.5   583.4 #> Pretrain     0.2   501.0 501.0   501.0   415.3   449.9   539.1   496.4   604.2 #> Pretrain     0.3   494.5 494.5   494.5   409.4   432.8   536.9   512.2   581.3 #> Pretrain     0.4   486.9 486.9   486.9   390.7   420.2   536.0   522.4   565.1 #> Pretrain     0.5   507.2 507.2   507.2   411.3   451.4   577.4   532.5   563.7 #> Pretrain     0.6   506.9 506.9   506.9   382.7   448.0   573.1   497.0   633.6 #> Pretrain     0.7   504.9 504.9   504.9   377.7   485.2   582.1   507.0   572.7 #> Pretrain     0.8   496.4 496.4   496.4   395.9   471.5   573.7   488.8   552.1 #> Pretrain     0.9   526.0 526.0   526.0   384.4   482.8   605.6   522.9   634.2 #> Pretrain     1.0   538.8 538.8   538.8   422.4   506.6   604.4   533.8   626.8 #> Individual         538.8 538.8   538.8   422.4   506.6   604.4   533.8   626.8 #>  #> alphahat (fixed) = 0.4 #> alphahat (varying): #> group_1 group_2 group_3 group_4 group_5  #>     0.7     0.4     0.0     0.8     0.8 plot(cvfit) preds = predict(cvfit, xtest, groupstest=groupstest, ytest=ytest) preds #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest)  #>  #>  #> alpha =  0.4  #>  #> Performance (Mean squared error): #>  #>            allGroups  mean group_1 group_2 group_3 group_4 group_5    r^2 #> Overall        757.1 757.1   815.7   542.6   567.1   792.7  1067.5 0.5362 #> Pretrain       511.1 511.1   579.7   460.1   547.5   502.9   465.6 0.6869 #> Individual     527.9 527.9   563.5   441.8   567.2   548.0   518.9 0.6766 #>  #> Support size: #>                                           #> Overall    50                             #> Pretrain   86 (29 common + 57 individual) #> Individual 109 preds = predict(cvfit, xtest, groupstest=groupstest, ytest=ytest,                  alphatype=\"varying\") preds #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest, alphatype = \"varying\")  #>  #>  #> alpha: #> [1] 0.7 0.4 0.0 0.8 0.8 #>  #>  #> Performance (Mean squared error): #>            overall  mean wtdMean group_1 group_2 group_3 group_4 group_5 #> Overall      757.1 757.1   757.1   815.7   542.6   567.1   792.7  1067.5 #> Pretrain     505.0 505.0   505.0   502.6   460.1   542.4   537.9   481.8 #> Individual   527.9 527.9   527.9   563.5   441.8   567.2   548.0   518.9 #>  #>  #> Support size: #>                                            #> Overall    50                              #> Pretrain   103 (29 common + 74 individual) #> Individual 109"},{"path":[]},{"path":"https://erincr.github.io/ptLasso/articles/ptLasso.html","id":"choosing-alpha-the-pretraining-parameter","dir":"Articles","previous_headings":"Other details","what":"Choosing α\\alpha, the pretraining parameter","title":"ptLasso Vignette and Manual","text":"Selecting parameter α\\alpha important part pretraining. simplest way use cv.ptLasso – automatically perform pretraining range α\\alpha values return CV performance . default values α\\alpha 0,0.1,0.2,…,10, 0.1, 0.2, \\dots, 1. course, can specify values α\\alpha consider: prediction time, cv.ptLasso uses α\\alpha best CV performance average across groups. instead choose use different α\\alpha group, cv.ptLasso already figured α\\alpha optimizes CV performance group. use group-specific values α\\alpha, specify alphatype = \"varying\" prediction time. example, best group-specific α\\alpha values happen 0.50.5 – overall α\\alpha.","code":"cvfit <- cv.ptLasso(x, y, groups) cvfit #>  #> Call:   #> cv.ptLasso(x = x, y = y, groups = groups, family = \"gaussian\",   #>     type.measure = \"mse\", use.case = \"inputGroups\", group.intercepts = TRUE)  #>  #>  #>  #> type.measure:  mse  #>  #>  #>            alpha overall  mean wtdMean group_1 group_2 group_3 group_4 group_5 #> Overall            699.7 699.7   699.7   748.4   501.9   575.6   663.0  1009.9 #> Pretrain     0.0   518.6 518.6   518.6   470.1   471.5   547.0   540.7   563.7 #> Pretrain     0.1   506.0 506.0   506.0   429.7   452.1   538.7   551.1   558.3 #> Pretrain     0.2   495.3 495.3   495.3   393.6   460.6   565.5   530.9   526.1 #> Pretrain     0.3   490.4 490.4   490.4   390.4   436.5   546.3   511.6   567.4 #> Pretrain     0.4   487.5 487.5   487.5   383.7   438.8   545.6   509.4   560.3 #> Pretrain     0.5   481.2 481.2   481.2   364.9   429.7   548.5   513.4   549.7 #> Pretrain     0.6   504.1 504.1   504.1   393.1   460.0   586.4   531.9   549.0 #> Pretrain     0.7   511.5 511.5   511.5   393.2   462.7   584.3   492.9   624.3 #> Pretrain     0.8   509.1 509.1   509.1   382.4   496.2   597.9   503.4   565.6 #> Pretrain     0.9   501.5 501.5   501.5   404.0   481.6   581.9   488.3   552.0 #> Pretrain     1.0   517.1 517.1   517.1   409.1   488.9   612.7   484.7   590.1 #> Individual         517.1 517.1   517.1   409.1   488.9   612.7   484.7   590.1 #>  #> alphahat (fixed) = 0.5 #> alphahat (varying): #> group_1 group_2 group_3 group_4 group_5  #>     0.5     0.5     0.1     1.0     0.2 cvfit <- cv.ptLasso(x, y, groups, alphalist = c(0, 0.5, 1)) cvfit #>  #> Call:   #> cv.ptLasso(x = x, y = y, groups = groups, alphalist = c(0, 0.5,   #>     1), family = \"gaussian\", type.measure = \"mse\", use.case = \"inputGroups\",   #>     group.intercepts = TRUE)  #>  #>  #> type.measure:  mse  #>  #>  #>            alpha overall  mean wtdMean group_1 group_2 group_3 group_4 group_5 #> Overall            708.8 708.8   708.8   739.0   514.4   575.4   665.0  1050.1 #> Pretrain     0.0   524.4 524.4   524.4   481.7   485.7   529.4   526.8   598.4 #> Pretrain     0.5   496.3 496.3   496.3   365.0   448.5   569.3   507.5   591.1 #> Pretrain     1.0   526.4 526.4   526.4   399.4   513.5   611.8   492.9   614.6 #> Individual         526.4 526.4   526.4   399.4   513.5   611.8   492.9   614.6 #>  #> alphahat (fixed) = 0.5 #> alphahat (varying): #> group_1 group_2 group_3 group_4 group_5  #>     0.5     0.5     0.0     1.0     0.5 ############################################### # Common alpha for all groups: ############################################### predict(cvfit, xtest, groupstest, ytest=ytest) #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest)  #>  #>  #> alpha =  0.5  #>  #> Performance (Mean squared error): #>  #>            allGroups  mean group_1 group_2 group_3 group_4 group_5    r^2 #> Overall        757.1 757.1   815.7   542.6   567.1   792.7  1067.5 0.5362 #> Pretrain       507.0 507.0   556.6   446.3   556.6   504.1   471.4 0.6894 #> Individual     527.9 527.9   572.6   443.2   562.4   550.5   510.7 0.6766 #>  #> Support size: #>                                           #> Overall    50                             #> Pretrain   95 (25 common + 70 individual) #> Individual 110  ############################################### # Different alpha for each group: ############################################### predict(cvfit, xtest, groupstest, ytest=ytest, alphatype = \"varying\") #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest, alphatype = \"varying\")  #>  #>  #> alpha: #> [1] 0.5 0.5 0.0 1.0 0.5 #>  #>  #> Performance (Mean squared error): #>            overall  mean wtdMean group_1 group_2 group_3 group_4 group_5 #> Overall      757.1 757.1   757.1   815.7   542.6   567.1   792.7  1067.5 #> Pretrain     517.3 517.3   517.3   556.6   446.3   561.5   550.5   471.4 #> Individual   527.9 527.9   527.9   572.6   443.2   562.4   550.5   510.7 #>  #>  #> Support size: #>                                           #> Overall    50                             #> Pretrain   99 (25 common + 74 individual) #> Individual 110"},{"path":"https://erincr.github.io/ptLasso/articles/ptLasso.html","id":"choosing-lambda-the-lasso-path-parameter-for-the-first-stage-of-pretraining","dir":"Articles","previous_headings":"Other details","what":"Choosing λ\\lambda, the lasso path parameter, for the first stage of pretraining","title":"ptLasso Vignette and Manual","text":"first step pretraining fits overall model cv.glmnet selects model along λ\\lambda path. second stage uses overall model’s support predictions train group-specific models. train time, need know choose value λ\\lambda use first stage. can specified ptLasso argument overall.lambda. default value “lambda.1se”, overall.lambda can accept “lambda.1se” “lambda.min”. Whatever choice made train time automatically used test time, changed. fitted model second stage pretraining expects offset computed using particular model – make sense compute offset using model different λ\\lambda.","code":"# Default: fit <- ptLasso(x, y, groups, alpha = 0.5, overall.lambda = \"lambda.1se\")  # Alternative: fit <- ptLasso(x, y, groups, alpha = 0.5, overall.lambda = \"lambda.min\")"},{"path":"https://erincr.github.io/ptLasso/articles/ptLasso.html","id":"fitting-elasticnet-or-ridge-models","dir":"Articles","previous_headings":"Other details","what":"Fitting elasticnet or ridge models","title":"ptLasso Vignette and Manual","text":"default, ptLasso fits lasso penalized models; glmnet, corresponds elasticnet parameter αen=1\\alpha_\\text{en} = 1 (subscript en stands “elasticnet”). Fitting pretrained elasticnet ridge models also possible ptLasso: use argument en.alpha 00 (ridge) 11 (lasso). example using pretraining parameter alpha = 0.5 elasticnet parameter en.alpha = 0.2.","code":"fit <- ptLasso(x, y, groups,                 alpha = 0.5,    # pretraining parameter                en.alpha = 0.2) # elasticnet parameter"},{"path":"https://erincr.github.io/ptLasso/articles/ptLasso.html","id":"printing-progress-during-model-training","dir":"Articles","previous_headings":"Other details","what":"Printing progress during model training","title":"ptLasso Vignette and Manual","text":"models take long time train, can useful print progress training. ptLasso two ways (can combined). First, can simply print model fitted using verbose = TRUE: can also print progress bar model fit – functionality comes directly cv.glmnet, follows notation. (avoid cluttering document, run following example.) course, can combine print (1) model trained (2) corresponding progress bar.","code":"fit <- ptLasso(x, y, groups, alpha = 0.5, verbose = TRUE) #> Fitting overall model #> Fitting individual models #>  Fitting individual model 1 / 5 #>  Fitting individual model 2 / 5 #>  Fitting individual model 3 / 5 #>  Fitting individual model 4 / 5 #>  Fitting individual model 5 / 5 #> Fitting pretrained lasso models #>  Fitting pretrained model 1 / 5 #>  Fitting pretrained model 2 / 5 #>  Fitting pretrained model 3 / 5 #>  Fitting pretrained model 4 / 5 #>  Fitting pretrained model 5 / 5 fit <- ptLasso(x, y, groups, alpha = 0.5, trace.it = TRUE) fit <- ptLasso(x, y, groups, alpha = 0.5, verbose = TRUE, trace.it = TRUE)"},{"path":"https://erincr.github.io/ptLasso/articles/ptLasso.html","id":"using-individual-and-overall-models-that-were-previously-trained","dir":"Articles","previous_headings":"Other details","what":"Using individual and overall models that were previously trained","title":"ptLasso Vignette and Manual","text":"ptLasso fit overall individual models. However, already trained overall individual models, can save compute time passing directly ptLasso – refitted. ptLasso expects models fitted using training data pass ptLasso, fitted argument keep = TRUE. example. fit overall model individual models, show pass ptLasso. Using verbose = TRUE call ptLasso shows us models trained (confirms refitting overall individual models). course pass just overall individual models `ptLasso:","code":"overall.model = cv.glmnet(x, y, keep = TRUE) individual.models = lapply(1:5,                             function(kk) cv.glmnet(x[groups == kk, ],                                                    y[groups == kk],                                                    keep = TRUE))  fit <- ptLasso(x, y, groups,                 fitoverall = overall.model,                fitind = individual.models,                verbose = TRUE) #> Fitting pretrained lasso models #>  Fitting pretrained model 1 / 5 #>  Fitting pretrained model 2 / 5 #>  Fitting pretrained model 3 / 5 #>  Fitting pretrained model 4 / 5 #>  Fitting pretrained model 5 / 5 fit <- ptLasso(x, y, groups, fitoverall = overall.model, verbose = TRUE) #> Fitting individual models #>  Fitting individual model 1 / 5 #>  Fitting individual model 2 / 5 #>  Fitting individual model 3 / 5 #>  Fitting individual model 4 / 5 #>  Fitting individual model 5 / 5 #> Fitting pretrained lasso models #>  Fitting pretrained model 1 / 5 #>  Fitting pretrained model 2 / 5 #>  Fitting pretrained model 3 / 5 #>  Fitting pretrained model 4 / 5 #>  Fitting pretrained model 5 / 5 fit <- ptLasso(x, y, groups, fitind = individual.models, verbose = TRUE) #> Fitting overall model #> Fitting pretrained lasso models #>  Fitting pretrained model 1 / 5 #>  Fitting pretrained model 2 / 5 #>  Fitting pretrained model 3 / 5 #>  Fitting pretrained model 4 / 5 #>  Fitting pretrained model 5 / 5"},{"path":"https://erincr.github.io/ptLasso/articles/ptLasso.html","id":"fitting-the-overall-model-without-group-specific-intercepts","dir":"Articles","previous_headings":"Other details","what":"Fitting the overall model without group-specific intercepts","title":"ptLasso Vignette and Manual","text":"fit overall model input grouped data, solve following: μ0̂,θ2̂,…,θK̂,β0̂=argminμ,θ2,…,θk,β12∑k=1K∥yk−(μ𝟏+θk𝟏+Xkβ)∥22+λ||β||1,\\begin{equation}     \\hat{\\mu_0}, \\hat{\\theta_2}, \\dots, \\hat{\\theta_K}, \\hat{\\beta_0} = \\arg \\min_{\\mu, \\theta_2, \\dots, \\theta_k, \\beta} \\frac{1}{2} \\sum_{k=1}^K \\| y_k - \\left(\\mu \\mathbf{1} + \\theta_k \\mathbf{1} + X_k \\beta\\right) \\|_2^2 + \\lambda ||\\beta||_1, \\end{equation} θ1̂\\hat{\\theta_1} defined 00. can instead omit θ1,…,θK\\theta_1, \\dots, \\theta_K instead fit following: μ0̂,β0̂=argminμ,β12∑k=1K∥yk−(μ𝟏+Xkβ)∥22+λ||β||1.\\begin{equation}     \\hat{\\mu_0}, \\hat{\\beta_0} = \\arg \\min_{\\mu, \\beta} \\frac{1}{2} \\sum_{k=1}^K \\| y_k - \\left(\\mu \\mathbf{1} + X_k \\beta\\right) \\|_2^2 + \\lambda ||\\beta||_1. \\end{equation} may useful settings groups different train test sets (see “Different groups train test data” “Input grouped data”). , use argument group.intercepts = FALSE.","code":"cvfit <- cv.ptLasso(x, y, groups, group.intercepts = FALSE) cvfit #>  #> Call:   #> cv.ptLasso(x = x, y = y, groups = groups, group.intercepts = FALSE,   #>     family = \"gaussian\", type.measure = \"mse\", use.case = \"inputGroups\")  #>  #>  #>  #> type.measure:  mse  #>  #>  #>            alpha overall  mean wtdMean group_1 group_2 group_3 group_4 group_5 #> Overall            696.2 696.2   696.2   694.7   489.2   580.5   670.0  1046.4 #> Pretrain     0.0   508.9 508.9   508.9   463.3   457.0   525.2   536.0   562.8 #> Pretrain     0.1   491.0 491.0   491.0   431.3   456.8   526.8   497.9   542.1 #> Pretrain     0.2   487.7 487.7   487.7   395.9   457.7   522.4   510.4   552.3 #> Pretrain     0.3   486.6 486.6   486.6   398.3   451.9   522.6   511.7   548.6 #> Pretrain     0.4   494.9 494.9   494.9   384.1   475.8   529.1   516.0   569.6 #> Pretrain     0.5   494.7 494.7   494.7   382.9   444.1   553.7   488.4   604.5 #> Pretrain     0.6   480.4 480.4   480.4   352.8   455.2   545.6   489.1   559.4 #> Pretrain     0.7   512.8 512.8   512.8   381.0   517.0   573.2   501.6   591.4 #> Pretrain     0.8   509.6 509.6   509.6   389.9   470.0   602.6   510.4   574.9 #> Pretrain     0.9   512.4 512.4   512.4   393.5   477.9   600.7   497.6   592.1 #> Pretrain     1.0   526.8 526.8   526.8   409.5   502.1   622.5   492.3   607.6 #> Individual         526.8 526.8   526.8   409.5   502.1   622.5   492.3   607.6 #>  #> alphahat (fixed) = 0.6 #> alphahat (varying): #> group_1 group_2 group_3 group_4 group_5  #>     0.6     0.5     0.2     0.5     0.1"},{"path":"https://erincr.github.io/ptLasso/articles/ptLasso.html","id":"arguments-for-use-in-cv-glmnet","dir":"Articles","previous_headings":"Other details","what":"Arguments for use in cv.glmnet","title":"ptLasso Vignette and Manual","text":"model fitting done cv.glmnet, ptLasso can take pass arguments cv.glmnet. Notable choices include penalty.factor, weights, upper.limits, lower.limits en.alpha (known alpha glmnet). Please refer glmnet documentation information use. ptLasso support arguments intercept, offset, fit check.args.","code":""},{"path":"https://erincr.github.io/ptLasso/articles/ptLasso.html","id":"parallelizing-model-fitting","dir":"Articles","previous_headings":"Other details","what":"Parallelizing model fitting","title":"ptLasso Vignette and Manual","text":"large datasets, can parallelize model fitting within calls cv.glmnet. cv.glmnet, pass argument parallel = TRUE, register parallel beforehand:","code":"require(doMC) registerDoMC(cores = 4) fit = ptLasso(x, y, groups = groups, family = \"gaussian\", type.measure = \"mse\",                parallel=TRUE)"},{"path":"https://erincr.github.io/ptLasso/articles/ptLasso.html","id":"input-grouped-data","dir":"Articles","previous_headings":"","what":"Input grouped data","title":"ptLasso Vignette and Manual","text":"","code":"require(ptLasso)"},{"path":"https://erincr.github.io/ptLasso/articles/ptLasso.html","id":"base-case-input-grouped-data-with-a-binomial-outcome","dir":"Articles","previous_headings":"Input grouped data","what":"Base case: input grouped data with a binomial outcome","title":"ptLasso Vignette and Manual","text":"Quick Start, applied ptLasso data continuous response. , ’ll use data binary outcome. creates dataset k=3k = 3 groups (100100 observations), 5 shared coefficients, 5 coefficients specific group. can fit predict . default, predict.ptLasso compute return deviance test set. instead compute AUC specifying type.measure call ptLasso. Note: type.measure specified model fitting prediction used call cv.glmnet. fit overall individual models, can use elasticnet instead lasso defining parameter en.alpha (glmnet described section “Fitting elasticnet ridge models”). Using cross validation Gaussian case:","code":"set.seed(1234)  out = binomial.example.data() x = out$x; y = out$y; groups = out$groups  outtest = binomial.example.data() xtest = outtest$x; ytest = outtest$y; groupstest = outtest$groups fit = ptLasso(x, y, groups, alpha = 0.5, family = \"binomial\")  predict(fit, xtest, groupstest, ytest = ytest) #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest)  #>  #>  #> alpha =  0.5  #>  #> Performance (Deviance): #>  #>            allGroups  mean wtdMean group_1 group_2 group_3 #> Overall        1.359 1.359   1.359   1.334   1.321   1.421 #> Pretrain       1.279 1.279   1.279   1.272   1.169   1.397 #> Individual     1.283 1.283   1.283   1.265   1.186   1.399 #>  #> Support size: #>                                         #> Overall    7                            #> Pretrain   12 (3 common + 9 individual) #> Individual 20 fit = ptLasso(x, y, groups, alpha = 0.5, family = \"binomial\",                type.measure = \"auc\")  predict(fit, xtest, groupstest, ytest = ytest) #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest)  #>  #>  #> alpha =  0.5  #>  #> Performance (AUC): #>  #>            allGroups   mean wtdMean group_1 group_2 group_3 #> Overall       0.6026 0.6039  0.6039  0.6161  0.6877  0.5080 #> Pretrain      0.6407 0.6524  0.6524  0.6936  0.7447  0.5190 #> Individual    0.6442 0.6618  0.6618  0.6936  0.7732  0.5186 #>  #> Support size: #>                                          #> Overall    15                            #> Pretrain   39 (3 common + 36 individual) #> Individual 40 fit = ptLasso(x, y, groups, alpha = 0.5, family = \"binomial\",                type.measure = \"auc\",                en.alpha = .5) predict(fit, xtest, groupstest, ytest = ytest) #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest)  #>  #>  #> alpha =  0.5  #>  #> Performance (AUC): #>  #>            allGroups   mean wtdMean group_1 group_2 group_3 #> Overall       0.6041 0.6018  0.6018  0.5928  0.6704  0.5422 #> Pretrain      0.6270 0.6547  0.6547  0.6781  0.7720  0.5141 #> Individual    0.6387 0.6598  0.6598  0.6756  0.7820  0.5218 #>  #> Support size: #>                                          #> Overall    3                             #> Pretrain   39 (3 common + 36 individual) #> Individual 36 ################################################## # Fit: ################################################## fit = cv.ptLasso(x, y, groups, family = \"binomial\", type.measure = \"auc\") #> Warning: from glmnet C++ code (error code -100); Convergence for 100th lambda #> value not reached after maxit=100000 iterations; solutions for larger lambdas #> returned #> Warning: from glmnet C++ code (error code -100); Convergence for 100th lambda #> value not reached after maxit=100000 iterations; solutions for larger lambdas #> returned #> Warning: from glmnet C++ code (error code -92); Convergence for 92th lambda #> value not reached after maxit=100000 iterations; solutions for larger lambdas #> returned #> Warning: from glmnet C++ code (error code -90); Convergence for 90th lambda #> value not reached after maxit=100000 iterations; solutions for larger lambdas #> returned  ################################################## # Predict with a common alpha for all groups: ################################################## predict(fit, xtest, groupstest, ytest = ytest) #>  #> Call:   #> predict.cv.ptLasso(object = fit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest)  #>  #>  #> alpha =  0.7  #>  #> Performance (AUC): #>  #>            allGroups   mean wtdMean group_1 group_2 group_3 #> Overall       0.5990 0.5960  0.5960  0.6030  0.6644  0.5206 #> Pretrain      0.6401 0.6640  0.6640  0.6965  0.7732  0.5222 #> Individual    0.6559 0.6707  0.6707  0.6936  0.7808  0.5377 #>  #> Support size: #>                                          #> Overall    7                             #> Pretrain   40 (3 common + 37 individual) #> Individual 37  ################################################## # Predict with a different alpha for each group: ################################################## predict(fit, xtest, groupstest, ytest = ytest, alphatype = \"varying\") #>  #> Call:   #> predict.cv.ptLasso(object = fit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest, alphatype = \"varying\")  #>  #>  #> alpha: #> [1] 0.2 0.5 0.2 #>  #>  #> Performance (AUC): #>            overall   mean wtdMean group_1 group_2 group_3 #> Overall     0.5990 0.5960  0.5960  0.6030  0.6644  0.5206 #> Pretrain    0.6359 0.6573  0.6573  0.6838  0.7736  0.5145 #> Individual  0.6559 0.6707  0.6707  0.6936  0.7808  0.5377 #>  #>  #> Support size: #>                                          #> Overall    7                             #> Pretrain   40 (3 common + 37 individual) #> Individual 37"},{"path":"https://erincr.github.io/ptLasso/articles/ptLasso.html","id":"base-case-input-grouped-survival-data","dir":"Articles","previous_headings":"Input grouped data","what":"Base case: input grouped survival data","title":"ptLasso Vignette and Manual","text":"Now, simulate survival times 3 groups; three groups overlapping support, 5 shared features 5 individual features. compute survival time, start computing survival=Xβ+ϵ\\text{survival} = X \\beta + \\epsilon, β\\beta specific group ϵ\\epsilon noise. survival times must positive, modify survival=survival+1.1*abs(min(survival))\\text{survival} = \\text{survival} + 1.1 * \\text{abs}(\\text{min}(\\text{survival})). Training ptLasso much continuous binomial cases; difference specify family = \"cox\". default, ptLasso uses partial likelihood model selection. instead use C index. call cv.ptLasso much ; need specify family (“cox”) type.measure (want use C index instead partial likelihood).","code":"require(survival) #> Loading required package: survival set.seed(1234)  n = 600; ntrain = 300 p = 50       x = matrix(rnorm(n*p), n, p) beta1 = c(rnorm(5), rep(0, p-5))  beta2 = runif(p) * beta1 # Shared support beta2 = beta2 + c(rep(0, 5), rnorm(5), rep(0, p-10)) # Individual features  beta3 = runif(p) * beta1 # Shared support beta3 = beta3 + c(rep(0, 10), rnorm(5), rep(0, p-15)) # Individual features  # Randomly split into groups groups = sample(1:3, n, replace = TRUE)  # Compute survival times: survival = x %*% beta1 survival[groups == 2] = x[groups == 2, ] %*% beta2 survival[groups == 3] = x[groups == 3, ] %*% beta3 survival = survival + rnorm(n) survival = survival + 1.1 * abs(min(survival))  # Censoring times from a random uniform distribution: censoring = runif(n, min = 1, max = 10)  # Did we observe surivival or censoring? y = Surv(pmin(survival, censoring), survival <= censoring)  # Split into train and test: xtest = x[-(1:300), ] ytest = y[-(1:300), ] groupstest = groups[-(1:300)]  x = x[1:300, ] y = y[1:300, ] groups = groups[1:300] ############################################################ # Default -- use partial likelihood as the type.measure: ############################################################ fit = ptLasso(x, y, groups, alpha = 0.5, family = \"cox\") predict(fit, xtest, groupstest, ytest = ytest) #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest)  #>  #>  #> alpha =  0.5  #>  #> Performance (Deviance): #>  #>            allGroups  mean wtdMean group_1 group_2 group_3 #> Overall        381.2 87.60   89.36   99.49  106.53   56.79 #> Pretrain       396.3 87.86   88.66   93.31   96.54   73.72 #> Individual     425.2 99.07   99.54  111.68  101.85   83.67 #>  #> Support size: #>                                          #> Overall    10                            #> Pretrain   20 (4 common + 16 individual) #> Individual 24  ############################################################ # Alternatively -- use the C index: ############################################################ fit = ptLasso(x, y, groups, alpha = 0.5, family = \"cox\", type.measure = \"C\") #> Warning: from glmnet C++ code (error code -30075); Numerical error at 75th #> lambda value; solutions for larger values of lambda returned predict(fit, xtest, groupstest, ytest = ytest) #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest)  #>  #>  #> alpha =  0.5  #>  #> Performance (C-index): #>  #>            allGroups   mean wtdMean group_1 group_2 group_3 #> Overall       0.8545 0.8673  0.8608  0.9139  0.7746  0.9133 #> Pretrain      0.8359 0.8396  0.8393  0.9152  0.8173  0.7864 #> Individual    0.7925 0.7985  0.8008  0.9075  0.8007  0.6873 #>  #> Support size: #>                                          #> Overall    6                             #> Pretrain   35 (4 common + 31 individual) #> Individual 37 ################################################## # Fit: ################################################## fit = cv.ptLasso(x, y, groups, family = \"cox\", type.measure = \"C\")  ################################################## # Predict with a common alpha for all groups: ################################################## predict(fit, xtest, groupstest, ytest = ytest) #>  #> Call:   #> predict.cv.ptLasso(object = fit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest)  #>  #>  #> alpha =  0.2  #>  #> Performance (C-index): #>  #>            allGroups   mean wtdMean group_1 group_2 group_3 #> Overall       0.8527 0.8652  0.8586  0.9113  0.7711  0.9133 #> Pretrain      0.8501 0.8795  0.8742  0.9177  0.8043  0.9164 #> Individual    0.7865 0.8005  0.8033  0.9126  0.8078  0.6811 #>  #> Support size: #>                                         #> Overall    8                            #> Pretrain   13 (4 common + 9 individual) #> Individual 31  ################################################## # Predict with a different alpha for each group: ################################################## predict(fit, xtest, groupstest, ytest = ytest, alphatype = \"varying\") #>  #> Call:   #> predict.cv.ptLasso(object = fit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest, alphatype = \"varying\")  #>  #>  #> alpha: #> [1] 0.3 0.4 0.4 #>  #>  #> Performance (C-index): #>            overall   mean wtdMean group_1 group_2 group_3 #> Overall     0.8527 0.8652  0.8586  0.9113  0.7711  0.9133 #> Pretrain    0.8081 0.8493  0.8475  0.9229  0.8078  0.8173 #> Individual  0.7865 0.8005  0.8033  0.9126  0.8078  0.6811 #>  #>  #> Support size: #>                                          #> Overall    8                             #> Pretrain   28 (4 common + 24 individual) #> Individual 31"},{"path":"https://erincr.github.io/ptLasso/articles/ptLasso.html","id":"different-groups-in-train-and-test-data","dir":"Articles","previous_headings":"Input grouped data","what":"Different groups in train and test data","title":"ptLasso Vignette and Manual","text":"Suppose observe groups test time unobserved train time. example, training set may consist KKpeople – many observations – test time, wish make predictions observations new people. can still use pretraining setting: train model using data, use guide training person-specific models. Now however, also fit extra model predict similarity test observations observations training people. train model, use (training) observation matrix XX response ysimy_{\\text{sim}}, ysim=ky_{\\text{sim}} = k observations kthk^\\text{th} person. used prediction, model gives us similarity (probability) vector length KK sums 1, describing similar observation training person. test time, make predictions (1) pretrained person-specific model (2) person-similarity model, compute weighted average pretrained predictions respect similarity vector. example using simulated data. ’re ready split train, validation test sets. use people 1, 2 3 training validation (two-thirds train, one-third validation), people 4 5 testing. start pretraining, person ID grouping variable. Now, train model predict person ID covariates. example simulated, can measure performance model test data (via confusion matrix comparing predicted group labels true labels). real settings, impossible. Finally can make predictions: everything need. test observation, get pretrained prediction 3 training classes. final predictions weighted combination predictions ptLasso class predictions glmnet. two reasonable baselines. first overall model grouping , second set individual models (one group). done – taking weighted average predictions respect similarity person – makes sense mathematically. However, found better empirical results instead train supervised learning algorithm make final prediction ŷ\\hat{y} using pretrained model predictions class similarity predictions features. , let’s , using -far-untouched validation set. Comparing performance models side--side shows (1) using input groups improved performance – including individual models (2) including final model help performance dramatically (still recommend trying real data).","code":"require(glmnet) require(ptLasso) set.seed(1234)  # Start with 5 people, each with 300 observations and 200 features. # 3 people will be used for training, and 2 for testing. n = 300*5; p = 200; groups = sort(rep(1:5, n/5))  # We will have different coefficients for each of the 3 training people,  # and the first 3 features are shared support. beta.group1 = c(-1, 1, 1, rep(0.5, 3), rep(0, p-6));  beta.group2 = c(-1, 1, 1, rep(0, 3), rep(0.5, 3), rep(0, p-9));  beta.group3 = c(-1, 1, 1, rep(0, 6), rep(0.5, 3), rep(0, p-12));   # The two test people are each a combination of of the training people. # Person 4 will have observations drawn from classes 1 and 2, and # Person 5 will have observations drawn from classes 1 and 3. # The vector \"hidden groups\" is a latent variable - used to simulate data # but unobserved in real data. hidden.gps = groups hidden.gps[hidden.gps == 4] = sample(c(1, 2), sum(groups == 4), replace = TRUE) hidden.gps[hidden.gps == 5] = sample(c(1, 3), sum(groups == 5), replace = TRUE)  # We modify X according to group membership; # we want X to cluster into groups 1, 2 and 3. x = matrix(rnorm(n * p), nrow = n, ncol = p) x[hidden.gps == 1, 1:3] = x[hidden.gps == 1, 1:3] + 1 x[hidden.gps == 2, 1:3] = x[hidden.gps == 2, 1:3] + 2 x[hidden.gps == 3, 1:3] = x[hidden.gps == 3, 1:3] + 3  # And now, we compute y using betas 1, 2 and 3:  x.beta = rep(0, n) x.beta[hidden.gps == 1] = x[hidden.gps == 1, ] %*% beta.group1  x.beta[hidden.gps == 2] = x[hidden.gps == 2, ] %*% beta.group2  x.beta[hidden.gps == 3] = x[hidden.gps == 3, ] %*% beta.group3 y = x.beta + 5 * rnorm(n) trn.index = groups < 4 val.sample = sample(1:sum(trn.index), 1/3 * sum(trn.index), replace = FALSE)  xtrain = x[trn.index, ][-val.sample, ] ytrain = y[trn.index][-val.sample] gpstrain = groups[trn.index][-val.sample]  xval   = x[trn.index, ][val.sample, ]  yval = y[trn.index][val.sample] gpsval = groups[trn.index][val.sample]  xtest  = x[!trn.index, ] ytest = y[!trn.index] gpstest = groups[!trn.index] cvfit = cv.ptLasso(xtrain, ytrain, gpstrain,                     type.measure = \"mse\",                     group.intercepts = FALSE,                    overall.lambda = \"lambda.1se\") simmod = cv.glmnet(xtrain, as.factor(gpstrain), family = \"multinomial\")  # Peek at performance on test data. # Not possible with real data. class.preds = predict(simmod, xtest, type=\"response\")[, , 1] table(apply(class.preds, 1, which.max),        hidden.gps[groups >= 4]) #>     #>       1   2   3 #>   1 260  37   3 #>   2  39  82  29 #>   3   0  36 114 alphahat  = cvfit$alphahat bestmodel = cvfit$fit[[which(cvfit$alphalist == alphahat)]]  offset = (1-alphahat) * predict(bestmodel$fitoverall, xtest, s = \"lambda.1se\")  # Get the prediction for all three classes for each test observation.  # This will be a matrix with three columns; one for each class. pretrained.preds = do.call(cbind,                          lapply(1:3,                                 function(i) predict(bestmodel$fitpre[[i]],                                                     xtest,                                                    newoffset = offset)                       ) )  assess.glmnet( rowSums(pretrained.preds * class.preds), newy = ytest)$mse #> [1] 28.17891 #> attr(,\"measure\") #> [1] \"Mean-Squared Error\" ######################################################## # Baseline 1: overall model ######################################################## overall.predictions = predict(cvfit$fitoverall, xtest) assess.glmnet(overall.predictions, newy = ytest)$mse #> lambda.1se  #>   29.64747  #> attr(,\"measure\") #> [1] \"Mean-Squared Error\"  ######################################################## # Baseline 2: individual models ######################################################## individual.preds = do.call(cbind,                             lapply(1:3,                                    function(i) predict(bestmodel$fitind[[i]],                                                        xtest,                                                       type = \"response\")                       ) ) assess.glmnet(rowSums(individual.preds * class.preds), newy = ytest)$mse #> [1] 29.17333 #> attr(,\"measure\") #> [1] \"Mean-Squared Error\" val.offset = predict(bestmodel$fitoverall, xval, s = \"lambda.1se\") val.offset = (1 - alphahat) * val.offset val.preds = do.call(cbind,                      lapply(1:3, function(i) predict(bestmodel$fitpre[[i]],                                                      xval,                                                     newoffset = val.offset,                                                     type = \"response\")                       ) ) val.class.preds = predict(simmod, xval)[, , 1]  pred.data = cbind(val.preds, val.class.preds, val.preds * val.class.preds)  final.model = cv.glmnet(pred.data, rowSums(val.preds * val.class.preds))  pred.data.test = cbind(pretrained.preds,                         class.preds,                         pretrained.preds * class.preds) assess.glmnet(predict(final.model, pred.data.test), newy = ytest)$mse #> lambda.1se  #>   28.28504  #> attr(,\"measure\") #> [1] \"Mean-Squared Error\" rd = function(x) round(x, 2)  cat(\"Overall model PSE: \",      rd(assess.glmnet(overall.predictions, newy = ytest)$mse)) #> Overall model PSE:  29.65 cat(\"Individual model PSE: \",      rd(assess.glmnet(rowSums(individual.preds*class.preds), newy = ytest)$mse)) #> Individual model PSE:  29.17 cat(\"Pretraining model PSE: \",      rd(assess.glmnet(rowSums(pretrained.preds*class.preds), newy = ytest)$mse)) #> Pretraining model PSE:  28.18 cat(\"Pretraining model + final prediction model PSE: \",      rd(assess.glmnet(predict(final.model,                            cbind(pretrained.preds,                                  class.preds,                                  pretrained.preds * class.preds)                       ),                newy = ytest)$mse)) #> Pretraining model + final prediction model PSE:  28.29"},{"path":"https://erincr.github.io/ptLasso/articles/ptLasso.html","id":"learning-the-input-groups","dir":"Articles","previous_headings":"Input grouped data","what":"Learning the input groups","title":"ptLasso Vignette and Manual","text":"Suppose dataset features XX response yy, input grouping. Suppose also small set meaningful features ZZ expect stratify observations (e.g. biomedicine, ZZ may consist age sex). setting, can learn input groups using ZZ. steps follows. Partition data two sets: one learn grouping one pretraining. first set, train small CART tree using ZZ yy. Make predictions remaining data; assign observations groups according terminal nodes. Apply pretraining using learned group assignments. , show example using simulated data. use rpart train CART tree. package ODRF (Liu Xia (2022)) another good choice – fits linear model terminal node, closer pretraining , may therefore better performance. Simulate data binary outcome: XX drawn random normal (p=50p = 50 uncorrelated features), ZZ simulated age (uniform 20 90) sex (half 0, half 1). true groups (1) age 50, (2) age 50 sex = 0 (3) age 50 sex = 1. Now, ’ll define coefficients βk\\beta_k P(yi=1∣xi)=11+exp(−xiTβk)P(y_i = 1 \\mid x_i) = \\frac{1}{1 + \\exp(-x_i^T \\beta_k)} group. Across groups, three coefficients shared, three group-specific rest 0. group unique intercept adjust baseline risk. design, P(y=1)P(y = 1) different across groups:  cluster using rpart. Note use maxdepth = 2: obvious choice simulated data know second-level interaction (age + sex) determines outcome. general, however, recommend keeping tree small (maxdepth smaller 4) easily interpretable. want tree return ID terminal node observation instead class probabilities. following trick causes predict behave desired. Finally, ready apply pretraining using predicted groups grouping variable. Note overall model trained cv.ptLasso takes advantage clustering: fits unique intercept group. Performance much worse hadn’t done clustering :","code":"require(rpart) #> Loading required package: rpart set.seed(1234)  n = 1000; p = 50 groupvars = cbind(age = round(runif(n, min = 20, max = 90)),                    sex = sample(c(0, 1), n, replace = TRUE)) groups = rep(1, n) groups[groupvars[, \"age\"] > 50 & groupvars[, \"sex\"] == 0] = 2 groups[groupvars[, \"age\"] > 50 & groupvars[, \"sex\"] == 1] = 3 beta.group1 = c(-0.5, 0.5, 0.1, c(0.1, 0.2, 0.3), rep(0, p-6));  beta.group2 = c(-0.5, 0.5, 0.1, rep(0, 3), c(0.1, 0.2, 0.3), rep(0, p-9));  beta.group3 = c(-0.5, 0.5, 0.1, rep(0, 6), c(0.1, 0.2, 0.3), rep(0, p-12));   x = matrix(rnorm(n * p), nrow = n, ncol = p) x.beta = rep(0, n) x.beta[groups == 1] = x[groups == 1, ] %*% beta.group1 - 0.75 x.beta[groups == 2] = x[groups == 2, ] %*% beta.group2  x.beta[groups == 3] = x[groups == 3, ] %*% beta.group3 + 0.75  y = rbinom(n, size = 1, prob = 1/(1 + exp(-x.beta)))  # Now that we have our data, we will partition it into 3 datasets:  # one to cluster, one to train models and one to test performance. xcluster = x[1:250, ]; xtrain = x[251:750, ]; xtest = x[751:1000, ]; ycluster = y[1:250];   ytrain = y[251:750];   ytest = y[751:1000];  zcluster = groupvars[1:250, ];  ztrain = groupvars[251:750, ];  ztest = groupvars[751:1000, ];  # We will use this just to see how our clustering performed. # Not possible with real data! groupstrain = groups[251:750]; ggplot() +    geom_boxplot(aes(x=groups, y=1/(1 + exp(-x.beta)), group = groups)) +   labs(x = \"Group\", y = \"P(y = 1)\") +   theme_minimal() treefit = rpart(ycluster~.,                  data = data.frame(zcluster, ycluster),                  control=rpart.control(maxdepth=2, minbucket=20)) treefit #> n= 250  #>  #> node), split, n, deviance, yval #>       * denotes terminal node #>  #> 1) root 250 61.82400 0.4480000   #>   2) age< 50.5 111 23.18919 0.2972973 * #>   3) age>=50.5 139 34.10072 0.5683453   #>     6) sex< 0.5 56 13.92857 0.4642857 * #>     7) sex>=0.5 83 19.15663 0.6385542 * leaf=treefit$frame[,1]==\"<leaf>\"    treefit$frame[leaf,\"yval\"]=1:sum(leaf)  predgroupstrain = predict(treefit, data.frame(ztrain)) predgroupstest  = predict(treefit, data.frame(ztest)) cvfit = cv.ptLasso(xtrain, ytrain, predgroupstrain, family = \"binomial\",                     type.measure = \"auc\", nfolds = 10,                     overall.lambda = \"lambda.min\") predict(cvfit, xtest, predgroupstest, ytest = ytest) #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, groupstest = predgroupstest,   #>     ytest = ytest)  #>  #>  #> alpha =  0  #>  #> Performance (AUC): #>  #>            allGroups   mean wtdMean group_1 group_2 group_3 #> Overall       0.7081 0.6448  0.6399  0.6085  0.6575  0.6684 #> Pretrain      0.7109 0.6590  0.6526  0.6147  0.6823  0.6800 #> Individual    0.7058 0.6525  0.6477  0.6085  0.6428  0.7063 #>  #> Support size: #>                                        #> Overall    8                           #> Pretrain   8 (8 common + 0 individual) #> Individual 19 baseline.model = cv.glmnet(xtrain, ytrain, family = \"binomial\", type.measure = \"auc\", nfolds = 5) assess.glmnet(baseline.model, newx=xtest, newy=ytest)$auc #> [1] 0.6050242 #> attr(,\"measure\") #> [1] \"AUC\""},{"path":"https://erincr.github.io/ptLasso/articles/ptLasso.html","id":"target-grouped-data","dir":"Articles","previous_headings":"","what":"Target grouped data","title":"ptLasso Vignette and Manual","text":"","code":"require(ptLasso)"},{"path":"https://erincr.github.io/ptLasso/articles/ptLasso.html","id":"intuition","dir":"Articles","previous_headings":"Target grouped data","what":"Intuition","title":"ptLasso Vignette and Manual","text":"Now turn target grouped setting, dataset multinomial outcome grouping observations. example, data might look like following: row XX belongs class 1, 2 3, wish predict class membership. fit single multinomial model data: , fit 3 one-vs-rest models; prediction time, assign observations class highest probability. Another alternative pretraining, fits something one model data three separate models. ptLasso , using arguments family = \"multinomial\" use.case = \"targetGroups\". exactly pretraining ? ’ll walk example, pretraining “hand”. steps : Train overall model: multinomial model using penalty coefficients β\\beta coefficient either 0 nonzero classes. Train individual one-vs-rest models using penalty factor offset defined overall model (input grouped setting). train overall model, use cv.glmnet type.multinomial = \"grouped\". puts penalty β\\beta force coefficients model classes. analogous overall model input grouped setting: want first learn shared information. , fit 3 one-vs-rest models using support offset multinomial model. Now everything need train one-vs-rest models. always, pretraining parameter α\\alpha - example, let’s use α=0.5\\alpha = 0.5: ’re done pretraining! predict, assign row class highest prediction: done automatically within ptLasso; now show example using ptLasso functions. example intended show pretraining works multinomial outcomes, technical details omitted. (example, ptLasso takes care crossfitting first second steps.)","code":"set.seed(1234)  n = 500; p = 75; k = 3 X = matrix(rnorm(n * p), nrow = n, ncol = p) y = sample(1:k, n, replace = TRUE)  Xtest = matrix(rnorm(n * p), nrow = n, ncol = p) multinomial = cv.glmnet(X, y, family = \"multinomial\")  multipreds  = predict(multinomial, Xtest, s = \"lambda.min\") multipreds.class = apply(multipreds, 1, which.max) class1 = cv.glmnet(X, y == 1, family = \"binomial\") class2 = cv.glmnet(X, y == 2, family = \"binomial\") class3 = cv.glmnet(X, y == 3, family = \"binomial\")  ovrpreds = cbind(   predict(class1, Xtest, s = \"lambda.min\"),   predict(class2, Xtest, s = \"lambda.min\"),   predict(class3, Xtest, s = \"lambda.min\")) ovrpreds.class = apply(ovrpreds, 1, which.max) fit = ptLasso(X, y, groups = y, alpha = 0.5,               family = \"multinomial\",                use.case = \"targetGroups\") multinomial = cv.glmnet(X, y, family = \"multinomial\",                          type.multinomial = \"grouped\",                         keep = TRUE) # The support of the overall model: nonzero.coefs = which((coef(multinomial, s = \"lambda.1se\")[[1]] != 0)[-1])  # The offsets - one for each class: offset = predict(multinomial, X, s = \"lambda.1se\") offset.class1 = offset[, 1, 1] offset.class2 = offset[, 2, 1] offset.class3 = offset[, 3, 1] alpha = 0.5 penalty.factor = rep(1/alpha, p) penalty.factor[nonzero.coefs] = 1  class1 = cv.glmnet(X, y == 1, family = \"binomial\",                     offset = (1-alpha) * offset.class1,                    penalty.factor = penalty.factor) class2 = cv.glmnet(X, y == 2, family = \"binomial\",                     offset = (1-alpha) * offset.class2,                    penalty.factor = penalty.factor) class3 = cv.glmnet(X, y == 3, family = \"binomial\",                     offset = (1-alpha) * offset.class3,                    penalty.factor = penalty.factor) newoffset = predict(multinomial, X, s = \"lambda.1se\") ovrpreds = cbind(   predict(class1, Xtest, s = \"lambda.min\", newoffset = newoffset[, 1, 1]),   predict(class2, Xtest, s = \"lambda.min\", newoffset = newoffset[, 2, 1]),   predict(class3, Xtest, s = \"lambda.min\", newoffset = newoffset[, 3, 1]) ) ovrpreds.class = apply(ovrpreds, 1, which.max)"},{"path":"https://erincr.github.io/ptLasso/articles/ptLasso.html","id":"example","dir":"Articles","previous_headings":"Target grouped data","what":"Example","title":"ptLasso Vignette and Manual","text":"First, let’s simulate multinomial data 5 classes. start drawing XX normal distribution (uncorrelated features), shift columns differently group. calls ptLasso cv.ptLasso almost input grouped setting, now specify use.case = \"targetGroups\". call predict require groups argument groups unknown prediction time.","code":"set.seed(1234)  n = 500; p = 50; k = 5 class.sizes = rep(n/k, k) ncommon = 10; nindiv = 5; shift.common = seq(-.2, .2, length.out = k) shift.indiv  = seq(-.1, .1, length.out = k)  x     = matrix(rnorm(n * p), n, p) xtest = matrix(rnorm(n * p), n, p) y = ytest = c(sapply(1:length(class.sizes), function(i) rep(i, class.sizes[i])))  start = ncommon + 1 for (i in 1:k) {   end = start + nindiv - 1   x[y == i, 1:ncommon] = x[y == i, 1:ncommon] + shift.common[i]   x[y == i, start:end] = x[y == i, start:end] + shift.indiv[i]      xtest[ytest == i, 1:ncommon] = xtest[ytest == i, 1:ncommon] + shift.common[i]   xtest[ytest == i, start:end] = xtest[ytest == i, start:end] + shift.indiv[i]   start = end + 1 } ################################################################################ # Fit the pretrained model. # By default, ptLasso uses type.measure = \"deviance\", but for ease of # interpretability, we use type.measure = \"class\" (the misclassification rate). ################################################################################ fit = ptLasso(x = x, y = y,                use.case = \"targetGroups\", type.measure = \"class\")  ################################################################################ # Predict ################################################################################ predict(fit, xtest, ytest = ytest) #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, ytest = ytest)  #>  #>  #>  #> alpha =  0.5  #>  #> Performance (Misclassification error): #>  #>            overall   mean group_1 group_2 group_3 group_4 group_5 #> Overall      0.738                                                #> Pretrain     0.728 0.2000   0.200     0.2     0.2     0.2   0.200 #> Individual   0.736 0.1984   0.196     0.2     0.2     0.2   0.196 #>  #> Support size: #>                                          #> Overall    29                            #> Pretrain   23 (23 common + 0 individual) #> Individual 32  ################################################################################ # Fit with CV to choose the alpha parameter ################################################################################ cvfit = cv.ptLasso(x = x, y = y,               use.case = \"targetGroups\", type.measure = \"class\")  ################################################################################ # Predict using one alpha for all classes ################################################################################ predict(cvfit, xtest, ytest = ytest) #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, ytest = ytest)  #>  #>  #>  #> alpha =  0.9  #>  #> Performance (Misclassification error): #>  #>            overall   mean group_1 group_2 group_3 group_4 group_5 #> Overall      0.738                                                #> Pretrain     0.722 0.1992     0.2     0.2     0.2     0.2   0.196 #> Individual   0.742 0.2000     0.2     0.2     0.2     0.2   0.200 #>  #> Support size: #>                                          #> Overall    39                            #> Pretrain   32 (23 common + 9 individual) #> Individual 36  ################################################################################ # Predict using a separate alpha for each class ################################################################################ predict(cvfit, xtest, ytest = ytest, alphatype = \"varying\") #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, ytest = ytest,   #>     alphatype = \"varying\")  #>  #>  #> alpha =  0.1 0 0.7 0 0.1  #>  #> Performance (Misclassification error): #>  #>            overall   mean group_1 group_2 group_3 group_4 group_5 #> Overall      0.738                                                #> Pretrain     0.742 0.2016   0.208     0.2     0.2   0.202   0.198 #> Individual   0.742 0.2000   0.200     0.2     0.2   0.200   0.200 #>  #> Support size: #>                                           #> Overall    39                             #> Pretrain   36 (23 common + 13 individual) #> Individual 36"},{"path":"https://erincr.github.io/ptLasso/articles/ptLasso.html","id":"multi-response-data-with-gaussian-responses","dir":"Articles","previous_headings":"","what":"Multi-response data with Gaussian responses","title":"ptLasso Vignette and Manual","text":"Multitask learning consists data XX two responses y1,…,yjy_1, \\dots, y_j. usually assume shared signal across responses, performance can improved jointly fitting models responses. , suppose wish predict multiple Gaussian responses. (goal predict multiple responses different type, see section “Multi-response data mixed response types”.) Pretraining natural choice multitask learning – allows us pass information models different responses. overview approach : fit multi-response Gaussian model using group lasso penalty (), extract support (shared across responses) offsets (one response), fit model response, using shared support appropriate offset. Importantly, group lasso penalty behaves like lasso, whole group coefficients response: either zero, else none zero (see glmnet documentation family = \"mgaussian\" detail). result, multi-response Gaussian model forced choose support responses y1,…,yjy_1, \\dots, y_j. encourages learning across responses first stage; second stage, find features specific individual response yky_k. done function ptLasso, using argument use.case = \"multiresponse\". illustrate simulated data two Gaussian responses; two responses share first 5 features, 5 features . two responses quite related, Pearson correlation around 0.5. Now, ready call ptLasso covariates x response matrix y, specify argument use.case = \"multiresponse\". call plot shows CV curves lasso parameter λ\\lambda model.  choose pretraining parameter α\\alpha, can use cv.ptLasso. Using plot, can view CV curve pretraining together overall model (multi-response Gaussian model) individual model (separate Gaussian model response).  previous examples, can predict using predict; ytest supplied, print mean squared error well support size pretrained, overall individual models using single α\\alpha minimizes average CV MSE across responses. Also , can choose use value α\\alpha minimizes CV MSE response.","code":"require(ptLasso) set.seed(1234)  # Define constants n = 1000         # Total number of samples ntrain = 650     # Number of training samples p = 500          # Number of features sigma = 2        # Standard deviation of noise        # Generate covariate matrix x = matrix(rnorm(n * p), n, p)  # Define coefficients for responses 1 and 2 beta1 = c(rep(1, 5), rep(0.5, 5), rep(0, p - 10)) beta2 = c(rep(1, 5), rep(0, 5), rep(0.5, 5), rep(0, p - 15))  mu = cbind(x %*% beta1, x %*% beta2) y  = cbind(mu[, 1] + sigma * rnorm(n),             mu[, 2] + sigma * rnorm(n))  cat(\"SNR for the two tasks:\", round(diag(var(mu)/var(y-mu)), 2)) #> SNR for the two tasks: 1.6 1.44 cat(\"Correlation between two tasks:\", cor(y[, 1], y[, 2])) #> Correlation between two tasks: 0.5164748  # Split into train and test xtest = x[-(1:ntrain), ] ytest = y[-(1:ntrain), ]  x = x[1:ntrain, ] y = y[1:ntrain, ] fit = ptLasso(x, y, use.case = \"multiresponse\") plot(fit) fit = cv.ptLasso(x, y, use.case = \"multiresponse\") plot(fit) preds = predict(fit, xtest, ytest = ytest) preds #>  #> Call:   #> predict.cv.ptLasso(object = fit, xtest = xtest, ytest = ytest)  #>  #>  #>  #> alpha =  0.3  #>  #> Performance (Mean squared error): #>  #>            allGroups  mean response_1 response_2 #> Overall        9.217 4.608      4.092      5.125 #> Pretrain       9.006 4.503      4.149      4.857 #> Individual     9.324 4.662      4.168      5.157 #>  #> Support size: #>                                          #> Overall    57                            #> Pretrain   22 (20 common + 2 individual) #> Individual 75 preds = predict(fit, xtest, ytest = ytest, alphatype = \"varying\") preds #>  #> Call:   #> predict.cv.ptLasso(object = fit, xtest = xtest, ytest = ytest,   #>     alphatype = \"varying\")  #>  #>  #> alpha: #> [1] 0.3 0.3 #>  #>  #> Performance (Mean squared error): #>            allGroups  mean response_1 response_2 #> Overall        9.217 4.608      4.092      5.125 #> Pretrain       9.006 4.503      4.149      4.857 #> Individual     9.324 4.662      4.168      5.157 #>  #>  #> Support size: #>                                          #> Overall    57                            #> Pretrain   22 (20 common + 2 individual) #> Individual 75"},{"path":"https://erincr.github.io/ptLasso/articles/ptLasso.html","id":"time-series-data","dir":"Articles","previous_headings":"","what":"Time series data","title":"ptLasso Vignette and Manual","text":"may repeated measurements XX yy across time; example, may observe patients two different points time. expect relationship XX yy different time 1 time 2, completely unrelated. Therefore, pretraining can useful: can use model fitted time 1 inform model time 2. ptLasso supports setting, example. first assume XX constant across time, yy changes. Later, show example XX changes across time. pretraining time series data, : fit model time 1 extract offset support, use offset support (usual pretraining) train model time 2. continue kk time points: fitting model time 2, extract offset support. Now, offset include offset time 1 prediction time 2; support union supports first two models.","code":"require(ptLasso)"},{"path":"https://erincr.github.io/ptLasso/articles/ptLasso.html","id":"example-1-covariates-are-constant-over-time","dir":"Articles","previous_headings":"Time series data","what":"Example 1: covariates are constant over time","title":"ptLasso Vignette and Manual","text":"’ll start simulating data – details comments. simulated data, ready call ptLasso; call ptLasso looks much examples, now (1) yy matrix one column time point (2) specify use.case = \"timeSeries\". fitting, call plot shows models fitted time points without using pretraining.  , can predict xtest. example, pretraining helps performance: two time points share support, pretraining discovers leverages . specified alpha = 0 example, cross validation advise us choose α=0.2\\alpha = 0.2. Plotting shows us average performance across two time points. Importantly, time 1, individual model pretrained model ; see advantage pretraining time 2 (use information time 1).  Note also treated multireponse problem, ignored time-ordering responses. See section called “Multi-response data Gaussian responses”. (However, time ordering can informative, multi-response approach make use .)","code":"set.seed(1234)  # Define constants n = 600          # Total number of samples ntrain = 300     # Number of training samples p = 100          # Number of features sigma = 3        # Standard deviation of noise  # Generate covariate matrix x = matrix(rnorm(n * p), n, p)  # Define coefficients for time points 1 and 2 beta1 = c(rep(2, 10), rep(0, p - 10))  # Coefs at time 1 beta2 = runif(p, 0.5, 2) * beta1       # Coefs at time 2, shared support with time 1  # Generate response variables for times 1 and 2 y = cbind(   x %*% beta1 + sigma * rnorm(n),   x %*% beta2 + sigma * rnorm(n) )  # Split data into training and testing sets xtest = x[-(1:ntrain), ]  # Test covariates ytest = y[-(1:ntrain), ]  # Test response  x = x[1:ntrain, ]  # Train covariates y = y[1:ntrain, ]  # Train response fit = ptLasso(x, y, use.case = \"timeSeries\", alpha = 0) plot(fit) preds = predict(fit, xtest, ytest = ytest) preds #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, ytest = ytest)  #>  #>  #>  #> alpha =  0  #>  #> Performance (Mean squared error): #>  #>              mean response_1 response_2 #> Pretrain    9.604      10.78      8.428 #> Individual 10.428      10.78     10.076 #>  #> Support size: #>                                           #> Pretrain   26 (10 common + 16 individual) #> Individual 39 cvfit = cv.ptLasso(x, y, use.case = \"timeSeries\") plot(cvfit) predict(cvfit, xtest, ytest = ytest) #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, ytest = ytest)  #>  #>  #>  #> alpha =  0.2  #>  #> Performance (Mean squared error): #>  #>             mean response_1 response_2 #> Pretrain   10.62      10.87      10.37 #> Individual 10.45      10.87      10.03 #>  #> Support size: #>                                           #> Pretrain   28 (10 common + 18 individual) #> Individual 40 fit = ptLasso(x, y, use.case = \"multiresponse\")"},{"path":"https://erincr.github.io/ptLasso/articles/ptLasso.html","id":"example-2-covariates-change-over-time","dir":"Articles","previous_headings":"Time series data","what":"Example 2: covariates change over time","title":"ptLasso Vignette and Manual","text":"Now, ’ll repeat , ’ll simulate data xx changes time. setting, ptLasso expects xx list one covariate matrix time. Now, xx list length two: can call ptLasso, cv.ptLasso, plot predict just :","code":"set.seed(1234)  # Set seed for reproducibility  # Define constants n = 600          # Total number of samples ntrain = 300     # Number of training samples p = 100          # Number of features sigma = 3        # Standard deviation of noise  # Covariates for times 1 and 2 x1 = matrix(rnorm(n * p), n, p) x2 = x1 + matrix(0.2 * rnorm(n * p), n, p)  # Perturbed covariates for time 2 x = list(x1, x2)  # Define coefficients for time points 1 and 2 beta1 = c(rep(2, 10), rep(0, p - 10))  # Coefs at time 1 beta2 = runif(p, 0.5, 2) * beta1       # Coefs at time 2, shared support with time 1  # Response variables for times 1 and 2: y = cbind(   x[[1]] %*% beta1 + sigma * rnorm(n),   x[[2]] %*% beta2 + sigma * rnorm(n) )  # Split data into training and testing sets xtest = lapply(x, function(xx) xx[-(1:ntrain), ])  # Test covariates ytest = y[-(1:ntrain), ]  # Test response  x = lapply(x, function(xx) xx[1:ntrain, ])  # Train covariates y = y[1:ntrain, ]  # Train response str(x) #> List of 2 #>  $ : num [1:300, 1:100] -1.207 0.277 1.084 -2.346 0.429 ... #>  $ : num [1:300, 1:100] -1.493 0.303 1.172 -2.316 0.224 ... fit = ptLasso(x, y, use.case = \"timeSeries\", alpha = 0) plot(fit)  # Plot the fitted model predict(fit, xtest, ytest = ytest)  # Predict using the fitted model #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, ytest = ytest)  #>  #>  #>  #> alpha =  0  #>  #> Performance (Mean squared error): #>  #>             mean response_1 response_2 #> Pretrain   11.92       12.1      11.75 #> Individual 11.46       12.1      10.82 #>  #> Support size: #>                                           #> Pretrain   36 (16 common + 20 individual) #> Individual 61  # With cross validation: cvfit = cv.ptLasso(x, y, use.case = \"timeSeries\") plot(cvfit, plot.alphahat = TRUE)  # Plot cross-validated model predict(cvfit, xtest, ytest = ytest)  # Predict using cross-validated model #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, ytest = ytest)  #>  #>  #>  #> alpha =  0.4  #>  #> Performance (Mean squared error): #>  #>             mean response_1 response_2 #> Pretrain   15.73      12.11      19.35 #> Individual 11.53      12.11      10.96 #>  #> Support size: #>                                           #> Pretrain   54 (19 common + 35 individual) #> Individual 65"},{"path":[]},{"path":"https://erincr.github.io/ptLasso/articles/ptLasso.html","id":"multi-response-data-with-mixed-response-types","dir":"Articles","previous_headings":"More examples of pretraining using glmnet","what":"Multi-response data with mixed response types","title":"ptLasso Vignette and Manual","text":"Muti-response data consists datasets covariates XX multiple outcomes y1,y2,y3,…y_1, y_2, y_3, \\dots. outcomes continuous, may natural treat multitask learning problem (see section “Multi-response data Gaussian responses”). outcomes mixed types however – e.g. y1y_1 continuous, y2y_2 binary y3y_3 survival – problem slightly challenging, fewer methods developed setting. Pretraining natural fit task: often believe shared information y1y_1, y2y_2 y3y_3. fit 3 separate models, never get take advantage shared information; , outcomes different types, methods fit one model outcomes (“overall model”). , use pretraining pass information models. : fit model y1y_1, extract offset support model, use offset support (usual pretraining) train models y2y_2 y3y_3. one small detail : must choose primary outcome y1y_1. important choice form support offset two outcomes. recommend making selection using domain knowledge, cross-validation (validation set) can course used. , walk example simulated data three outcomes y1,y2y_1, y_2 y3y_3. three outcomes overlapping support; first 10 features predictive . Outcomes 2 3 additionally 5 features unique . ’ll define y1y_1 continuous, y2y_2 binomial y3y_3 survival. first step pretraining, train model primary outcome (y1y_1) record offset support – used training models y2y_2 y3y_3. Now everything need train models y2y_2 y3y_3. following code, loop α=0,0.1,…,1\\alpha = 0, 0.1, \\dots, 1; step, (1) train models y2y_2 y3y_3 (2) record CV error models. CV error used determine values α\\alpha use final models. Plotting CV performance suggests value α\\alpha choose outcome:  Now selected values α\\alpha, can fit final models y2y_2 y3y_3: also train models y2y_2 y3y_3without pretraining; natural benchmark. models trained. Let’s compare performance without pretraining; ’ll start model y2y_2. now, models y3y_3: y2y_2 y3y_3, saw performance improvement using pretraining. didn’t technically need train individual (non-pretrained) models y2y_2 y3y_3: CV loop choose α\\alpha, saw cross validation performance individual models (special case α=1\\alpha = 1), CV recommended smaller value α\\alpha outcomes. Note , example, trained model using y1y_1, used model form offset support models y2y_2 y3y_3 parallel. using pretraining multi-response data flexible. Pretraining simply method pass information one model another, free choose information flows. example, chose pass information model 1 (y1y_1) model 2 (y2y_2) model 3 (y3y_3). , instead chained models pass information model 1 model 2, model 2 model 3 following way: fit model y1y_1, extract offset support model, use offset support (usual pretraining) train model y2y_2, extract offset support second model, use train model y3y_3. framework, model y3y_3 depends implicitly models y1y_1and y2y_2, offset support model y2y_2 informed model y1y_1. Choosing information passed outcomes context specific recommend relying domain knowledge selecting approach (though many options may tried compared cross-validation validation set).","code":"require(glmnet) require(survival) set.seed(1234)  # Define constants n = 600          # Total number of samples ntrain = 300     # Number of training samples p = 50           # Number of features  # Define covariates      x = matrix(rnorm(n*p), n, p)  # y1: continuous response beta1 = c(rep(.5, 10), rep(0, p-10)) y1 = x %*% beta1 + rnorm(n)  # y2: binomial response beta2 = runif(p, min = 0.5, max = 1) * beta1  # Shared with group 1 beta2 = beta2 + c(rep(0, 10),                    runif(5, min = 0, max = 0.5),                    rep(0, p-15)) # Individual y2 = rbinom(n, 1, prob = 1/(1 + exp(-x %*% beta2)))  # y3: survival response beta3 = beta1  # Shared with group 1 beta3 = beta3 + c(rep(0, 10),                    runif(5, min = -0.1, max = 0.1),                   rep(0, p-15)) # Individual y3.true = - log(runif(n)) / exp(x %*% beta3) y3.cens = runif(n) y3 = Surv(pmin(y3.true, y3.cens), y3.true <= y3.cens)  # Split into train and test xtest = x[-(1:ntrain), ] y1test = y1[-(1:ntrain)] y2test = y2[-(1:ntrain)] y3test = y3[-(1:ntrain), ]  x = x[1:ntrain, ] y1 = y1[1:ntrain] y2 = y2[1:ntrain] y3 = y3[1:ntrain, ]  # Define training folds nfolds = 10 foldid = sample(rep(1:10, trunc(nrow(x)/nfolds)+1))[1:nrow(x)] y1_fit = cv.glmnet(x, y1, keep=TRUE, foldid = foldid)  train_offset = y1_fit$fit.preval[, y1_fit$lambda == y1_fit$lambda.1se] support = which(coef(y1_fit, s = y1_fit$lambda.1se)[-1] != 0) cv.error.y2 = cv.error.y3 = NULL alphalist = seq(0, 1, length.out = 11)  for(alpha in alphalist){   pf = rep(1/alpha, p)   pf[support] = 1      offset = (1 - alpha) * train_offset        y2_fit = cv.glmnet(x, y2,                       foldid = foldid,                      offset = offset,                      penalty.factor = pf,                      family = \"binomial\",                      type.measure = \"auc\")   cv.error.y2 = c(cv.error.y2, max(y2_fit$cvm))      y3_fit = cv.glmnet(x, y3,                       foldid = foldid,                      offset = offset,                      penalty.factor = pf,                      family = \"cox\",                      type.measure = \"C\")   cv.error.y3 = c(cv.error.y3, max(y3_fit$cvm)) } par(mfrow = c(2, 1)) plot(alphalist, cv.error.y2, type = \"b\",      main = bquote(\"Outcome 2: CV AUC vs \" ~ alpha),      xlab = expression(alpha),      ylab = \"CV AUC\") abline(v = alphalist[which.max(cv.error.y2)])  plot(alphalist, cv.error.y3, type = \"b\",      main = bquote(\"Outcome 3: CV C index vs \" ~ alpha),      xlab = expression(alpha),      ylab = \"CV C index\") abline(v = alphalist[which.max(cv.error.y3)]) ############################################################ # Model for y2: ############################################################ best.alpha.y2 = alphalist[which.max(cv.error.y2)] pf = rep(1/best.alpha.y2, p); pf[support] = 1  y2_fit = cv.glmnet(x, y2,                     foldid = foldid,                    offset = (1-best.alpha.y2) * train_offset,                    penalty.factor = pf,                    family = \"binomial\",                    type.measure = \"auc\")  ############################################################ # Repeat for y3: ############################################################ best.alpha.y3 = alphalist[which.max(cv.error.y3)] pf = rep(1/best.alpha.y3, p); pf[support] = 1  y3_fit = cv.glmnet(x, y3,                     foldid = foldid,                    offset = (1-best.alpha.y3) * train_offset,                    penalty.factor = pf,                    family = \"cox\",                     type.measure = \"C\") y2_fit_no_pretrain = cv.glmnet(x, y2, foldid = foldid,                                 family = \"binomial\", type.measure = \"auc\")  y3_fit_no_pretrain = cv.glmnet(x, y3,                                 foldid = foldid,                                 family = \"cox\", type.measure = \"C\") testoffset = predict(y1_fit, xtest, s = \"lambda.1se\")  cat(\"Model 2 AUC with pretraining:\",      round(assess.glmnet(y2_fit, xtest, newy = y2test,                          newoffset = (1 - best.alpha.y2) * testoffset)$auc, 2),     fill=TRUE) #> Model 2 AUC with pretraining: 0.76  cat(\"Model 2 AUC without pretraining:\",      round(assess.glmnet(y2_fit_no_pretrain, xtest, newy = y2test)$auc, 2)     ) #> Model 2 AUC without pretraining: 0.66 cat(\"Model 3 C-index with pretraining:\",      round(assess.glmnet(y3_fit, xtest, newy = y3test,                          newoffset = (1 - best.alpha.y3) * testoffset)$C, 2)) #> Model 3 C-index with pretraining: 0.8  cat(\"Model 3 C-index without pretraining:\",      round(assess.glmnet(y3_fit_no_pretrain, xtest, newy = y3test)$C, 2)     ) #> Model 3 C-index without pretraining: 0.78"},{"path":"https://erincr.github.io/ptLasso/articles/ptLasso.html","id":"conditional-average-treatment-effect-estimation","dir":"Articles","previous_headings":"More examples of pretraining using glmnet","what":"Conditional average treatment effect estimation","title":"ptLasso Vignette and Manual","text":"","code":"require(glmnet)"},{"path":"https://erincr.github.io/ptLasso/articles/ptLasso.html","id":"background-cate-estimation-and-pretraining","dir":"Articles","previous_headings":"More examples of pretraining using glmnet > Conditional average treatment effect estimation","what":"Background: CATE estimation and pretraining","title":"ptLasso Vignette and Manual","text":"causal inference, often interested predicting treatment effect individual observations; called conditional average treatment effect (CATE). example, prescribing drug patient, want know whether drug likely work well patient - just whether works well average. One tool model CATE R-learner (Nie Wager (2021)), minimizes R loss: $$ \\hat{L}_n\\{\\tau(\\cdot)\\}=\\arg \\min_\\tau \\frac{1}{n}\\sum\\Bigl[ (y_i- m^*(x_i)) - (W_i-e^*(x_i))\\tau(x_i) \\Bigr]^2. $$ , xix_i yiy_i covariates outcome observation ii, e*(xi)e^*(x_i) treatment propensity WiW_i treatment assignment, m*(xi)m^*(x_i) conditional mean outcome (E[yi∣x=xi]E[y_i \\mid x = x_i]). , τ̂\\hat\\tau estimate heterogeneous treatment effect function. fitted stages: first, R-learner fits m*m^* e*e^* get m̂*\\hat{m}^* ê*\\hat{e}^*; plugs m̂*(xi)\\hat{m}^*(x_i) ê*(xi)\\hat{e}^*(x_i) fit τ\\tau. minor detail cross-fitting (prevalidation) used first stage plugin value e.g. m̂*(xi)\\hat{m}^*(x_i) comes model trained without using xix_i. τ\\tau linear function, second stage fitting straightforward. values m̂*(xi)\\hat{m}^*(x_i) ê*(xi)\\hat{e}^*(x_i) known, can use linear regression model yi−m̂*(xi)y_i - \\hat{m}^*(x_i) function weighted feature vector (Wi−ê*(xi))xi(W_i-\\hat{e}^*(x_i)) x_i. following example. can pretraining useful ? Well, separately fitting models m*m^* (conditional mean) τ\\tau (heterogeneous treatment effect), two functions likely share support: sensible assume features modulate mean treatment effect also modulate heterogeneous treatment effect. can use pretraining (1) training model m*m^* (2) using support model guide fitting τ\\tau. Note offset used case; m*m^* τ\\tau designed predict different outcomes.","code":""},{"path":"https://erincr.github.io/ptLasso/articles/ptLasso.html","id":"a-simulated-example","dir":"Articles","previous_headings":"More examples of pretraining using glmnet > Conditional average treatment effect estimation","what":"A simulated example","title":"ptLasso Vignette and Manual","text":"example. simplify problem assuming treatment randomized – true e*(xi)=0.5e^*(x_i) = 0.5 ii. begin model fitting, starting estimate e*e^* (probability receiving treatment). fit τ\\tau, also need record cross-fitted ê*(x)\\hat{e}^*(x). Now, stage 1 pretraining: fit model m*m^* record support. , also record cross-fitted m̂*(x)\\hat{m}^*(x). fit τ\\tau, regress ỹ=yi−m̂*(xi)\\tilde{y} = y_i - \\hat{m}^*(x_i) x̃=(wi−ê*(xi))xi\\tilde{x} = (w_i - \\hat{e}^*(x_i)) x_i; ’ll define : now, pretraining τ\\tau. Loop α=0,0.1,…,1\\alpha = 0, 0.1, \\dots, 1; α\\alpha, fit model τ\\tau using penalty factor defined support m̂\\hat{m} α\\alpha. ’ll keep track CV MSE step can choose α\\alpha minimizes MSE.  plot , value α=1\\alpha = 1 corresponds usual R learner, makes assumption shared support τ\\tau m*m^*. Based plot, choose α=0.2\\alpha = 0.2 best performing model: concretely compare pretrained R-learner usual R-learner, ’ll train usual R-learner : anticipated, pretraining improves prediction squared error relative R learner – designed simulation:","code":"set.seed(1234)  n = 600; ntrain = 300 p = 20       x = matrix(rnorm(n*p), n, p)  # Treatment assignment w = rbinom(n, 1, 0.5)  # m^* m.coefs = c(rep(2,10), rep(0, p-10)) m = x %*% m.coefs  # tau tau.coefs = runif(p, 0.5, 1)*m.coefs  tau = 1.5*m + x%*%tau.coefs  mu = m + w * tau y  = mu + 10 * rnorm(n) cat(\"Signal to noise ratio:\", var(mu)/var(y-mu)) #> Signal to noise ratio: 2.301315  # Split into train/test xtest = x[-(1:ntrain), ] tautest = tau[-(1:ntrain)]  wtest = w[-(1:ntrain)]  x = x[1:ntrain, ] y = y[1:ntrain]  w = w[1:ntrain]  # Define training folds nfolds = 10 foldid = sample(rep(1:10, trunc(nrow(x)/nfolds)+1))[1:nrow(x)] e_fit = cv.glmnet(x, w, foldid = foldid,                   family=\"binomial\", type.measure=\"deviance\",                   keep = TRUE)  e_hat = e_fit$fit.preval[, e_fit$lambda == e_fit$lambda.1se] e_hat = 1/(1 + exp(-e_hat)) m_fit = cv.glmnet(x, y, foldid = foldid, keep = TRUE)  m_hat = m_fit$fit.preval[, m_fit$lambda == m_fit$lambda.1se]  bhat = coef(m_fit, s = m_fit$lambda.1se) support = which(bhat[-1] != 0) y_tilde = y - m_hat x_tilde = cbind(as.numeric(w - e_hat) * cbind(1, x)) cv.error = NULL alphalist = seq(0, 1, length.out = 11)  for(alpha in alphalist){   pf = rep(1/alpha, p)   pf[support] = 1   pf = c(0, pf) # Don't penalize the intercept      tau_fit = cv.glmnet(x_tilde, y_tilde,                        foldid = foldid,                       penalty.factor = pf,                       intercept = FALSE, # already include in x_tilde                       standardize = FALSE)   cv.error = c(cv.error, min(tau_fit$cvm)) }   plot(alphalist, cv.error, type = \"b\",      xlab = expression(alpha),       ylab = \"CV MSE\",       main = bquote(\"CV mean squared error as a function of \" ~ alpha)) abline(v = alphalist[which.min(cv.error)]) best.alpha = alphalist[which.min(cv.error)] cat(\"Chosen alpha:\", best.alpha) #> Chosen alpha: 0.2  pf = rep(1/best.alpha, p) pf[support] = 1 pf = c(0, pf) tau_fit = cv.glmnet(x_tilde, y_tilde, foldid = foldid,                     penalty.factor = pf,                     intercept = FALSE,                     standardize = FALSE) tau_rlearner = cv.glmnet(x_tilde, y_tilde, foldid = foldid,                           penalty.factor = c(0, rep(1, ncol(x))),                          intercept = FALSE,                          standardize = FALSE) rlearner_preds   = predict(tau_rlearner, cbind(1, xtest), s = \"lambda.min\") cat(\"R-learner PSE: \",      round(mean((rlearner_preds - tautest)^2), 2)) #> R-learner PSE:  45.85  pretrained_preds = predict(tau_fit, cbind(1, xtest), s = \"lambda.min\") cat(\"Pretrained R-learner PSE: \",      round(mean((pretrained_preds - tautest)^2), 2)) #> Pretrained R-learner PSE:  37.63"},{"path":"https://erincr.github.io/ptLasso/articles/ptLasso.html","id":"what-if-the-pretraining-assumption-is-wrong","dir":"Articles","previous_headings":"More examples of pretraining using glmnet > Conditional average treatment effect estimation","what":"What if the pretraining assumption is wrong?","title":"ptLasso Vignette and Manual","text":", repeat everything , now overlap support m*m^* τ\\tau. Pretraining hurt performance, even though support m*m^* τ\\tau shared. ? Recall defined y=m*(x)+W*τ(x)+ϵy =  m^*(x) + W * \\tau(x) + \\epsilon, relationship yy xx function supports m*m^* τ\\tau. first stage pretraining, fitted m*m^* using y ~ x – support m*m^*include support τ\\tau. result, using pretraining R-learner harm predictive performance.","code":"###################################################### # Simulate data ###################################################### x = matrix(rnorm(n*p), n, p)  # Treatment assignment w = rbinom(n, 1, 0.5)  # m^* m.coefs = c(rep(2,10), rep(0, p-10)) m = x %*% m.coefs  # tau # Note these coefficients have no overlap with m.coefs! tau.coefs = c(rep(0, 10), rep(2, 10), rep(0, p-20)) tau = x%*%tau.coefs  mu = m + w * tau y  = mu + 10 * rnorm(n) cat(\"Signal to noise ratio:\", var(mu)/var(y-mu)) #> Signal to noise ratio: 0.6938152  # Split into train/test xtest = x[-(1:ntrain), ] tautest = tau[-(1:ntrain)]  wtest = w[-(1:ntrain)]  x = x[1:ntrain, ] y = y[1:ntrain]  w = w[1:ntrain]  ###################################################### # Model fitting: e^* ###################################################### e_fit = cv.glmnet(x, w, foldid = foldid,                   family=\"binomial\", type.measure=\"deviance\",                   keep = TRUE) e_hat = e_fit$fit.preval[, e_fit$lambda == e_fit$lambda.1se] e_hat = 1/(1 + exp(-e_hat))  ###################################################### # Model fitting: m^* ###################################################### m_fit = cv.glmnet(x, y, foldid = foldid, keep = TRUE)  m_hat = m_fit$fit.preval[, m_fit$lambda == m_fit$lambda.1se]  bhat = coef(m_fit, s = m_fit$lambda.1se) support = which(bhat[-1] != 0)  ###################################################### # Pretraining: tau ###################################################### y_tilde = y - m_hat x_tilde = cbind(as.numeric(w - e_hat) * cbind(1, x))  cv.error = NULL alphalist = seq(0, 1, length.out = 11)  for(alpha in alphalist){   pf = rep(1/alpha, p)   pf[support] = 1   pf = c(0, pf) # Don't penalize the intercept      tau_fit = cv.glmnet(x_tilde, y_tilde,                        foldid = foldid,                       penalty.factor = pf,                       intercept = FALSE, # already include in x_tilde                       standardize = FALSE)   cv.error = c(cv.error, min(tau_fit$cvm)) }  # Our final model for tau: best.alpha = alphalist[which.min(cv.error)] cat(\"Chosen alpha:\", best.alpha) #> Chosen alpha: 1  pf = rep(1/best.alpha, p) pf[support] = 1 pf = c(0, pf) tau_fit = cv.glmnet(x_tilde, y_tilde, foldid = foldid,                     penalty.factor = pf,                     intercept = FALSE,                     standardize = FALSE)  ###################################################### # Fit the usual R-learner: ###################################################### tau_rlearner = cv.glmnet(x_tilde, y_tilde, foldid = foldid,                           penalty.factor = c(0, rep(1, ncol(x))),                          intercept = FALSE,                          standardize = FALSE)  ###################################################### # Measure performance: ###################################################### rlearner_preds = predict(tau_rlearner, cbind(1, xtest), s = \"lambda.min\") cat(\"R-learner prediction squared error: \",      round(mean((rlearner_preds - tautest)^2), 2)) #> R-learner prediction squared error:  31.11  pretrained_preds = predict(tau_fit, cbind(1, xtest), s = \"lambda.min\") cat(\"Pretrained R-learner prediction squared error: \",      round(mean((pretrained_preds - tautest)^2), 2)) #> Pretrained R-learner prediction squared error:  31.11"},{"path":"https://erincr.github.io/ptLasso/articles/ptLasso.html","id":"using-non-linear-bases","dir":"Articles","previous_headings":"More examples of pretraining using glmnet","what":"Using non-linear bases","title":"ptLasso Vignette and Manual","text":"Suppose dataset features XX response yy, relationship XX yy nonlinear function columns XX. Can still use lasso? Yes! can pretrain linear model using xgboost obtain basis functions (features). Let’s walk example.","code":"require(glmnet) require(ptLasso)"},{"path":"https://erincr.github.io/ptLasso/articles/ptLasso.html","id":"example-1-xgboost-pretraining","dir":"Articles","previous_headings":"More examples of pretraining using glmnet > Using non-linear bases","what":"Example 1: xgboost pretraining","title":"ptLasso Vignette and Manual","text":"start simulating data (n=1800n = 1800, p=1000p = 1000) continuous response. coefficients β\\beta sparse; first 200 entries drawn standard univariate normal, remainder 00. define yy y=1(X>0)β+ϵy = 1(X > 0) \\beta + \\epsilon, ϵ\\epsilon noise; hope xgboost learn splits corresponding X>0X > 0. Now, run xgboost get basis functions: ready model fitting cv.glmnet. two baselines (1) linear model pretrain xgboost, (2) xgboost. find glmnet together xgboost outperforms glmnet alone xgboost alone.","code":"require(xgboost) #> Loading required package: xgboost set.seed(1234)  n = 1800; p = 1000; noise = 5;  x     = matrix(rnorm(n * p), nrow=n, ncol=p) xtest = matrix(rnorm(n * p), nrow=n, ncol=p)  x.model     = 1*(x > 0)      xtest.model = 1*(xtest > 0)   beta = c(rnorm(200), rep(0, p-200))  y     = x.model %*% beta + noise * rnorm(n) ytest = xtest.model %*% beta + noise * rnorm(n)  train.folds = sample(rep(1:10, n/10)) xgbfit      = xgboost(data=x, label=y, nrounds=200, max_depth=1, verbose=0)  x.boost     = predict(xgbfit, x, predleaf = TRUE) - 1 xtest.boost = predict(xgbfit, xtest, predleaf = TRUE) - 1 cvfit = cv.glmnet(x.boost, y, type.measure = \"mse\", foldid = train.folds) cvfit.noboost = cv.glmnet(x, y, type.measure = \"mse\", foldid = train.folds)  cat(\"Lasso with xgboost pretraining PSE: \",      assess.glmnet(cvfit, newx = xtest.boost, newy = ytest)$mse) #> Lasso with xgboost pretraining PSE:  46.23225  cat(\"Lasso without xgboost pretraining PSE: \",      assess.glmnet(cvfit.noboost, newx = xtest, newy = ytest)$mse) #> Lasso without xgboost pretraining PSE:  60.68818  cat(\"xgboost alone PSE: \",      assess.glmnet(predict(xgbfit, xtest), newy = ytest)$mse) #> xgboost alone PSE:  49.47738"},{"path":"https://erincr.github.io/ptLasso/articles/ptLasso.html","id":"example-2-xgboost-pretraining-with-input-groups","dir":"Articles","previous_headings":"More examples of pretraining using glmnet > Using non-linear bases","what":"Example 2: xgboost pretraining with input groups","title":"ptLasso Vignette and Manual","text":"Now, let’s repeat supposing data input groups. difference use cv.ptLasso model instead cv.glmnet, use group indicators feature fitting xgboost. start simulating data 3 groups (600600 observations group) continuous response. , simulate yy y=1(X>0)β+ϵy = 1(X > 0) \\beta + \\epsilon, now different β\\beta group. coefficients groups Table @ref(tab:nonlinear). Coefficients simulating data use xgboost pretraining dummy variables group indicators; use fit predict xgboost. Now, let’s train xgboost predict get new features. Note now use max_depth = 2: intended allow interactions group indicators features. Finally, ready fit two models trained cv.ptLasso: one uses xgboost features . , find pretraining xgboost improves performance relative (1) model fitting original feature space (2) xgboost alone.","code":"set.seed(1234)  n = 1800; p = 500; k = 3; noise = 5;  groups = groupstest = sort(rep(1:k, n/k))  x     = matrix(rnorm(n * p), nrow=n, ncol=p) xtest = matrix(rnorm(n * p), nrow=n, ncol=p)  x.model     = 1*(x > 0)      xtest.model = 1*(xtest > 0)  common.beta = c(rep(2, 50), rep(0, p-50)) beta.1 = c(rep(0, 50),  rep(1, 50), rep(0, p-100))  beta.2 = c(rep(0, 100), rep(1, 50), rep(0, p-150))  beta.3 = c(rep(0, 150), rep(1, 50), rep(0, p-200))   y = x.model %*% common.beta + noise * rnorm(n) y[groups == 1] = y[groups == 1] + x.model[groups == 1, ] %*% beta.1 y[groups == 2] = y[groups == 2] + x.model[groups == 2, ] %*% beta.2 y[groups == 3] = y[groups == 3] + x.model[groups == 3, ] %*% beta.3  ytest = xtest.model %*% common.beta + noise * rnorm(n) ytest[groups == 1] = ytest[groups == 1] + xtest.model[groups == 1, ] %*% beta.1 ytest[groups == 2] = ytest[groups == 2] + xtest.model[groups == 2, ] %*% beta.2 ytest[groups == 3] = ytest[groups == 3] + xtest.model[groups == 3, ] %*% beta.3 group.ids     = model.matrix(~as.factor(groups) - 1)  grouptest.ids = model.matrix(~as.factor(groupstest) - 1)  colnames(grouptest.ids) = colnames(group.ids) xgbfit      = xgboost(data=cbind(x, group.ids), label=y,                        nrounds=200, max_depth=2, verbose=0)  x.boost     = predict(xgbfit, cbind(x, group.ids), predleaf = TRUE) - 1 xtest.boost = predict(xgbfit, cbind(xtest, grouptest.ids), predleaf = TRUE) - 1 cvfit = cv.ptLasso(x.boost, y, groups=groups, type.measure = \"mse\") preds = predict(cvfit, xtest.boost, groups=groupstest, alphatype = \"varying\") preds = preds$yhatpre  cvfit.noboost = cv.ptLasso(x, y, groups=groups, type.measure = \"mse\") preds.noboost = predict(cvfit.noboost, xtest, groups=groupstest,                          alphatype = \"varying\") preds.noboost = preds.noboost$yhatpre  cat(\"ptLasso with xgboost pretraining PSE: \",      assess.glmnet(preds, newy = ytest)$mse) #> ptLasso with xgboost pretraining PSE:  55.1535  cat(\"ptLasso without xgboost pretraining PSE: \",      assess.glmnet(preds.noboost, newy = ytest)$mse) #> ptLasso without xgboost pretraining PSE:  66.37259  cat(\"xgboost alone PSE: \",      assess.glmnet(predict(xgbfit, xtest), newy = ytest)$mse) #> xgboost alone PSE:  59.63781"},{"path":"https://erincr.github.io/ptLasso/articles/ptLasso.html","id":"unsupervised-pretraining","dir":"Articles","previous_headings":"More examples of pretraining using glmnet","what":"Unsupervised pretraining","title":"ptLasso Vignette and Manual","text":"Suppose dataset features XX response yy. Suppose also large set unlabeled data X*X^*. , show pretrain model using X*X^*. steps : sparse PCA using X*X^*. Identify nonzero features first principal component (PC). Use glmnet (cv.glmnet) train model using XX yy. Define penalty factor using support identified sparse PCA. Unlike usual pretraining, offset defined sparse PCA. step 1, may choose use nonzero features first kk PCs instead just first PC; examples follow, use first PC simplicity. demonstrate unsupervised pretraining using simulated data. covariates XX X*X^* drawn multivariate normal distribution first 10 features describe variance, yy defined Xβ+ϵX \\beta + \\epsilon, first 10 coefficients β\\beta nonzero ϵ\\epsilon noise. example, 10 times much unlabeled data labeled data; generally happens labels difficult obtain. Now, sparse PCA using X*X^* identify features nonzero loadings first PC. argument k=1k = 1 means obtain first PC. set success: simulated data, know first 10 features explain variance XX. also features define relationship XX yy. Let’s check sparse PCA found right features: Now, ready model! don’t need call ptLasso . need call cv.glmnet across grid values α\\alpha different penalty.factor call. Note offset used – sparse PCA identifies features may important, doesn’t suggest value fitted coefficients. model selection, want know value α\\alpha gave us best CV error. Fortunately, cv.glmnet record CV MSE model vector called cvm; just need keep track minimum error model.  , using CV performance metric, choose α=0.2\\alpha = 0.2. Now, train final model predict measure performance held-data. find pretraining gives us boost performance.","code":"require(glmnet) require(MASS) # for mvrnorm #> Loading required package: MASS  set.seed(1234)  n = 100; p = 150;   mu = rep(0, p) sigma <- matrix(runif(p^2)*2-1, ncol=p)  sigma[, 11:p] = 1e-2 # The first 10 features are the most important sigma <- t(sigma) %*% sigma diag(sigma)[11:p] = 1  x     = mvrnorm(n = n, mu = mu, Sigma = sigma) xtest = mvrnorm(n = n, mu = mu, Sigma = sigma) xstar = mvrnorm(n = 10 * n, mu = mu, Sigma = sigma) # unlabeled  noise = 3 beta = c(rep(1, 10), rep(0, p - 10)) y     = x %*% beta     + noise * rnorm(n) ytest = xtest %*% beta + noise * rnorm(n)  train.folds = sample(rep(1:10, 10)) require(sparsepca) #> Loading required package: sparsepca  pcs = spca(xstar, k = 1, verbose=FALSE, alpha=1e-2, beta=1e-2) nonzero.loadings = which(pcs$loadings != 0) nonzero.loadings #>  [1]  1  2  3  4  5  6  7  8  9 10 alphalist = seq(0, 1, length.out = 11)  cvm = NULL for(alpha in alphalist){   # Define the penalty factor:   pf = rep(1/alpha, p)   pf[nonzero.loadings] = 1      # Train a model:   model = cv.glmnet(x, y, family = \"gaussian\", type.measure = \"mse\",                               penalty.factor = pf,                                foldid = train.folds)      # Record the minmum CV MSE for this model:   cvm = c(cvm, min(model$cvm)) }  best.alpha = alphalist[which.min(cvm)]  # Plot performance as a function of alpha # with a vertical line to show us the minimum mse: plot(alphalist, cvm,       xlab = expression(alpha),       ylab = \"Mean squared error (CV)\" ) abline(v = best.alpha) pf = rep(1/best.alpha, p) pf[nonzero.loadings] = 1  selected.model =  cv.glmnet(x, y, family = \"gaussian\", type.measure = \"mse\",                               penalty.factor = pf,                                foldid = train.folds)  # Prediction squared error with pretraining: assess.glmnet(selected.model, xtest, newy = ytest, s = \"lambda.min\")[\"mse\"] #> $mse #> lambda.min  #>   9.914999  #> attr(,\"measure\") #> [1] \"Mean-Squared Error\"  without.pretraining =  cv.glmnet(x, y, family = \"gaussian\", type.measure = \"mse\",                                  foldid = train.folds)  # Prediction squared error without pretraining: assess.glmnet(without.pretraining, xtest, newy = ytest, s = \"lambda.min\")[\"mse\"] #> $mse #> lambda.min  #>   14.42903  #> attr(,\"measure\") #> [1] \"Mean-Squared Error\""},{"path":[]},{"path":"https://erincr.github.io/ptLasso/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Erin Craig. Author, maintainer. Rob Tibshirani. Author.","code":""},{"path":"https://erincr.github.io/ptLasso/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Craig E, Tibshirani R (????). ptLasso: Pretrained Lasso. R package version 1.0, https://erincr.github.io/ptLasso/.","code":"@Manual{,   title = {ptLasso: Pretrained Lasso},   author = {Erin Craig and Rob Tibshirani},   note = {R package version 1.0},   url = {https://erincr.github.io/ptLasso/}, }"},{"path":"https://erincr.github.io/ptLasso/index.html","id":"pretraining-and-the-lasso","dir":"","previous_headings":"","what":"Pretrained Lasso","title":"Pretrained Lasso","text":"package fits pretrained generalized linear models (1) data grouped observations (2) data without grouped observations, multinomial responses. Details method may found Craig et al. (2024). model fitting package done cv.glmnet, syntax closely follows glmnet package (2010).","code":""},{"path":"https://erincr.github.io/ptLasso/index.html","id":"references","dir":"","previous_headings":"","what":"References","title":"Pretrained Lasso","text":"Craig, Erin, Mert Pilanci, Thomas Le Menestrel, Balasubramanian Narasimhan, Manuel Rivas, Roozbeh Dehghannasiri, Julia Salzman, Jonathan Taylor, Robert Tibshirani. “Pretraining Lasso.” arXiv preprint arXiv:2401.12911 (2024). Friedman, Jerome, Trevor Hastie, Robert Tibshirani. 2010. “Regularization Paths Generalized Linear Models via Coordinate Descent.” Journal Statistical Software, Articles 33 (1): 1–22. https://doi.org/10.18637/jss.v033.i01.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/binomial.example.data.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulate input grouped data (binomial outcome) for testing with ptLasso. — binomial.example.data","title":"Simulate input grouped data (binomial outcome) for testing with ptLasso. — binomial.example.data","text":"required arguments; used primarily documentation. Simply calls makedata reasonable set features.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/binomial.example.data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulate input grouped data (binomial outcome) for testing with ptLasso. — binomial.example.data","text":"","code":"binomial.example.data(   k = 3,   class.sizes = rep(100, k),   n = sum(class.sizes),   scommon = 5,   sindiv = rep(5, k),   p = 2 * (sum(sindiv) + scommon),   beta.common = list(c(-0.5, 0.5, 0.3, -0.9, 0.1), c(-0.3, 0.9, 0.1, -0.1, 0.2), c(0.1,     0.2, -0.1, 0.2, 0.3)),   beta.indiv = lapply(1:k, function(i) 0.9 * beta.common[[i]]),   intercepts = rep(0, k),   sigma = NULL )"},{"path":"https://erincr.github.io/ptLasso/reference/binomial.example.data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulate input grouped data (binomial outcome) for testing with ptLasso. — binomial.example.data","text":"k Default: 3. class.sizes Default: rep(100, k). n Default: sum(class.sizes). scommon Default: 5. sindiv Default: rep(5, k). p Default: 2*(sum(sindiv) + scommon). beta.common Default: list(c(-.5, .5, .3, -.9, .1), c(-.3, .9, .1, -.1, .2), c(0.1, .2, -.1, .2, .3)). beta.indiv Default: lapply(1:k, function()  0.9 * beta.common[[]]). intercepts Default: rep(0,k). sigma Default: NULL.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/binomial.example.data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simulate input grouped data (binomial outcome) for testing with ptLasso. — binomial.example.data","text":"list data 5 groups binomial outcome, n=300 p=40: x Simulated features, size n x p. y Outcomes y, length n. groups Vector length n, indicating observations belong group. snr Gaussian outcome : signal noise ratio. mu Gaussian outcome : value y noise added.","code":""},{"path":[]},{"path":"https://erincr.github.io/ptLasso/reference/binomial.example.data.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Simulate input grouped data (binomial outcome) for testing with ptLasso. — binomial.example.data","text":"Erin Craig Rob Tibshirani Maintainer: Erin Craig <erincr@stanford.edu>","code":""},{"path":"https://erincr.github.io/ptLasso/reference/binomial.example.data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulate input grouped data (binomial outcome) for testing with ptLasso. — binomial.example.data","text":"","code":"out = binomial.example.data() x = out$x; y=out$y; groups = out$group"},{"path":"https://erincr.github.io/ptLasso/reference/coef.cv.ptLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the coefficients from a fitted cv.ptLasso model. — coef.cv.ptLasso","title":"Get the coefficients from a fitted cv.ptLasso model. — coef.cv.ptLasso","text":"Get coefficients fitted cv.ptLasso model.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/coef.cv.ptLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the coefficients from a fitted cv.ptLasso model. — coef.cv.ptLasso","text":"","code":"# S3 method for class 'cv.ptLasso' coef(   object,   model = c(\"all\", \"individual\", \"overall\", \"pretrain\"),   alpha = NULL,   ... )"},{"path":"https://erincr.github.io/ptLasso/reference/coef.cv.ptLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the coefficients from a fitted cv.ptLasso model. — coef.cv.ptLasso","text":"object fitted \"cv.ptLasso\" object. model string indicating coefficients retrieve. Must one \"\", \"individual\", \"overall\" \"pretrain\". alpha value 0 1, indicating alpha use. NULL, return coefficients models.  impacts results model = \"\" model = \"pretrain\". ... arguments passed \"coef\" function. May e.g. s = \"lambda.min\".","code":""},{"path":[]},{"path":"https://erincr.github.io/ptLasso/reference/coef.cv.ptLasso.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Get the coefficients from a fitted cv.ptLasso model. — coef.cv.ptLasso","text":"Erin Craig Rob Tibshirani Maintainer: Erin Craig <erincr@stanford.edu>","code":""},{"path":"https://erincr.github.io/ptLasso/reference/coef.cv.ptLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get the coefficients from a fitted cv.ptLasso model. — coef.cv.ptLasso","text":"","code":"set.seed(1234) out = gaussian.example.data() x = out$x; y=out$y; groups = out$group;  cvfit = cv.ptLasso(x, y, groups = groups, family = \"gaussian\", type.measure = \"mse\") # Get all model coefficients. names(coef(cvfit)) #> [1] \"individual\" \"pretrain\"   \"overall\"     coef(cvfit, model = \"overall\") # Overall model only #> 125 x 1 sparse Matrix of class \"dgCMatrix\" #>                       s1 #> (Intercept) -1.293077574 #> groups2     -0.802576770 #> groups3      4.688453083 #> groups4     -1.067174861 #> groups5     -0.173873523 #>              6.534854270 #>              6.813808490 #>              5.141281910 #>              7.160160979 #>              6.284458960 #>              6.739156736 #>              7.910505452 #>              6.294609076 #>              5.231380388 #>              7.437161907 #>              0.316816148 #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              0.119524657 #>              .           #>              0.002353154 #>              0.257401597 #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              0.522864277 #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>             -0.040566927 #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           #>              .           length(coef(cvfit, model = \"individual\")) # List of coefficients for each group model #> [1] 5 length(coef(cvfit, model = \"pretrain\", alpha = .5)) # List of coefficients for each group model #> [1] 5"},{"path":"https://erincr.github.io/ptLasso/reference/coef.ptLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the coefficients from a fitted ptLasso model. — coef.ptLasso","title":"Get the coefficients from a fitted ptLasso model. — coef.ptLasso","text":"Get coefficients fitted ptLasso model.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/coef.ptLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the coefficients from a fitted ptLasso model. — coef.ptLasso","text":"","code":"# S3 method for class 'ptLasso' coef(object, model = c(\"all\", \"individual\", \"overall\", \"pretrain\"), ...)"},{"path":"https://erincr.github.io/ptLasso/reference/coef.ptLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the coefficients from a fitted ptLasso model. — coef.ptLasso","text":"object fitted \"ptLasso\" object. model string indicating coefficients retrieve. Must one \"\", \"individual\", \"overall\" \"pretrain\". ... arguments passed \"coef\" function. May e.g. s = \"lambda.min\".","code":""},{"path":"https://erincr.github.io/ptLasso/reference/coef.ptLasso.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the coefficients from a fitted ptLasso model. — coef.ptLasso","text":"Model coefficients. model = \"overall\", function returns output coef. model \"individual\" \"pretrain\", function returns list containing results coef group-specific model. model = \"\", returns list containing (overall, individual pretrain) coefficients.","code":""},{"path":[]},{"path":"https://erincr.github.io/ptLasso/reference/coef.ptLasso.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Get the coefficients from a fitted ptLasso model. — coef.ptLasso","text":"Erin Craig Rob Tibshirani Maintainer: Erin Craig <erincr@stanford.edu>","code":""},{"path":"https://erincr.github.io/ptLasso/reference/coef.ptLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get the coefficients from a fitted ptLasso model. — coef.ptLasso","text":"","code":"# Train data out = gaussian.example.data() x = out$x; y=out$y; groups = out$group;  fit = ptLasso(x, y, groups = groups, family = \"gaussian\", type.measure = \"mse\") # Get all model coefficients. names(coef(fit)) #> [1] \"individual\" \"pretrain\"   \"overall\"     coef(fit, model = \"overall\") # Overall model only #> 125 x 1 sparse Matrix of class \"dgCMatrix\" #>                      s1 #> (Intercept)  1.79300815 #> groups2     -4.77841749 #> groups3     -2.20745229 #> groups4     -1.29396505 #> groups5     -2.80081665 #>              7.03787622 #>              5.95742513 #>              6.93417474 #>              7.71212703 #>              5.83781201 #>              6.19147485 #>              6.35333791 #>              8.39876277 #>              6.86451762 #>              6.59835750 #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              0.09713763 #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              0.89219086 #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          #>              .          length(coef(fit, model = \"individual\")) # List of coefficients for each group model #> [1] 5 length(coef(fit, model = \"pretrain\")) # List of coefficients for each group model #> [1] 5"},{"path":"https://erincr.github.io/ptLasso/reference/cv.ptLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"Cross-validation for ptLasso — cv.ptLasso","title":"Cross-validation for ptLasso — cv.ptLasso","text":"Cross-validation ptLasso.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/cv.ptLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cross-validation for ptLasso — cv.ptLasso","text":"","code":"cv.ptLasso(   x,   y,   groups = NULL,   alphalist = seq(0, 1, length = 11),   family = c(\"default\", \"gaussian\", \"multinomial\", \"binomial\", \"cox\"),   use.case = c(\"inputGroups\", \"targetGroups\", \"multiresponse\", \"timeSeries\"),   type.measure = c(\"default\", \"mse\", \"mae\", \"auc\", \"deviance\", \"class\", \"C\"),   nfolds = 10,   foldid = NULL,   verbose = FALSE,   fitoverall = NULL,   fitind = NULL,   s = \"lambda.min\",   gamma = \"gamma.min\",   alphahat.choice = \"overall\",   group.intercepts = TRUE,   ... )"},{"path":"https://erincr.github.io/ptLasso/reference/cv.ptLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cross-validation for ptLasso — cv.ptLasso","text":"x x matrix ptLasso. y y vector matrix ptLasso. groups vector length nobs indicating group observation belongs. data k groups, groups coded integers 1 k. 'use.case = \"inputGroups\"'. alphalist vector values pretraining hyperparameter alpha. Defaults seq(0, 1, length.=11). function pretraining choice alpha alphalist return CV performance alpha. family Response type ptLasso. use.case type grouping observed data. Can one \"inputGroups\", \"targetGroups\", \"multiresponse\" \"timeSeries\". type.measure Measure computed cv.glmnet, ptLasso. nfolds Number folds CV (default 10). Although nfoldscan large sample size (leave-one-CV), recommended large datasets. Smallest value allowable nfolds = 3. foldid optional vector values 1 nfolds identifying fold observation . supplied, nfolds can missing. verbose verbose=1, print statement showing model currently fit. fitoverall optional cv.glmnet object specifying overall model. trained full training data, argument keep = TRUE. fitind optional list cv.glmnet objects specifying individual models. trained training data, argumnet keep = TRUE. s choice lambda used models estimating CV performance choice alpha. Defaults \"lambda.min\". May \"lambda.1se\", numeric value. (Use caution supplying numeric value: lambda used models.) gamma use relax = TRUE. choice gamma used models estimating CV performance choice alpha. Defaults \"gamma.min\". May also \"gamma.1se\". alphahat.choice choosing alphahat, may prefer best performance using data (alphahat.choice = \"overall\") best average performance across groups (alphahat.choice = \"mean\"). particularly useful type.measure \"auc\" \"C\", average performance across groups different performance full dataset. default \"overall\". group.intercepts 'use.case = \"inputGroups\"' . `TRUE`, fit overall model separate intercept group. `FALSE`, ignore grouping fit one overall intercept. Default `TRUE`. ... Additional arguments passed `cv.glmnet` function. Notable choices include \"trace.\" \"parallel\". trace.= TRUE, progress bar displayed call cv.glmnet; useful big models take long time fit. parallel = TRUE, use parallel foreach fit fold.  Must register parallel hand, doMC others. Importantly, \"cv.ptLasso\" support arguments \"intercept\", \"offset\", \"fit\" \"check.args\".","code":""},{"path":"https://erincr.github.io/ptLasso/reference/cv.ptLasso.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cross-validation for ptLasso — cv.ptLasso","text":"object class \"cv.ptLasso\", list ingredients cross-validation fit. call call produced object. alphahat Value alpha optimizes CV performance data. varying.alphahat Vector values alpha, kth optimizes performance group k. alphalist Vector alphas compared. errall CV performance overall model. errpre CV performance pretrained models (one alpha tried). errind CV performance individual model. fit List ptLasso objects, one alpha tried. fitoverall fitted overall model used first stage pretraining. fitoverall.lambda value lambda used first stage pretraining. fitind list containing one individual model group. use.case use case: \"inputGroups\" \"targetGroups\". family family used. type.measure type.measure used.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/cv.ptLasso.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Cross-validation for ptLasso — cv.ptLasso","text":"function runs ptLasso requested choice alpha, returns cross validated performance.","code":""},{"path":[]},{"path":"https://erincr.github.io/ptLasso/reference/cv.ptLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cross-validation for ptLasso — cv.ptLasso","text":"","code":"# Getting started. First, we simulate data: we need covariates x, response y and group IDs. set.seed(1234) x = matrix(rnorm(1000*20), 1000, 20) y = rnorm(1000) groups = sort(rep(1:5, 200))  xtest = matrix(rnorm(1000*20), 1000, 20) ytest = rnorm(1000) groupstest = sort(rep(1:5, 200))  # Model fitting cvfit = cv.ptLasso(x, y, groups = groups, family = \"gaussian\", type.measure = \"mse\") cvfit #>  #> Call:   #> cv.ptLasso(x = x, y = y, groups = groups, family = \"gaussian\",   #>     type.measure = \"mse\", use.case = \"inputGroups\", group.intercepts = TRUE)  #>  #>  #>  #> type.measure:  mse  #>  #>  #>            alpha overall   mean wtdMean group_1 group_2 group_3 group_4 group_5 #> Overall           0.9739 0.9739  0.9739  0.8485  0.9735   1.149  0.9145  0.9838 #> Pretrain     0.0  0.9954 0.9954  0.9954  0.8655  0.9953   1.175  0.9458  0.9954 #> Pretrain     0.1  0.9850 0.9850  0.9850  0.8669  0.9724   1.161  0.9347  0.9904 #> Pretrain     0.2  0.9884 0.9884  0.9884  0.8634  0.9695   1.186  0.9330  0.9897 #> Pretrain     0.3  0.9838 0.9838  0.9838  0.8605  0.9724   1.158  0.9422  0.9863 #> Pretrain     0.4  0.9810 0.9810  0.9810  0.8647  0.9615   1.163  0.9306  0.9853 #> Pretrain     0.5  0.9841 0.9841  0.9841  0.8662  0.9766   1.158  0.9261  0.9934 #> Pretrain     0.6  0.9747 0.9747  0.9747  0.8590  0.9571   1.157  0.9232  0.9771 #> Pretrain     0.7  0.9794 0.9794  0.9794  0.8620  0.9679   1.171  0.9181  0.9777 #> Pretrain     0.8  0.9810 0.9810  0.9810  0.8621  0.9784   1.157  0.9230  0.9843 #> Pretrain     0.9  0.9765 0.9765  0.9765  0.8656  0.9730   1.160  0.9191  0.9646 #> Pretrain     1.0  0.9803 0.9803  0.9803  0.8590  0.9759   1.165  0.9312  0.9699 #> Individual        0.9803 0.9803  0.9803  0.8590  0.9759   1.165  0.9312  0.9699 #>  #> alphahat (fixed) = 0.6 #> alphahat (varying): #> group_1 group_2 group_3 group_4 group_5  #>     0.6     0.6     0.6     0.7     0.9  plot(cvfit) # to see CV performance as a function of alpha   predict(cvfit, xtest, groupstest, s=\"lambda.min\") # to predict with held out data #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, groupstest = groupstest,   #>     s = \"lambda.min\")  #>  #>  #> alpha =  0.6  #>  #> Support size: #>                                        #> Overall    3                           #> Pretrain   9 (0 common + 9 individual) #> Individual 9                           predict(cvfit, xtest, groupstest, s=\"lambda.min\", ytest=ytest) # to also measure performance #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest, s = \"lambda.min\")  #>  #>  #> alpha =  0.6  #>  #> Performance (Mean squared error): #>  #>            allGroups  mean group_1 group_2 group_3 group_4 group_5       r^2 #> Overall        1.141 1.141   1.098   1.136   1.313   1.043   1.115 -0.003342 #> Pretrain       1.148 1.148   1.100   1.177   1.308   1.041   1.116 -0.009888 #> Individual     1.149 1.149   1.100   1.182   1.308   1.041   1.116 -0.010818 #>  #> Support size: #>                                        #> Overall    3                           #> Pretrain   9 (0 common + 9 individual) #> Individual 9                            # By default, we used s = \"lambda.min\" to compute CV performance. # We could instead use s = \"lambda.1se\": cvfit = cv.ptLasso(x, y, groups = groups, family = \"gaussian\", type.measure = \"mse\",                    s = \"lambda.1se\")  # We could have used the glmnet option relax = TRUE: cvfit = cv.ptLasso(x, y, groups = groups, family = \"gaussian\", type.measure = \"mse\",                    relax = TRUE) # And, as we did with lambda, we may want to specify the choice of gamma to compute CV performance: cvfit = cv.ptLasso(x, y, groups = groups, family = \"gaussian\", type.measure = \"mse\",                    relax = TRUE, gamma = \"gamma.1se\")  # Note that the first stage of pretraining uses \"lambda.1se\" and \"gamma.1se\" by default. # This behavior can be modified by specifying overall.lambda and overall.gamma; # see the documentation for ptLasso for more information.  # Now, we are ready to simulate slightly more realistic data. # This continuous outcome example has k = 5 groups, where each group has 200 observations. # There are scommon = 10 features shared across all groups, and # sindiv = 10 features unique to each group. # n = 1000 and p = 120 (60 informative features and 60 noise features). # The coefficients of the common features differ across groups (beta.common). # In group 1, these coefficients are rep(1, 10); in group 2 they are rep(2, 10), etc. # Each group has 10 unique features, the coefficients of which are all 3 (beta.indiv). # The intercept in all groups is 0. # The variable sigma = 20 indicates that we add noise to y according to 20 * rnorm(n).  set.seed(1234) k=5 class.sizes=rep(200, k) scommon=10; sindiv=rep(10, k) n=sum(class.sizes); p=2*(sum(sindiv) + scommon) beta.common=3*(1:k); beta.indiv=rep(3, k) intercepts=rep(0, k) sigma=20 out = gaussian.example.data(k=k, class.sizes=class.sizes,                             scommon=scommon, sindiv=sindiv,                             n=n, p=p,                             beta.common=beta.common, beta.indiv=beta.indiv,                             intercepts=intercepts, sigma=20) x = out$x; y=out$y; groups = out$group  outtest = gaussian.example.data(k=k, class.sizes=class.sizes,                                 scommon=scommon, sindiv=sindiv,                                 n=n, p=p,                                 beta.common=beta.common, beta.indiv=beta.indiv,                                 intercepts=intercepts, sigma=20) xtest=outtest$x; ytest=outtest$y; groupstest=outtest$groups  cvfit = cv.ptLasso(x, y, groups = groups, family = \"gaussian\", type.measure = \"mse\") cvfit #>  #> Call:   #> cv.ptLasso(x = x, y = y, groups = groups, family = \"gaussian\",   #>     type.measure = \"mse\", use.case = \"inputGroups\", group.intercepts = TRUE)  #>  #>  #>  #> type.measure:  mse  #>  #>  #>            alpha overall  mean wtdMean group_1 group_2 group_3 group_4 group_5 #> Overall            699.7 699.7   699.7   748.4   501.9   575.6   663.0  1009.9 #> Pretrain     0.0   518.6 518.6   518.6   470.1   471.5   547.0   540.7   563.7 #> Pretrain     0.1   506.0 506.0   506.0   429.7   452.1   538.7   551.1   558.3 #> Pretrain     0.2   495.3 495.3   495.3   393.6   460.6   565.5   530.9   526.1 #> Pretrain     0.3   490.4 490.4   490.4   390.4   436.5   546.3   511.6   567.4 #> Pretrain     0.4   487.5 487.5   487.5   383.7   438.8   545.6   509.4   560.3 #> Pretrain     0.5   481.2 481.2   481.2   364.9   429.7   548.5   513.4   549.7 #> Pretrain     0.6   504.1 504.1   504.1   393.1   460.0   586.4   531.9   549.0 #> Pretrain     0.7   511.5 511.5   511.5   393.2   462.7   584.3   492.9   624.3 #> Pretrain     0.8   509.1 509.1   509.1   382.4   496.2   597.9   503.4   565.6 #> Pretrain     0.9   501.5 501.5   501.5   404.0   481.6   581.9   488.3   552.0 #> Pretrain     1.0   517.1 517.1   517.1   409.1   488.9   612.7   484.7   590.1 #> Individual         517.1 517.1   517.1   409.1   488.9   612.7   484.7   590.1 #>  #> alphahat (fixed) = 0.5 #> alphahat (varying): #> group_1 group_2 group_3 group_4 group_5  #>     0.5     0.5     0.1     1.0     0.2  # plot(cvfit) # to see CV performance as a function of alpha  predict(cvfit, xtest, groupstest, ytest=ytest, s=\"lambda.min\") #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest, s = \"lambda.min\")  #>  #>  #> alpha =  0.5  #>  #> Performance (Mean squared error): #>  #>            allGroups  mean group_1 group_2 group_3 group_4 group_5    r^2 #> Overall        755.7 755.7   836.0   554.9   565.4   777.9  1044.0 0.5371 #> Pretrain       500.2 500.2   539.0   443.8   553.5   502.5   462.4 0.6936 #> Individual     532.8 532.8   584.1   443.2   567.2   550.5   518.9 0.6736 #>  #> Support size: #>                                           #> Overall    64                             #> Pretrain   92 (21 common + 71 individual) #> Individual 109                             # Now, we repeat with a binomial outcome. # This example has k = 3 groups, where each group has 100 observations. # There are scommon = 5 features shared across all groups, and # sindiv = 5 features unique to each group. # n = 300 and p = 40 (20 informative features and 20 noise features). # The coefficients of the common features differ across groups (beta.common), # as do the coefficients specific to each group (beta.indiv). set.seed(1234) k=3 class.sizes=rep(100, k) scommon=5; sindiv=rep(5, k) n=sum(class.sizes); p=2*(sum(sindiv) + scommon) beta.common=list(c(-.5, .5, .3, -.9, .1), c(-.3, .9, .1, -.1, .2), c(0.1, .2, -.1, .2, .3)) beta.indiv = lapply(1:k, function(i)  0.9 * beta.common[[i]])  out = binomial.example.data(k=k, class.sizes=class.sizes,                             scommon=scommon, sindiv=sindiv,                             n=n, p=p,                             beta.common=beta.common, beta.indiv=beta.indiv) x = out$x; y=out$y; groups = out$group  outtest = binomial.example.data(k=k, class.sizes=class.sizes,                                 scommon=scommon, sindiv=sindiv,                                 n=n, p=p,                                 beta.common=beta.common, beta.indiv=beta.indiv) xtest=outtest$x; ytest=outtest$y; groupstest=outtest$groups  cvfit = cv.ptLasso(x, y, groups = groups, family = \"binomial\",                    type.measure = \"auc\", nfolds=3, verbose=TRUE, alphahat.choice=\"mean\") #>  #> alpha= 0 #> Fitting overall model #> Fitting individual models #> \tFitting individual model 1 / 3 #> \tFitting individual model 2 / 3 #> \tFitting individual model 3 / 3 #> Fitting pretrained lasso models #> \tFitting pretrained model 1 / 3 #> \tFitting pretrained model 2 / 3 #> \tFitting pretrained model 3 / 3 #>  #> alpha= 0.1 #> Fitting pretrained lasso models #> \tFitting pretrained model 1 / 3 #> \tFitting pretrained model 2 / 3 #> \tFitting pretrained model 3 / 3 #>  #> alpha= 0.2 #> Fitting pretrained lasso models #> \tFitting pretrained model 1 / 3 #> \tFitting pretrained model 2 / 3 #> \tFitting pretrained model 3 / 3 #>  #> alpha= 0.3 #> Fitting pretrained lasso models #> \tFitting pretrained model 1 / 3 #> \tFitting pretrained model 2 / 3 #> \tFitting pretrained model 3 / 3 #>  #> alpha= 0.4 #> Fitting pretrained lasso models #> \tFitting pretrained model 1 / 3 #> \tFitting pretrained model 2 / 3 #> \tFitting pretrained model 3 / 3 #>  #> alpha= 0.5 #> Fitting pretrained lasso models #> \tFitting pretrained model 1 / 3 #> \tFitting pretrained model 2 / 3 #> \tFitting pretrained model 3 / 3 #>  #> alpha= 0.6 #> Fitting pretrained lasso models #> \tFitting pretrained model 1 / 3 #> \tFitting pretrained model 2 / 3 #> \tFitting pretrained model 3 / 3 #>  #> alpha= 0.7 #> Fitting pretrained lasso models #> \tFitting pretrained model 1 / 3 #> \tFitting pretrained model 2 / 3 #> \tFitting pretrained model 3 / 3 #>  #> alpha= 0.8 #> Fitting pretrained lasso models #> \tFitting pretrained model 1 / 3 #> \tFitting pretrained model 2 / 3 #> \tFitting pretrained model 3 / 3 #>  #> alpha= 0.9 #> Fitting pretrained lasso models #> \tFitting pretrained model 1 / 3 #> \tFitting pretrained model 2 / 3 #> \tFitting pretrained model 3 / 3 #>  #> alpha= 1 #> Fitting pretrained lasso models cvfit #>  #> Call:   #> cv.ptLasso(x = x, y = y, groups = groups, family = \"binomial\",   #>     type.measure = \"auc\", nfolds = 3, verbose = TRUE, alphahat.choice = \"mean\",   #>     use.case = \"inputGroups\", group.intercepts = TRUE)  #>  #>  #>  #> type.measure:  auc  #>  #>  #>            alpha overall   mean wtdMean group_1 group_2 group_3 #> Overall           0.5770 0.5717  0.5717  0.6933  0.5994  0.4223 #> Pretrain     0.0  0.6272 0.6242  0.6242  0.8001  0.6819  0.3905 #> Pretrain     0.1  0.7223 0.7613  0.7613  0.8574  0.7504  0.6761 #> Pretrain     0.2  0.6486 0.7434  0.7434  0.8224  0.7625  0.6453 #> Pretrain     0.3  0.6919 0.7124  0.7124  0.8391  0.7060  0.5921 #> Pretrain     0.4  0.6665 0.7127  0.7127  0.8191  0.7501  0.5688 #> Pretrain     0.5  0.7311 0.7600  0.7600  0.8223  0.7798  0.6778 #> Pretrain     0.6  0.6856 0.7527  0.7527  0.8370  0.7576  0.6635 #> Pretrain     0.7  0.6641 0.6990  0.6990  0.7963  0.7441  0.5566 #> Pretrain     0.8  0.6637 0.7133  0.7133  0.8131  0.7571  0.5696 #> Pretrain     0.9  0.6741 0.7470  0.7470  0.8367  0.7809  0.6235 #> Pretrain     1.0  0.5882 0.6894  0.6894  0.7896  0.7112  0.5673 #> Individual        0.5882 0.6894  0.6894  0.7896  0.7112  0.5673 #>  #> alphahat (fixed) = 0.1 #> alphahat (varying): #> group_1 group_2 group_3  #>     0.1     0.9     0.5  # plot(cvfit) # to see CV performance as a function of alpha  predict(cvfit, xtest, groupstest, ytest=ytest, s=\"lambda.1se\") #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest, s = \"lambda.1se\")  #>  #>  #> alpha =  0.1  #>  #> Performance (AUC): #>  #>            allGroups   mean wtdMean group_1 group_2 group_3 #> Overall       0.6070 0.6072  0.6072  0.5985  0.6764  0.5467 #> Pretrain      0.6372 0.6383  0.6383  0.6646  0.7061  0.5443 #> Individual    0.6762 0.6435  0.6435  0.6663  0.7644  0.5000 #>  #> Support size: #>                                          #> Overall    3                             #> Pretrain   23 (3 common + 20 individual) #> Individual 7                              if (FALSE) { # \\dontrun{ ### Model fitting with parallel = TRUE require(doMC) registerDoMC(cores = 4) cvfit = cv.ptLasso(x, y, groups = groups, family = \"binomial\",                    type.measure = \"auc\", parallel=TRUE) } # }  # Multiresponse pretraining # Now let's consider the case of a multiresponse outcome. We'll start by simulating data: set.seed(1234) n = 1000; ntrain = 500; p = 500 sigma = 2       x = matrix(rnorm(n*p), n, p) beta1 = c(rep(1, 5), rep(0.5, 5), rep(0, p - 10)) beta2 = c(rep(1, 5), rep(0, 5), rep(0.5, 5), rep(0, p - 15))  mu = cbind(x %*% beta1, x %*% beta2) y  = cbind(mu[, 1] + sigma * rnorm(n),             mu[, 2] + sigma * rnorm(n)) cat(\"SNR for the two tasks:\", round(diag(var(mu)/var(y-mu)), 2), fill=TRUE) #> SNR for the two tasks: 1.6 1.44 cat(\"Correlation between two tasks:\", cor(y[, 1], y[, 2]), fill=TRUE) #> Correlation between two tasks: 0.5164748  xtest = x[-(1:ntrain), ] ytest = y[-(1:ntrain), ]  x = x[1:ntrain, ] y = y[1:ntrain, ]  # Now, we can fit a ptLasso model: fit = cv.ptLasso(x, y, type.measure = \"mse\", use.case = \"multiresponse\") plot(fit) # to see the cv curve.  predict(fit, xtest) # to predict with new data #>  #> Call:   #> predict.cv.ptLasso(object = fit, xtest = xtest)  #>  #>  #>  #> alpha =  0.2  #>  #> Support size: #>                                          #> Overall    57                            #> Pretrain   23 (19 common + 4 individual) #> Individual 80                            predict(fit, xtest, ytest=ytest) # if ytest is included, we also measure performance #>  #> Call:   #> predict.cv.ptLasso(object = fit, xtest = xtest, ytest = ytest)  #>  #>  #>  #> alpha =  0.2  #>  #> Performance (Mean squared error): #>  #>            allGroups  mean response_1 response_2 #> Overall        9.394 4.697      4.227      5.168 #> Pretrain       8.907 4.453      4.186      4.721 #> Individual     9.465 4.733      4.243      5.222 #>  #> Support size: #>                                          #> Overall    57                            #> Pretrain   23 (19 common + 4 individual) #> Individual 80                            # By default, we used s = \"lambda.min\" to compute CV performance. # We could instead use s = \"lambda.1se\": cvfit = cv.ptLasso(x, y, type.measure = \"mse\", s = \"lambda.1se\", use.case = \"multiresponse\")  # We could also use the glmnet option relax = TRUE: cvfit = cv.ptLasso(x, y, type.measure = \"mse\", relax = TRUE, use.case = \"multiresponse\") # And, as we did with lambda, we may want to specify the choice of gamma to compute CV performance: cvfit = cv.ptLasso(x, y, type.measure = \"mse\", relax = TRUE, gamma = \"gamma.1se\",                    use.case = \"multiresponse\")  # Time series pretraining # Now suppose we have time series data with a binomial outcome measured at 3 different time points. set.seed(1234) n = 600; ntrain = 300; p = 50 x = matrix(rnorm(n*p), n, p)  beta1 = c(rep(0.25, 10), rep(0, p-10)) beta2 = beta1 + c(rep(0.1, 10), runif(5, min = -0.25, max = 0), rep(0, p-15)) beta3 = beta1 + c(rep(0.2, 10), runif(5, min = -0.25, max = 0),                   runif(5, min = 0, max = 0.1), rep(0, p-20))  y1 = rbinom(n, 1, prob = 1/(1 + exp(-x %*% beta1))) y2 = rbinom(n, 1, prob = 1/(1 + exp(-x %*% beta2))) y3 = rbinom(n, 1, prob = 1/(1 + exp(-x %*% beta3))) y = cbind(y1, y2, y3)  xtest = x[-(1:ntrain), ] ytest = y[-(1:ntrain), ]  x = x[1:ntrain, ] y = y[1:ntrain, ]  cvfit =  cv.ptLasso(x, y, use.case=\"timeSeries\", family=\"binomial\",                     type.measure=\"auc\") plot(cvfit, plot.alphahat = TRUE)  predict(cvfit, xtest, ytest=ytest) #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, ytest = ytest)  #>  #>  #>  #> alpha =  0.8  #>  #> Performance (AUC): #>  #>              mean response_1 response_2 response_3 #> Pretrain   0.7108     0.6731     0.7179     0.7414 #> Individual 0.7042     0.6731     0.7116     0.7279 #>  #> Support size: #>                                           #> Pretrain   39 (21 common + 18 individual) #> Individual 39                              # The glmnet option relax = TRUE: cvfit = cv.ptLasso(x, y, type.measure = \"auc\", family = \"binomial\", relax = TRUE,                    use.case = \"timeSeries\")"},{"path":"https://erincr.github.io/ptLasso/reference/gaussian.example.data.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulate input grouped data (gaussian outcome) for testing with ptLasso. — gaussian.example.data","title":"Simulate input grouped data (gaussian outcome) for testing with ptLasso. — gaussian.example.data","text":"required arguments; used primarily documentation. Simply calls makedata reasonable set features.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/gaussian.example.data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulate input grouped data (gaussian outcome) for testing with ptLasso. — gaussian.example.data","text":"","code":"gaussian.example.data(   k = 5,   class.sizes = rep(200, k),   n = sum(class.sizes),   scommon = 10,   sindiv = rep(10, k),   p = 2 * (sum(sindiv) + scommon),   beta.common = 3 * (1:k),   beta.indiv = rep(3, k),   intercepts = rep(0, k),   sigma = 20 )"},{"path":"https://erincr.github.io/ptLasso/reference/gaussian.example.data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulate input grouped data (gaussian outcome) for testing with ptLasso. — gaussian.example.data","text":"k Default: 5. class.sizes Default: rep(200, k). n Default: sum(class.sizes). scommon Default: 10. sindiv Default: rep(10, k). p Default: 2*(sum(sindiv) + scommon). beta.common Default: 3*(1:k). beta.indiv Default: rep(3, k). intercepts Default: rep(0, k). sigma Default: 20.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/gaussian.example.data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simulate input grouped data (gaussian outcome) for testing with ptLasso. — gaussian.example.data","text":"list data 5 groups gaussian outcome, n=1000 p=120: x Simulated features, size n x p. y Outcomes y, length n. groups Vector length n, indicating observations belong group. snr Gaussian outcome : signal noise ratio. mu Gaussian outcome : value y noise added.","code":""},{"path":[]},{"path":"https://erincr.github.io/ptLasso/reference/gaussian.example.data.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Simulate input grouped data (gaussian outcome) for testing with ptLasso. — gaussian.example.data","text":"Erin Craig Rob Tibshirani Maintainer: Erin Craig <erincr@stanford.edu>","code":""},{"path":"https://erincr.github.io/ptLasso/reference/gaussian.example.data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulate input grouped data (gaussian outcome) for testing with ptLasso. — gaussian.example.data","text":"","code":"out = gaussian.example.data() x = out$x; y=out$y; groups = out$group"},{"path":"https://erincr.github.io/ptLasso/reference/get.individual.support.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the support for individual models — get.individual.support","title":"Get the support for individual models — get.individual.support","text":"Get indices nonzero coefficients individual models fitted ptLasso cv.ptLasso object, excluding intercept.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/get.individual.support.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the support for individual models — get.individual.support","text":"","code":"get.individual.support(   fit,   s = \"lambda.min\",   gamma = \"gamma.min\",   commonOnly = FALSE,   groups = 1:length(fit$fitind) )"},{"path":"https://erincr.github.io/ptLasso/reference/get.individual.support.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the support for individual models — get.individual.support","text":"fit fitted \"ptLasso\" \"cv.ptLasso\" object. s choice lambda use. May \"lambda.min\", \"lambda.1se\" numeric value. Default \"lambda.min\". gamma use 'relax = TRUE' specified training. choice 'gamma' use. May \"gamma.min\" \"gamma.1se\". Default \"gamma.min\". commonOnly whether return features chosen half group- response-specific models (TRUE) features chosen group-specific models (FALSE). Default FALSE. groups groups responses include computing support. Default include groups/responses.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/get.individual.support.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the support for individual models — get.individual.support","text":"returns vector containing indices nonzero coefficients (excluding intercept).","code":""},{"path":[]},{"path":"https://erincr.github.io/ptLasso/reference/get.individual.support.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Get the support for individual models — get.individual.support","text":"Erin Craig Rob Tibshirani Maintainer: Erin Craig <erincr@stanford.edu>","code":""},{"path":"https://erincr.github.io/ptLasso/reference/get.individual.support.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get the support for individual models — get.individual.support","text":"","code":"# Train data set.seed(1234) out = gaussian.example.data() x = out$x; y=out$y; groups = out$group;  fit = ptLasso(x, y, groups = groups, family = \"gaussian\", type.measure = \"mse\")  get.individual.support(fit)  #>   [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18 #>  [19]  19  20  21  22  23  25  27  28  29  30  31  32  34  35  36  37  38  39 #>  [37]  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57 #>  [55]  58  59  60  62  63  64  65  66  67  68  69  70  72  73  75  76  77  78 #>  [73]  79  80  82  83  84  87  88  89  90  91  92  93  94  95  98  99 100 101 #>  [91] 102 103 104 105 106 107 108 109 110 111 112 113 114 115 117 118 119 120  # only return features common to all groups  get.individual.support(fit, commonOnly = TRUE)  #>  [1]   1   2   3   4   5   6   7   8   9  10  11  13  14  37  38  48  51  54  60 #> [20]  65  67  73  80  82  83 108 118  # group 1 only get.individual.support(fit, groups = 1)  #>  [1]   1   2   3   4   5   6   7   8   9  10  11  13  14  15  16  18  20  22  29 #> [20]  31  32  34  37  38  47  48  50  54  57  58  59  60  62  64  66  67  70  72 #> [39]  73  77  79  80  83  88  89  91  93  94  95 101 103 104 106 108 111 113 114 #> [58] 115 117 118 119  cvfit = cv.ptLasso(x, y, groups = groups, family = \"gaussian\", type.measure = \"mse\")  get.individual.support(cvfit) #>   [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18 #>  [19]  19  20  21  22  23  25  27  28  29  30  31  32  34  35  36  37  38  39 #>  [37]  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57 #>  [55]  58  59  60  61  62  63  64  65  66  67  68  69  70  72  73  75  76  77 #>  [73]  78  79  80  82  83  84  87  88  89  90  91  92  93  94  95  98  99 100 #>  [91] 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 117 118 119 #> [109] 120  # group 1 only get.individual.support(cvfit, groups = 1)  #>  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  18  19  20 #> [20]  22  29  31  32  34  37  38  43  47  48  50  54  56  57  58  59  60  62  64 #> [39]  65  66  67  70  72  73  75  77  79  80  83  88  89  90  91  93  94  95 101 #> [58] 103 104 106 108 111 113 114 115 117 118 119"},{"path":"https://erincr.github.io/ptLasso/reference/get.overall.support.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the support for the overall model — get.overall.support","title":"Get the support for the overall model — get.overall.support","text":"Get indices nonzero coefficients overall model fitted ptLasso cv.ptLasso object, excluding intercept.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/get.overall.support.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the support for the overall model — get.overall.support","text":"","code":"get.overall.support(fit, s = \"lambda.min\", gamma = \"gamma.min\")"},{"path":"https://erincr.github.io/ptLasso/reference/get.overall.support.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the support for the overall model — get.overall.support","text":"fit fitted \"ptLasso\" \"cv.ptLasso\" object. s choice lambda use. May \"lambda.min\", \"lambda.1se\" numeric value. Default \"lambda.min\". gamma use 'relax = TRUE' specified training. choice 'gamma' use. May \"gamma.min\" \"gamma.1se\". Default \"gamma.min\".","code":""},{"path":"https://erincr.github.io/ptLasso/reference/get.overall.support.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the support for the overall model — get.overall.support","text":"returns vector containing indices nonzero coefficients (excluding intercept).","code":""},{"path":[]},{"path":"https://erincr.github.io/ptLasso/reference/get.overall.support.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Get the support for the overall model — get.overall.support","text":"Erin Craig Rob Tibshirani Maintainer: Erin Craig <erincr@stanford.edu>","code":""},{"path":"https://erincr.github.io/ptLasso/reference/get.overall.support.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get the support for the overall model — get.overall.support","text":"","code":"# Train data set.seed(1234) out = gaussian.example.data() x = out$x; y=out$y; groups = out$group;  fit = ptLasso(x, y, groups = groups, family = \"gaussian\", type.measure = \"mse\")  get.overall.support(fit, s=\"lambda.min\")  #>  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  15  21  25  27  29  31 #> [20]  35  36  38  43  45  46  48  50  52  54  56  57  59  61  62  63  65  69  73 #> [39]  79  80  84  88  94  98 100 105 109 get.overall.support(fit, s=\"lambda.1se\")  #>  [1]  1  2  3  4  5  6  7  8  9 10 11 54 56 57 69 84  cvfit = cv.ptLasso(x, y, groups = groups, family = \"gaussian\", type.measure = \"mse\")  get.overall.support(cvfit, s=\"lambda.min\")  #>  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  15  21  25  27  29  31 #> [20]  35  36  38  43  45  46  48  50  52  54  56  57  59  61  62  63  65  69  73 #> [39]  79  80  84  88  93  94  98 100 105 109 112 115 get.overall.support(cvfit, s=\"lambda.1se\")  #>  [1]   1   2   3   4   5   6   7   8   9  10  11  27  35  36  38  45  50  52  54 #> [20]  56  57  61  62  63  69  80  84  88 100"},{"path":"https://erincr.github.io/ptLasso/reference/get.pretrain.support.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the support for pretrained models — get.pretrain.support","title":"Get the support for pretrained models — get.pretrain.support","text":"Get indices nonzero coefficients pretrained models fitted ptLasso cv.ptLasso object, excluding intercept.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/get.pretrain.support.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the support for pretrained models — get.pretrain.support","text":"","code":"get.pretrain.support(   fit,   s = \"lambda.min\",   gamma = \"gamma.min\",   commonOnly = FALSE,   includeOverall = TRUE,   groups = 1:length(fit$fitind) )"},{"path":"https://erincr.github.io/ptLasso/reference/get.pretrain.support.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the support for pretrained models — get.pretrain.support","text":"fit fitted \"ptLasso\" \"cv.ptLasso\" object. s choice lambda use. May \"lambda.min\", \"lambda.1se\" numeric value. Default \"lambda.min\". gamma use 'relax = TRUE' specified training. choice 'gamma' use. May \"gamma.min\" \"gamma.1se\". Default \"gamma.min\". commonOnly whether return features chosen half group- response-specific models (TRUE) features chosen group-specific models (FALSE). Default FALSE. includeOverall whether return features chosen overall model group-specific models (TRUE) features chosen overall model group-specific models (FALSE). Default TRUE. used 'use.case = \"timeSeries\"'. groups groups responses include computing support. Default include groups/responses.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/get.pretrain.support.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the support for pretrained models — get.pretrain.support","text":"ptLasso object supplied, returns vector containing indices nonzero coefficients (excluding intercept). cv.ptLasso object supplied, returns list results - one value alpha.","code":""},{"path":[]},{"path":"https://erincr.github.io/ptLasso/reference/get.pretrain.support.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Get the support for pretrained models — get.pretrain.support","text":"Erin Craig Rob Tibshirani Maintainer: Erin Craig <erincr@stanford.edu>","code":""},{"path":"https://erincr.github.io/ptLasso/reference/get.pretrain.support.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get the support for pretrained models — get.pretrain.support","text":"","code":"# Train data set.seed(1234) out = gaussian.example.data() x = out$x; y=out$y; groups = out$group;  fit = ptLasso(x, y, groups = groups, family = \"gaussian\", type.measure = \"mse\")  get.pretrain.support(fit)  #>  [1]   1   2   3   4   5   6   7   8   9  10  11  13  14  15  16  18  19  20  21 #> [20]  22  23  25  27  28  29  30  31  32  34  35  37  38  39  41  42  43  44  45 #> [39]  46  47  48  49  50  51  52  53  54  55  56  57  59  60  62  64  65  66  67 #> [58]  68  69  72  73  75  76  77  79  80  82  83  84  88  89  90  94  97  98  99 #> [77] 100 102 105 108 109 110 111 112 114 115 118 120  # only return features common to all groups  get.pretrain.support(fit, commonOnly = TRUE)  #>  [1]  1  2  3  4  5  6  7  8  9 10 11 51 54 56 57 65 69 80 83 84 88  # group 1 only, don't include the overall model support get.pretrain.support(fit, groups = 1, includeOverall = FALSE)  #>  [1]   1   5  11  13  14  15  16  20  31  32  34  57  64  72  73  80  83  88  89 #> [20]  94 111 114  # group 1 only, include the overall model support get.pretrain.support(fit, groups = 1, includeOverall = TRUE)  #>  [1]   1   2   3   4   5   6   7   8   9  10  11  13  14  15  16  20  31  32  34 #> [20]  54  56  57  64  69  72  73  80  83  84  88  89  94 111 114  cvfit = cv.ptLasso(x, y, groups = groups, family = \"gaussian\", type.measure = \"mse\")  get.pretrain.support(cvfit) #> [[1]] #>  [1]   1   2   3   4   5   6   7   8   9  10  11  27  35  36  38  45  50  52  54 #> [20]  56  57  61  62  63  69  80  84  88 100 #>  #> [[2]] #>  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  18  20  22 #> [20]  24  25  27  29  31  32  34  35  36  37  38  43  45  47  48  50  52  53  54 #> [39]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  72  73  75 #> [58]  80  83  84  88  89  90  91  93  94 100 103 106 108 111 113 114 115 117 118 #> [77] 119 #>  #> [[3]] #>  [1]   1   2   3   4   5   6   7   8   9  10  11  13  14  15  16  18  20  21  22 #> [20]  23  25  27  29  30  31  32  34  35  36  37  38  43  45  46  47  48  50  51 #> [39]  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  69  70  72 #> [58]  73  75  80  82  83  84  88  89  91  94 100 103 106 108 110 111 113 114 115 #> [77] 117 119 120 #>  #> [[4]] #>  [1]   1   2   3   4   5   6   7   8   9  10  11  13  14  15  16  18  20  21  22 #> [20]  23  27  29  30  31  32  34  35  36  37  38  41  42  43  45  46  47  48  49 #> [39]  50  51  52  53  54  55  56  57  60  61  62  63  64  65  66  67  69  72  73 #> [58]  75  79  80  83  84  88  89  90  91  94 100 106 108 109 110 111 114 115 117 #> [77] 119 120 #>  #> [[5]] #>  [1]   1   2   3   4   5   6   7   8   9  10  11  13  14  20  21  22  23  25  27 #> [20]  29  30  32  35  36  37  38  41  42  43  45  46  47  48  49  50  51  52  53 #> [39]  54  55  56  57  59  60  61  62  63  65  69  75  79  80  82  83  84  88  90 #> [58] 100 105 108 110 120 #>  #> [[6]] #>  [1]   1   2   3   4   5   6   7   8   9  10  11  13  14  15  16  20  21  22  23 #> [20]  25  27  29  30  31  32  34  35  36  37  38  41  42  43  45  46  47  48  49 #> [39]  50  51  52  53  54  55  56  57  59  60  61  62  63  64  65  66  69  72  73 #> [58]  75  76  77  79  80  83  84  88  89  90  94  99 100 108 109 110 111 112 114 #> [77] 117 118 120 #>  #> [[7]] #>  [1]   1   2   3   4   5   6   7   8   9  10  11  13  14  15  16  18  20  21  22 #> [20]  23  25  27  28  29  30  31  32  34  35  36  37  38  39  40  41  42  43  44 #> [39]  45  46  47  48  49  50  51  52  53  54  55  56  57  59  60  61  62  63  64 #> [58]  65  66  67  68  69  72  73  75  76  77  79  80  82  83  84  88  89  90  91 #> [77]  93  94  98  99 100 102 105 108 109 110 111 112 113 114 115 117 118 119 120 #>  #> [[8]] #>  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  18  19  20 #> [20]  21  22  23  25  27  28  29  30  31  32  34  35  36  37  38  39  40  41  42 #> [39]  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  59  60  61  62 #> [58]  63  64  65  66  67  68  69  72  73  75  76  77  79  80  82  83  84  88  89 #> [77]  90  93  94  98  99 100 102 105 108 109 110 111 112 113 114 115 118 120 #>  #> [[9]] #>  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19 #> [20]  20  21  22  23  25  27  28  29  30  32  35  36  37  38  39  40  41  42  43 #> [39]  44  45  46  47  48  49  50  51  52  53  54  55  56  57  59  60  61  62  63 #> [58]  64  65  66  67  68  69  72  73  75  76  77  79  80  82  83  84  88  89  90 #> [77]  93  94  98  99 100 102 105 108 109 110 111 112 115 118 120 #>  #> [[10]] #>   [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18 #>  [19]  19  20  21  22  23  25  27  28  29  30  31  32  34  35  36  37  38  39 #>  [37]  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57 #>  [55]  58  59  60  61  62  63  64  65  66  67  68  69  70  72  73  75  76  77 #>  [73]  78  79  80  82  83  84  87  88  89  90  91  92  93  94  95  97  98  99 #>  [91] 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 117 118 #> [109] 119 120 #>  #> [[11]] #>   [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18 #>  [19]  19  20  21  22  23  25  27  28  29  30  31  32  34  35  36  37  38  39 #>  [37]  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57 #>  [55]  58  59  60  61  62  63  64  65  66  67  68  69  70  72  73  75  76  77 #>  [73]  78  79  80  82  83  84  87  88  89  90  91  92  93  94  95  98  99 100 #>  [91] 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 117 118 119 #> [109] 120 #>  get.pretrain.support(cvfit, groups = 1)  #> [[1]] #>  [1]   1   2   3   4   5   6   7   8   9  10  11  27  35  36  38  45  50  52  54 #> [20]  56  57  61  62  63  69  80  84  88 100 #>  #> [[2]] #>  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  18  20  22 #> [20]  24  25  27  29  31  32  34  35  36  37  38  43  45  47  48  50  52  54  56 #> [39]  57  58  59  60  61  62  63  64  65  66  67  68  69  70  72  73  75  80  83 #> [58]  84  88  89  90  91  93  94 100 103 106 108 111 113 114 115 117 118 119 #>  #> [[3]] #>  [1]   1   2   3   4   5   6   7   8   9  10  11  13  14  15  16  18  20  22  27 #> [20]  29  31  32  34  35  36  37  38  43  45  47  50  52  54  56  57  58  59  60 #> [39]  61  62  63  64  65  66  67  69  70  72  73  75  80  83  84  88  89  91  94 #> [58] 100 103 106 108 111 113 114 115 117 119 #>  #> [[4]] #>  [1]   1   2   3   4   5   6   7   8   9  10  11  13  14  15  16  18  20  27  31 #> [20]  32  34  35  36  37  38  45  50  52  54  56  57  60  61  62  63  64  66  67 #> [39]  69  72  73  80  83  84  88  89  91  94 100 106 108 111 114 115 117 119 #>  #> [[5]] #>  [1]   1   2   3   4   5   6   7   8   9  10  11  27  35  36  38  45  50  52  54 #> [20]  56  57  61  62  63  69  80  84  88 100 #>  #> [[6]] #>  [1]   1   2   3   4   5   6   7   8   9  10  11  13  14  15  16  20  27  31  32 #> [20]  34  35  36  37  38  45  50  52  54  56  57  61  62  63  64  66  69  72  73 #> [39]  80  83  84  88  89  94 100 108 111 114 117 #>  #> [[7]] #>  [1]   1   2   3   4   5   6   7   8   9  10  11  13  14  15  16  18  20  27  29 #> [20]  31  32  34  35  36  37  38  45  50  52  54  56  57  60  61  62  63  64  66 #> [39]  67  69  72  73  80  83  84  88  89  91  93  94 100 108 111 113 114 115 117 #> [58] 119 #>  #> [[8]] #>  [1]   1   2   3   4   5   6   7   8   9  10  11  13  14  15  16  20  27  31  32 #> [20]  34  35  36  38  45  50  52  54  56  57  61  62  63  64  66  69  72  73  80 #> [39]  83  84  88  89  94 100 111 113 114 #>  #> [[9]] #>  [1]   1   2   3   4   5   6   7   8   9  10  11  13  14  15  16  20  27  35  36 #> [20]  38  45  50  52  54  56  57  61  62  63  69  80  83  84  88  89  94 100 111 #>  #> [[10]] #>  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  18  20  22 #> [20]  27  29  31  32  34  35  36  37  38  43  45  47  48  50  52  54  56  57  58 #> [39]  59  60  61  62  63  64  65  66  67  69  70  72  73  75  77  79  80  83  84 #> [58]  88  89  90  91  93  94  95 100 101 103 104 106 108 111 113 114 115 117 118 #> [77] 119 #>  #> [[11]] #>  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  18  19  20 #> [20]  22  29  31  32  34  37  38  43  47  48  50  54  56  57  58  59  60  62  64 #> [39]  65  66  67  70  72  73  75  77  79  80  83  88  89  90  91  93  94  95 101 #> [58] 103 104 106 108 111 113 114 115 117 118 119 #>"},{"path":"https://erincr.github.io/ptLasso/reference/makedata.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulate input grouped data for testing with ptLasso. — makedata","title":"Simulate input grouped data for testing with ptLasso. — makedata","text":"Simulate input grouped data testing ptLasso.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/makedata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulate input grouped data for testing with ptLasso. — makedata","text":"","code":"makedata(   n,   p,   k,   scommon,   sindiv,   class.sizes,   beta.common,   beta.indiv,   intercepts = rep(0, k),   sigma = 0,   outcome = c(\"gaussian\", \"binomial\", \"multinomial\"),   mult.classes = 3 )"},{"path":"https://erincr.github.io/ptLasso/reference/makedata.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulate input grouped data for testing with ptLasso. — makedata","text":"n Total number observations simulate. p Total number features simulate. k Number groups. scommon Number features shared groups. sindiv Vector length k. ^th entry indicates number features specific group . class.sizes Vector length k. ^th entry indicates number observations group . beta.common coefficients common features. can vector length k, case, ^th entry coefficient scommon features group . can alternatively list length k (one group). entry list vector length scommon, containing coefficients scommon features. beta.indiv coefficients individual features, form beta.common. intercepts vector length k, indicating intercept group. Default 0. sigma used Gaussian outcome. number greater equal 0, used modify amount noise added. Default 0. outcome May '\"gaussian\"', '\"binomial\"' '\"multinomial\"'. mult.classes Number classes simulate multinomial setting.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/makedata.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simulate input grouped data for testing with ptLasso. — makedata","text":"list: x Simulated features, size n x p. y Outcomes y, length n. groups Vector length n, indicating observations belong group. snr Gaussian outcome : signal noise ratio. mu Gaussian outcome : value y noise added.","code":""},{"path":[]},{"path":"https://erincr.github.io/ptLasso/reference/makedata.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Simulate input grouped data for testing with ptLasso. — makedata","text":"Erin Craig Rob Tibshirani Maintainer: Erin Craig <erincr@stanford.edu>","code":""},{"path":"https://erincr.github.io/ptLasso/reference/makedata.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulate input grouped data for testing with ptLasso. — makedata","text":"","code":"# Data with a binary outcome: k = 3 class.sizes = rep(100, k) n = sum(class.sizes) scommon = 5 sindiv = rep(5, k)  p = 2*(sum(sindiv) + scommon) beta.common = lapply(1:k, function(i)  c(-.5, .5, .3, -.9, .1)) beta.indiv = lapply(1:k, function(i)  0.9 * beta.common[[i]])  out = makedata(n=n, p=p, k=k, scommon=scommon, sindiv=sindiv,                beta.common=beta.common, beta.indiv=beta.indiv,                class.sizes=class.sizes, outcome=\"binomial\") x = out$x; y=out$y; groups = out$group"},{"path":"https://erincr.github.io/ptLasso/reference/makedata.targetgroups.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulate target grouped data for testing with ptLasso. — makedata.targetgroups","title":"Simulate target grouped data for testing with ptLasso. — makedata.targetgroups","text":"Simulate target grouped data testing ptLasso.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/makedata.targetgroups.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulate target grouped data for testing with ptLasso. — makedata.targetgroups","text":"","code":"makedata.targetgroups(   n,   p,   scommon,   sindiv,   class.sizes,   shift.common,   shift.indiv )"},{"path":"https://erincr.github.io/ptLasso/reference/makedata.targetgroups.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulate target grouped data for testing with ptLasso. — makedata.targetgroups","text":"n Total number observations simulate. p Total number features simulate. scommon Number features shared groups. sindiv Vector length k. ^th entry indicates number features specific group . class.sizes Vector length k. ^th entry indicates number observations group . shift.common list length k (one group). entry list vector length scommon, containing shifts scommon features. ^th entry list added first scommon columns x observations group . shift.indiv shifts individual features, form shift.common.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/makedata.targetgroups.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simulate target grouped data for testing with ptLasso. — makedata.targetgroups","text":"list: x Simulated features, size n x p. y Outcomes y, length n.","code":""},{"path":[]},{"path":"https://erincr.github.io/ptLasso/reference/makedata.targetgroups.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Simulate target grouped data for testing with ptLasso. — makedata.targetgroups","text":"Erin Craig Rob Tibshirani Maintainer: Erin Craig <erincr@stanford.edu>","code":""},{"path":"https://erincr.github.io/ptLasso/reference/makedata.targetgroups.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulate target grouped data for testing with ptLasso. — makedata.targetgroups","text":"","code":"k = 5 class.sizes = rep(50, k) n = sum(class.sizes) scommon = 3 sindiv = rep(3, k) p = 3*(sum(sindiv) + scommon) shift.common  = lapply(seq(-.1, .1, length.out = k), function(i) rep(i, scommon)) shift.indiv = lapply(1:k, function(i) -shift.common[[i]])  out = makedata.targetgroups(n=n, p=p, scommon=scommon,                             sindiv=sindiv, class.sizes=class.sizes,                             shift.common=shift.common, shift.indiv=shift.indiv) x = out$x; y=out$y"},{"path":"https://erincr.github.io/ptLasso/reference/plot.cv.ptLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot the cross-validation curve produced by cv.ptLasso, as a function of the alpha values used. — plot.cv.ptLasso","title":"Plot the cross-validation curve produced by cv.ptLasso, as a function of the alpha values used. — plot.cv.ptLasso","text":"plot produced, nothing returned.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/plot.cv.ptLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot the cross-validation curve produced by cv.ptLasso, as a function of the alpha values used. — plot.cv.ptLasso","text":"","code":"# S3 method for class 'cv.ptLasso' plot(x, plot.alphahat = FALSE, ...)"},{"path":"https://erincr.github.io/ptLasso/reference/plot.cv.ptLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot the cross-validation curve produced by cv.ptLasso, as a function of the alpha values used. — plot.cv.ptLasso","text":"x Fitted \"cv.ptLasso\" object. plot.alphahat TRUE, show dashed vertical line indicating single value alpha maximized overall cross-validated performance. ... graphical parameters plot.","code":""},{"path":[]},{"path":"https://erincr.github.io/ptLasso/reference/plot.cv.ptLasso.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plot the cross-validation curve produced by cv.ptLasso, as a function of the alpha values used. — plot.cv.ptLasso","text":"Erin Craig Rob Tibshirani Maintainer: Erin Craig <erincr@stanford.edu>","code":""},{"path":"https://erincr.github.io/ptLasso/reference/plot.cv.ptLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot the cross-validation curve produced by cv.ptLasso, as a function of the alpha values used. — plot.cv.ptLasso","text":"","code":"set.seed(1234) out = gaussian.example.data() x = out$x; y=out$y; groups = out$group  cvfit = cv.ptLasso(x, y, groups = groups, family = \"gaussian\", type.measure = \"mse\") plot(cvfit)"},{"path":"https://erincr.github.io/ptLasso/reference/plot.ptLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot the models trained by a ptLasso object — plot.ptLasso","title":"Plot the models trained by a ptLasso object — plot.ptLasso","text":"plot produced, nothing returned.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/plot.ptLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot the models trained by a ptLasso object — plot.ptLasso","text":"","code":"# S3 method for class 'ptLasso' plot(x, ...)"},{"path":"https://erincr.github.io/ptLasso/reference/plot.ptLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot the models trained by a ptLasso object — plot.ptLasso","text":"x Fitted \"ptLasso\" object. ... parameters pass plot function.","code":""},{"path":[]},{"path":"https://erincr.github.io/ptLasso/reference/plot.ptLasso.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plot the models trained by a ptLasso object — plot.ptLasso","text":"Erin Craig Rob Tibshirani Maintainer: Erin Craig <erincr@stanford.edu>","code":""},{"path":"https://erincr.github.io/ptLasso/reference/plot.ptLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot the models trained by a ptLasso object — plot.ptLasso","text":"","code":"set.seed(1234) out = gaussian.example.data() x = out$x; y=out$y; groups = out$group  fit = ptLasso(x, y, groups = groups, alpha = 0.5, family = \"gaussian\", type.measure = \"mse\") plot(fit)"},{"path":"https://erincr.github.io/ptLasso/reference/predict.cv.ptLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict using a cv.ptLasso object. — predict.cv.ptLasso","title":"Predict using a cv.ptLasso object. — predict.cv.ptLasso","text":"Return predictions performance measures test set.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/predict.cv.ptLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict using a cv.ptLasso object. — predict.cv.ptLasso","text":"","code":"# S3 method for class 'cv.ptLasso' predict(   object,   xtest,   groupstest = NULL,   ytest = NULL,   alpha = NULL,   alphatype = c(\"fixed\", \"varying\"),   type = c(\"link\", \"response\", \"class\"),   s = \"lambda.min\",   gamma = \"gamma.min\",   return.link = FALSE,   ... )"},{"path":"https://erincr.github.io/ptLasso/reference/predict.cv.ptLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict using a cv.ptLasso object. — predict.cv.ptLasso","text":"object Fitted \"cv.ptLasso\" object. xtest Input matrix, matching form used \"cv.ptLasso\" model training. groupstest vector indicating group observation belongs. Coding match used model training. NULL target grouped data. ytest Response variable. Optional. included, \"predict\" compute performance measures xtest using code\"type.measure\" cvfit object. alpha chosen alpha use prediction. May vector containing one value alpha group. NULL, rely choice \"alphatype\". alphatype Choice '\"fixed\"' '\"varying\"'. '\"fixed\"', use alpha achieved best cross-validated performance. '\"varying\"', group uses alpha optimized group-specific cross-validated performance. type Type prediction required. Type '\"link\"' gives linear predictors '\"binomial\", '\"multinomial\"' '\"cox\"' models; '\"gaussian\"' models gives fitted values. Type '\"response\"' gives fitted probabilities '\"binomial\"' '\"multinomial\"', fitted relative-risk '\"cox\"'; '\"gaussian\"' type '\"response\"' equivalent type '\"link\"'. Note '\"binomial\"' models, results returned class corresponding second level factor response. Type '\"class\"' applies '\"binomial\"' '\"multinomial\"' models, produces class label corresponding maximum probability. s Value penalty parameter 'lambda' predictions required. use lambda models; can numeric value, '\"lambda.min\"' '\"lambda.1se\"'. Default '\"lambda.min\"'. gamma use 'relax = TRUE' specified training. Value penalty parameter 'gamma' predictions required. use gamma models; can numeric value, '\"gamma.min\"' '\"gamma.1se\"'. Default '\"gamma.min\"'. return.link TRUE, additionally return linear link overall, pretrained individual models: linkoverall, linkpre linkind. ... arguments passed \"predict\" function.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/predict.cv.ptLasso.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict using a cv.ptLasso object. — predict.cv.ptLasso","text":"list containing requested predictions. ytest included, also return error measures. call call produced object. alpha value(s) alpha used generate predictions. yhatoverall Predictions overall model. yhatind Predictions individual models. yhatpre Predictions pretrained models. supoverall Indices features selected overall model. supind Union indices features selected individual models. suppre.common Features selected first stage pretraining. suppre.individual Union indices features selected pretrained models, without features selected first stage. type.measure ytest supplied, performance measure computed. erroverall ytest supplied, performance overall model. named vector containing performance (1) entire dataset, (2) average performance across groups, (3) average performance across groups weighted group size (4) group-specific performance. errind ytest supplied, performance overall model. described erroverall. errpre ytest supplied, performance overall model. described erroverall. linkoverall return.link TRUE, return linear link overall model. linkind return.link TRUE, return linear link individual models. linkpre return.link TRUE, return linear link pretrained models.","code":""},{"path":[]},{"path":"https://erincr.github.io/ptLasso/reference/predict.cv.ptLasso.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Predict using a cv.ptLasso object. — predict.cv.ptLasso","text":"Erin Craig Rob Tibshirani Maintainer: Erin Craig <erincr@stanford.edu>","code":""},{"path":"https://erincr.github.io/ptLasso/reference/predict.cv.ptLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict using a cv.ptLasso object. — predict.cv.ptLasso","text":"","code":"#### Gaussian example set.seed(1234) out = gaussian.example.data() x = out$x; y=out$y; groups = out$group; outtest = gaussian.example.data() xtest=outtest$x; ytest=outtest$y; groupstest=outtest$groups;  # Model fitting # By default, use the single value of alpha that had the best CV performance on the entire dataset: cvfit = cv.ptLasso(x, y, groups = groups, family = \"gaussian\", type.measure = \"mse\") pred = predict(cvfit, xtest, groupstest, ytest=ytest, s=\"lambda.min\") pred #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest, s = \"lambda.min\")  #>  #>  #> alpha =  0.5  #>  #> Performance (Mean squared error): #>  #>            allGroups  mean group_1 group_2 group_3 group_4 group_5    r^2 #> Overall        755.7 755.7   836.0   554.9   565.4   777.9  1044.0 0.5371 #> Pretrain       500.2 500.2   539.0   443.8   553.5   502.5   462.4 0.6936 #> Individual     532.8 532.8   584.1   443.2   567.2   550.5   518.9 0.6736 #>  #> Support size: #>                                           #> Overall    64                             #> Pretrain   92 (21 common + 71 individual) #> Individual 109                             # For each group, use the value of alpha that had the best CV performance for that group: pred = predict(cvfit, xtest, groupstest, ytest=ytest, s=\"lambda.min\", alphatype = \"varying\") pred #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest, alphatype = \"varying\", s = \"lambda.min\")  #>  #>  #>  #> alpha: #> [1] 0.5 0.5 0.1 1.0 0.2 #>  #>  #> Performance (Mean squared error): #>            overall  mean wtdMean group_1 group_2 group_3 group_4 group_5 #> Overall      755.7 755.7   755.7   836.0   554.9   565.4   777.9  1044.0 #> Pretrain     508.5 508.5   508.5   539.0   443.8   554.0   550.5   455.1 #> Individual   532.8 532.8   532.8   584.1   443.2   567.2   550.5   518.9 #>  #>  #> Support size: #>                                           #> Overall    64                             #> Pretrain   97 (21 common + 76 individual) #> Individual 109                             # Specify a single value of alpha and use lambda.1se. pred = predict(cvfit, xtest, groupstest, ytest=ytest, s=\"lambda.1se\",                alphatype = \"varying\", alpha = .3) pred #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest, alpha = 0.3, alphatype = \"varying\", s = \"lambda.1se\")  #>  #>  #>  #> alpha =  0.3  #>  #> Performance (Mean squared error): #>  #>            allGroups  mean group_1 group_2 group_3 group_4 group_5    r^2 #> Overall        789.1 789.1   723.7   490.3   609.6   897.3  1224.8 0.5166 #> Pretrain       530.6 530.6   531.1   463.8   572.2   524.4   561.6 0.6750 #> Individual     564.4 564.4   536.7   468.7   643.8   572.0   600.8 0.6543 #>  #> Support size: #>                                           #> Overall    21                             #> Pretrain   56 (21 common + 35 individual) #> Individual 80                              # Specify a vector of choices for alpha:  pred = predict(cvfit, xtest, groupstest, ytest=ytest, s=\"lambda.min\",                alphatype = \"varying\", alpha = c(.1, .2, .3, .4, .5)) pred #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest, alpha = c(0.1, 0.2, 0.3, 0.4, 0.5), alphatype = \"varying\",   #>     s = \"lambda.min\")  #>  #>  #> alpha: #> [1] 0.1 0.2 0.3 0.4 0.5 #>  #>  #> Performance (Mean squared error): #>            overall  mean wtdMean group_1 group_2 group_3 group_4 group_5 #> Overall      755.7 755.7   755.7   836.0   554.9   565.4   777.9  1044.0 #> Pretrain     524.5 524.5   524.5   649.9   464.0   552.1   494.3   462.4 #> Individual   532.8 532.8   532.8   584.1   443.2   567.2   550.5   518.9 #>  #>  #> Support size: #>                                           #> Overall    64                             #> Pretrain   91 (21 common + 70 individual) #> Individual 109"},{"path":"https://erincr.github.io/ptLasso/reference/predict.ptLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict using a ptLasso object. — predict.ptLasso","title":"Predict using a ptLasso object. — predict.ptLasso","text":"Return predictions performance measures test set.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/predict.ptLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict using a ptLasso object. — predict.ptLasso","text":"","code":"# S3 method for class 'ptLasso' predict(   object,   xtest,   groupstest = NULL,   ytest = NULL,   type = c(\"link\", \"response\", \"class\"),   s = \"lambda.min\",   gamma = \"gamma.min\",   return.link = FALSE,   ... )"},{"path":"https://erincr.github.io/ptLasso/reference/predict.ptLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict using a ptLasso object. — predict.ptLasso","text":"object Fitted \"ptLasso\" object. xtest Input matrix, matching form used \"ptLasso\" model training. groupstest vector indicating group observation belongs. Coding match used model training. NULL target grouped data. ytest Response variable. Optional. included, \"predict\" compute performance measures xtest using code\"type.measure\" cvfit object. type Type prediction required. Type '\"link\"' gives linear predictors '\"binomial\", '\"multinomial\"' '\"cox\"' models; '\"gaussian\"' models gives fitted values. Type '\"response\"' gives fitted probabilities '\"binomial\"' '\"multinomial\"', fitted relative-risk '\"cox\"'; '\"gaussian\"' type '\"response\"' equivalent type '\"link\"'. Note '\"binomial\"' models, results returned class corresponding second level factor response. Type '\"class\"' applies '\"binomial\"' '\"multinomial\"' models, produces class label corresponding maximum probability. s Value penalty parameter 'lambda' predictions required. use lambda models; can numeric value, '\"lambda.min\"' '\"lambda.1se\"'. Default '\"lambda.min\"'. gamma use 'relax = TRUE' specified training. Value penalty parameter 'gamma' predictions required. use gamma models; can numeric value, '\"gamma.min\"' '\"gamma.1se\"'. Default '\"gamma.min\"'. return.link TRUE, additionally return linear link overall, pretrained individual models: linkoverall, linkpre linkind. ... arguments passed \"predict\" function.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/predict.ptLasso.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict using a ptLasso object. — predict.ptLasso","text":"list containing requested predictions. ytest included, also return error measures. call call produced object. alpha value(s) alpha used generate predictions. alpha used model training. yhatoverall Predictions overall model. yhatind Predictions individual models. yhatpre Predictions pretrained models. supoverall Indices features selected overall model. supind Union indices features selected individual models. suppre.common Features selected first stage pretraining. suppre.individual Union indices features selected pretrained models, without features selected first stage. type.measure ytest supplied, string name computed performance measure. erroverall ytest supplied, performance overall model. named vector containing performance (1) entire dataset, (2) average performance across groups, (3) average performance across groups weighted group size (4) group-specific performance. errind ytest supplied, performance overall model. described erroverall. errpre ytest supplied, performance overall model. described erroverall. linkoverall Ifreturn.link TRUE, return linear link overall model. linkind Ifreturn.link TRUE, return linear link individual models. linkpre Ifreturn.link TRUE, return linear link pretrained models.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/predict.ptLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict using a ptLasso object. — predict.ptLasso","text":"","code":"# Gaussian example set.seed(1234) out = gaussian.example.data() x = out$x; y=out$y; groups = out$group  outtest = gaussian.example.data() xtest=outtest$x; ytest=outtest$y; groupstest=outtest$groups  fit = ptLasso(x, y, groups = groups, alpha = 0.5, family = \"gaussian\", type.measure = \"mse\") pred = predict(fit, xtest, groupstest, ytest=ytest) pred #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest)  #>  #>  #> alpha =  0.5  #>  #> Performance (Mean squared error): #>  #>            allGroups  mean group_1 group_2 group_3 group_4 group_5    r^2 #> Overall        755.7 755.7   836.0   554.9   565.4   777.9  1044.0 0.5371 #> Pretrain       503.2 503.2   550.6   443.3   553.5   505.6   462.9 0.6918 #> Individual     532.8 532.8   584.1   443.2   567.2   550.5   518.9 0.6736 #>  #> Support size: #>                                           #> Overall    64                             #> Pretrain   94 (21 common + 73 individual) #> Individual 109"},{"path":"https://erincr.github.io/ptLasso/reference/print.cv.ptLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"Print the cv.ptLasso object. — print.cv.ptLasso","title":"Print the cv.ptLasso object. — print.cv.ptLasso","text":"Print cv.ptLasso object.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/print.cv.ptLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print the cv.ptLasso object. — print.cv.ptLasso","text":"","code":"# S3 method for class 'cv.ptLasso' print(x, ...)"},{"path":"https://erincr.github.io/ptLasso/reference/print.cv.ptLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print the cv.ptLasso object. — print.cv.ptLasso","text":"x fitted \"cv.ptLasso\" object. ... arguments pass print function.","code":""},{"path":[]},{"path":"https://erincr.github.io/ptLasso/reference/print.cv.ptLasso.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Print the cv.ptLasso object. — print.cv.ptLasso","text":"Erin Craig Rob Tibshirani Maintainer: Erin Craig <erincr@stanford.edu>","code":""},{"path":"https://erincr.github.io/ptLasso/reference/print.cv.ptLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print the cv.ptLasso object. — print.cv.ptLasso","text":"","code":"out = gaussian.example.data() x = out$x; y=out$y; groups = out$group;  cvfit = cv.ptLasso(x, y, groups = groups, family = \"gaussian\", type.measure = \"mse\") print(cvfit) #>  #> Call:   #> cv.ptLasso(x = x, y = y, groups = groups, family = \"gaussian\",   #>     type.measure = \"mse\", use.case = \"inputGroups\", group.intercepts = TRUE)  #>  #>  #>  #> type.measure:  mse  #>  #>  #>            alpha overall  mean wtdMean group_1 group_2 group_3 group_4 group_5 #> Overall            685.1 685.1   685.1   665.6   543.7   555.4   646.8  1014.1 #> Pretrain     0.0   555.3 555.3   555.3   530.0   516.4   603.9   543.7   582.3 #> Pretrain     0.1   521.9 521.9   521.9   499.5   492.5   560.5   517.9   539.0 #> Pretrain     0.2   526.3 526.3   526.3   489.2   484.7   584.2   511.8   561.8 #> Pretrain     0.3   514.2 514.2   514.2   485.8   500.8   531.1   523.8   529.4 #> Pretrain     0.4   519.8 519.8   519.8   467.7   487.7   570.6   505.2   567.7 #> Pretrain     0.5   522.9 522.9   522.9   465.3   499.8   567.7   517.4   564.3 #> Pretrain     0.6   518.0 518.0   518.0   459.9   504.8   563.9   515.7   545.7 #> Pretrain     0.7   545.7 545.7   545.7   479.2   552.6   601.5   553.7   541.2 #> Pretrain     0.8   545.6 545.6   545.6   500.8   539.2   584.8   536.7   566.7 #> Pretrain     0.9   538.9 538.9   538.9   516.5   535.4   571.6   515.7   555.1 #> Pretrain     1.0   579.8 579.8   579.8   554.9   587.5   638.8   541.7   575.9 #> Individual         579.8 579.8   579.8   554.9   587.5   638.8   541.7   575.9 #>  #> alphahat (fixed) = 0.3 #> alphahat (varying): #> group_1 group_2 group_3 group_4 group_5  #>     0.6     0.2     0.3     0.4     0.3"},{"path":"https://erincr.github.io/ptLasso/reference/print.predict.cv.ptLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"Print the predict.cv.ptLasso object. — print.predict.cv.ptLasso","title":"Print the predict.cv.ptLasso object. — print.predict.cv.ptLasso","text":"Print predict.cv.ptLasso object.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/print.predict.cv.ptLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print the predict.cv.ptLasso object. — print.predict.cv.ptLasso","text":"","code":"# S3 method for class 'predict.cv.ptLasso' print(x, ...)"},{"path":"https://erincr.github.io/ptLasso/reference/print.predict.cv.ptLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print the predict.cv.ptLasso object. — print.predict.cv.ptLasso","text":"x output predict called ptLasso object. ... arguments pass print function.","code":""},{"path":[]},{"path":"https://erincr.github.io/ptLasso/reference/print.predict.cv.ptLasso.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Print the predict.cv.ptLasso object. — print.predict.cv.ptLasso","text":"Erin Craig Rob Tibshirani Maintainer: Erin Craig <erincr@stanford.edu>","code":""},{"path":"https://erincr.github.io/ptLasso/reference/print.predict.cv.ptLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print the predict.cv.ptLasso object. — print.predict.cv.ptLasso","text":"","code":"# Train data out = gaussian.example.data() x = out$x; y=out$y; groups = out$group;  # Test data outtest = gaussian.example.data() xtest=outtest$x; ytest=outtest$y; groupstest=outtest$groups  cvfit = cv.ptLasso(x, y, groups = groups, family = \"gaussian\", type.measure = \"mse\") pred = predict(cvfit, xtest, groupstest, ytest=ytest, s=\"lambda.min\") print(pred) #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest, s = \"lambda.min\")  #>  #>  #> alpha =  0.5  #>  #> Performance (Mean squared error): #>  #>            allGroups  mean group_1 group_2 group_3 group_4 group_5    r^2 #> Overall        653.8 653.8   699.0   477.3   398.2   647.3  1047.4 0.5271 #> Pretrain       519.6 519.6   428.4   482.6   551.9   526.1   609.1 0.6242 #> Individual     563.6 563.6   481.3   545.4   590.1   582.0   619.0 0.5924 #>  #> Support size: #>                                           #> Overall    57                             #> Pretrain   89 (14 common + 75 individual) #> Individual 108                             # If ytest is not supplied, just prints the pretrained predictions. pred = predict(cvfit, xtest, groupstest, s=\"lambda.min\") print(pred) #>  #> Call:   #> predict.cv.ptLasso(object = cvfit, xtest = xtest, groupstest = groupstest,   #>     s = \"lambda.min\")  #>  #>  #> alpha =  0.5  #>  #> Support size: #>                                           #> Overall    57                             #> Pretrain   89 (14 common + 75 individual) #> Individual 108"},{"path":"https://erincr.github.io/ptLasso/reference/print.predict.ptLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"Print the predict.ptLasso object. — print.predict.ptLasso","title":"Print the predict.ptLasso object. — print.predict.ptLasso","text":"Print predict.ptLasso object.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/print.predict.ptLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print the predict.ptLasso object. — print.predict.ptLasso","text":"","code":"# S3 method for class 'predict.ptLasso' print(x, ...)"},{"path":"https://erincr.github.io/ptLasso/reference/print.predict.ptLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print the predict.ptLasso object. — print.predict.ptLasso","text":"x output predict called ptLasso object. ... arguments pass print function.","code":""},{"path":[]},{"path":"https://erincr.github.io/ptLasso/reference/print.predict.ptLasso.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Print the predict.ptLasso object. — print.predict.ptLasso","text":"Erin Craig Rob Tibshirani Maintainer: Erin Craig <erincr@stanford.edu>","code":""},{"path":"https://erincr.github.io/ptLasso/reference/print.predict.ptLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print the predict.ptLasso object. — print.predict.ptLasso","text":"","code":"# Train data out = gaussian.example.data() x = out$x; y=out$y; groups = out$group;  # Test data outtest = gaussian.example.data() xtest=outtest$x; ytest=outtest$y; groupstest=outtest$groups  fit = ptLasso(x, y, groups = groups, family = \"gaussian\", type.measure = \"mse\") pred = predict(fit, xtest, groupstest, ytest=ytest, s=\"lambda.min\") print(pred) #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest, s = \"lambda.min\")  #>  #>  #> alpha =  0.5  #>  #> Performance (Mean squared error): #>  #>            allGroups  mean group_1 group_2 group_3 group_4 group_5    r^2 #> Overall        685.8 685.8   637.4   498.3   570.1   735.4   987.9 0.5213 #> Pretrain       519.3 519.3   446.3   434.3   600.1   583.5   532.3 0.6376 #> Individual     563.0 563.0   524.2   455.1   656.3   622.9   556.4 0.6070 #>  #> Support size: #>                                           #> Overall    44                             #> Pretrain   89 (12 common + 77 individual) #> Individual 105                             # If ytest is not supplied, just prints the pretrained predictions. pred = predict(fit, xtest, groupstest, s=\"lambda.min\") print(pred) #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, groupstest = groupstest,   #>     s = \"lambda.min\")  #>  #>  #> alpha =  0.5  #>  #> Support size: #>                                           #> Overall    44                             #> Pretrain   89 (12 common + 77 individual) #> Individual 105"},{"path":"https://erincr.github.io/ptLasso/reference/print.ptLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"Print the ptLasso object. — print.ptLasso","title":"Print the ptLasso object. — print.ptLasso","text":"Print ptLasso object.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/print.ptLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print the ptLasso object. — print.ptLasso","text":"","code":"# S3 method for class 'ptLasso' print(x, ...)"},{"path":"https://erincr.github.io/ptLasso/reference/print.ptLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print the ptLasso object. — print.ptLasso","text":"x fitted \"ptLasso\" object. ... arguments pass print function.","code":""},{"path":[]},{"path":"https://erincr.github.io/ptLasso/reference/print.ptLasso.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Print the ptLasso object. — print.ptLasso","text":"Erin Craig Rob Tibshirani Maintainer: Erin Craig <erincr@stanford.edu>","code":""},{"path":"https://erincr.github.io/ptLasso/reference/print.ptLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print the ptLasso object. — print.ptLasso","text":"","code":"out = gaussian.example.data() x = out$x; y=out$y; groups = out$group;  fit = ptLasso(x, y, groups = groups, family = \"gaussian\", type.measure = \"mse\") print(fit) #>  #> Call:  ptLasso(x = x, y = y, groups = groups, family = \"gaussian\", type.measure = \"mse\",      use.case = \"inputGroups\", group.intercepts = TRUE)  #>"},{"path":"https://erincr.github.io/ptLasso/reference/ptLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit a pretrained lasso model using glmnet. — ptLasso","title":"Fit a pretrained lasso model using glmnet. — ptLasso","text":"Fits pretrained lasso model using glmnet package, fixed choice pretraining hyperparameter alpha. Additionally fits \"overall\" model (using data) \"individual\" models (use individual group). Can fit input-grouped data Gaussian, multinomial, binomial Cox outcomes, target-grouped data, necessarily multinomial outcome. Many ptLasso arguments passed directly glmnet, therefore glmnet documentation another good reference ptLasso.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/ptLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit a pretrained lasso model using glmnet. — ptLasso","text":"","code":"ptLasso(   x,   y,   groups = NULL,   alpha = 0.5,   family = c(\"default\", \"gaussian\", \"multinomial\", \"binomial\", \"cox\"),   type.measure = c(\"default\", \"mse\", \"mae\", \"auc\", \"deviance\", \"class\", \"C\"),   use.case = c(\"inputGroups\", \"targetGroups\", \"multiresponse\", \"timeSeries\"),   overall.lambda = c(\"lambda.1se\", \"lambda.min\"),   overall.gamma = \"gamma.1se\",   foldid = NULL,   nfolds = 10,   standardize = TRUE,   verbose = FALSE,   weights = NULL,   penalty.factor = rep(1, p),   fitoverall = NULL,   fitind = NULL,   en.alpha = 1,   group.intercepts = TRUE,   ... )"},{"path":"https://erincr.github.io/ptLasso/reference/ptLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit a pretrained lasso model using glmnet. — ptLasso","text":"x input matrix, dimension nobs x nvars; row observation vector. Can sparse matrix format (inherit class '\"sparseMatrix\"' package 'Matrix'). Requirement: 'nvars >1'; words, 'x' 2 columns. 'use.case = \"timeSeries\"', x may list matrices identical dimensions, one point time. y response variable. Quantitative 'family=\"gaussian\"'. 'family=\"binomial\"' either factor two levels, two-column matrix counts proportions (second column treated target class; factor, last level alphabetical order target class). 'family=\"multinomial\"', can 'nc>=2' level factor, matrix 'nc' columns counts proportions. either '\"binomial\"' '\"multinomial\"', 'y' presented vector, coerced factor. 'family=\"cox\"', preferably 'Surv' object survival package: see Detail section information. 'use.case = \"multiresponse\"' 'use.case = \"timeSeries\"', 'y' matrix responses. groups vector length nobs indicating group observation belongs. 'use.case = \"inputGroups\"'. alpha pretrained lasso hyperparameter, \\(0\\le\\alpha\\le 1\\). range alpha 0 (fits overall model fine tuning) 1 (individual models). default value 0.5, chosen mostly random. choose appropriate value data, please either run ptLasso choices alpha evaluate validation set, use cv.ptLasso, recommends value alpha using cross validation. family Either character string representing one built-families, else 'glm()' family object. information, see Details section documentation response type (see ). type.measure loss use cross-validation within individual, overall, pretrained lasso model. Currently five options, available models. default 'type.measure=\"deviance\"', uses squared-error gaussian models (.k.'type.measure=\"mse\"' ), deviance logistic poisson regression, partial-likelihood Cox model. 'type.measure=\"class\"' applies binomial multinomial logistic regression , gives misclassification error. 'type.measure=\"auc\"' two-class logistic regression , gives area ROC curve. 'type.measure=\"mse\"' 'type.measure=\"mae\"' (mean absolute error) can used models except '\"cox\"'; measure deviation fitted mean response. 'type.measure=\"C\"' Harrel's concordance measure, available 'cox' models. use.case type grouping observed data. Can one \"inputGroups\", \"targetGroups\", \"multiresponse\" \"timeSeries\". overall.lambda choice lambda used overall model define offset penalty factor pretrained lasso. Defaults \"lambda.1se\", alternatively \"lambda.min\". choice lambda used compute offset penalty factor (1) model training (2) prediction. predict function, another lambda must specified individual models, second stage pretraining overall model. overall.gamma use option relax = TRUE specified. choice gamma used overall model define offset penalty factor pretrained lasso. Defaults \"gamma.1se\", \"gamma.min\" also good option. choice gamma used compute offset penalty factor (1) model training (2) prediction. predict function, another gamma must specified individual models, second stage pretraining overall model. foldid optional vector values 1 nfolds identifying fold observation . supplied, nfold can missing. nfolds Number folds CV (default 10). Although nfolds can large sample size (leave-one-CV), recommended large datasets. Smallest value allowable nfolds = 3. standardize predictors standardized fitting (default TRUE). verbose verbose=TRUE, print statement showing model currently fit cv.glmnet. weights observation weights. Default 1 observation. penalty.factor Separate penalty factors can applied coefficient. number multiplies 'lambda' allow differential shrinkage. Can 0 variables,  implies shrinkage, variable always included model. Default 1 variables (implicitly infinity variables listed 'exclude'). information, see ?glmnet. pretraining, user-supplied penalty.factor multiplied computed overall model. fitoverall optional cv.glmnet object specifying overall model. trained full training data, argument 'keep = TRUE'. fitind optional list cv.glmnet objects specifying individual models. trained original training data, argument 'keep = TRUE'. en.alpha elasticnet mixing parameter, 0 <= en.alpha <= 1. penalty defined (1-alpha)/2||beta||_2^2+alpha||beta||_1. 'alpha=1' lasso penalty, 'alpha=0' ridge penalty. Default `en.alpha = 1` (lasso). group.intercepts 'use.case = \"inputGroups\"' . `TRUE`, fit overall model separate intercept group. `FALSE`, ignore grouping fit one overall intercept. Default `TRUE`. ... Additional arguments passed cv.glmnet functions. Notable choices include \"trace.\" \"parallel\". trace.= TRUE, progress bar displayed call \"cv.glmnet\"; useful big models take long time fit. parallel = TRUE, use parallel foreach fit fold.  Must register parallel hand, doMC others. ptLasso support arguments intercept, offset, fit check.args.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/ptLasso.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit a pretrained lasso model using glmnet. — ptLasso","text":"object class \"ptLasso\", list ingredients fitted models. call call produced object. k number groups ('use.case = \"inputGroups\"'). nresps number responses ('use.case = \"multiresponse\"' 'use.case = \"timeseries\"'). alpha value alpha used pretraining. group.levels IDs groups used training ('use.case = \"inputGroups\"'). group.legend Mapping user-supplied group ids numeric group ids. internal use (e.g. predict). ('use.case = \"inputGroups\"') fitoverall fitted cv.glmnet object trained using full data. available 'use.case = \"timeseries\"'. fitpre list fitted (pretrained) cv.glmnet objects, one trained data group response. fitind list fitted cv.glmnet objects, one trained group response. fitoverall.lambda Lambda used fitoverall, compute offset pretraining. fitoverall.gamma Gamma used fitoverall 'relax = TRUE', compute offset pretraining.","code":""},{"path":"https://erincr.github.io/ptLasso/reference/ptLasso.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Fit a pretrained lasso model using glmnet. — ptLasso","text":"Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization paths generalized linear models via coordinate descent. Journal Statistical Software, 33(1), 1-22.","code":""},{"path":[]},{"path":"https://erincr.github.io/ptLasso/reference/ptLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit a pretrained lasso model using glmnet. — ptLasso","text":"","code":"# Getting started. First, we simulate data: we need covariates x, response y and group IDs. set.seed(1234) x = matrix(rnorm(1000*20), 1000, 20) y = rnorm(1000) groups = sort(rep(1:5, 200))  xtest = matrix(rnorm(1000*20), 1000, 20) ytest = rnorm(1000) groupstest = sort(rep(1:5, 200))  # Now, we can fit a ptLasso model: fit = ptLasso(x, y, groups = groups, alpha = 0.5, family = \"gaussian\", type.measure = \"mse\") plot(fit) # to see all of the cv.glmnet models trained  predict(fit, xtest, groupstest) # to predict on new data #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, groupstest = groupstest)  #>  #>  #>  #> alpha =  0.5  #>  #> Support size: #>                                        #> Overall    3                           #> Pretrain   9 (0 common + 9 individual) #> Individual 9                           predict(fit, xtest, groupstest, ytest=ytest) # if ytest is included, we also measure performance #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest)  #>  #>  #> alpha =  0.5  #>  #> Performance (Mean squared error): #>  #>            allGroups  mean group_1 group_2 group_3 group_4 group_5       r^2 #> Overall        1.141 1.141   1.098   1.136   1.313   1.043   1.115 -0.003342 #> Pretrain       1.149 1.149   1.100   1.182   1.308   1.041   1.116 -0.010747 #> Individual     1.149 1.149   1.100   1.182   1.308   1.041   1.116 -0.010818 #>  #> Support size: #>                                        #> Overall    3                           #> Pretrain   9 (0 common + 9 individual) #> Individual 9                            # When we trained our model, we used \"lambda.1se\" in the first stage of pretraining by default. # This is a necessary choice to make during model training; we need to select the model # we want to use to define the offset and penalty factor for the second stage of pretraining. # We could instead have used \"lambda.min\": fit = ptLasso(x, y, groups = groups, alpha = 0.5, family = \"gaussian\", type.measure = \"mse\",               overall.lambda = \"lambda.min\")  # We can use the 'relax' option to fit relaxed lasso models: fit = ptLasso(x, y, groups = groups, alpha = 0.5,               family = \"gaussian\", type.measure = \"mse\",               relax = TRUE)  # As we did for lambda, we may want to specify the choice of gamma for stage one # of pretraining. (The default is \"gamma.1se\".) fit = ptLasso(x, y, groups = groups, alpha = 0.5,               family = \"gaussian\", type.measure = \"mse\",               relax = TRUE, overall.gamma = \"gamma.min\")  # In practice, we may want to try many values of alpha. # alpha may range from 0 (the overall model with fine tuning) to 1 (the individual models). # To choose alpha, you may either (1) run ptLasso with different values of alpha # and measure performance with a validation set, or (2) use cv.ptLasso.   # Now, we are ready to simulate slightly more realistic data. # This continuous outcome example has k = 5 groups, where each group has 200 observations. # There are scommon = 10 features shared across all groups, and # sindiv = 10 features unique to each group. # n = 1000 and p = 120 (60 informative features and 60 noise features). # The coefficients of the common features differ across groups (beta.common). # In group 1, these coefficients are rep(1, 10); in group 2 they are rep(2, 10), etc. # Each group has 10 unique features, the coefficients of which are all 3 (beta.indiv). # The intercept in all groups is 0. # The variable sigma = 20 indicates that we add noise to y according to 20 * rnorm(n).  set.seed(1234) k=5 class.sizes=rep(200, k) scommon=10; sindiv=rep(10, k) n=sum(class.sizes); p=2*(sum(sindiv) + scommon) beta.common=3*(1:k); beta.indiv=rep(3, k) intercepts=rep(0, k) sigma=20 out = gaussian.example.data(k=k, class.sizes=class.sizes,                             scommon=scommon, sindiv=sindiv,                             n=n, p=p,                             beta.common=beta.common, beta.indiv=beta.indiv,                             intercepts=intercepts, sigma=20) x = out$x; y=out$y; groups = out$group  outtest = gaussian.example.data(k=k, class.sizes=class.sizes,                                 scommon=scommon, sindiv=sindiv,                                 n=n, p=p,                                 beta.common=beta.common, beta.indiv=beta.indiv,                                 intercepts=intercepts, sigma=20) xtest=outtest$x; ytest=outtest$y; groupstest=outtest$groups  fit = ptLasso(x, y, groups = groups, alpha = 0.5, family = \"gaussian\", type.measure = \"mse\") plot(fit) # to see all of the cv.glmnet models trained  predict(fit, xtest, groupstest, ytest=ytest) #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest)  #>  #>  #> alpha =  0.5  #>  #> Performance (Mean squared error): #>  #>            allGroups  mean group_1 group_2 group_3 group_4 group_5    r^2 #> Overall        755.7 755.7   836.0   554.9   565.4   777.9  1044.0 0.5371 #> Pretrain       503.2 503.2   550.6   443.3   553.5   505.6   462.9 0.6918 #> Individual     532.8 532.8   584.1   443.2   567.2   550.5   518.9 0.6736 #>  #> Support size: #>                                           #> Overall    64                             #> Pretrain   94 (21 common + 73 individual) #> Individual 109                             # Now, we repeat with a binomial outcome. # This example has k = 3 groups, where each group has 100 observations. # There are scommon = 5 features shared across all groups, and # sindiv = 5 features unique to each group. # n = 300 and p = 40 (20 informative features and 20 noise features). # The coefficients of the common features differ across groups (beta.common), # as do the coefficients specific to each group (beta.indiv). set.seed(1234) k=3 class.sizes=rep(100, k) scommon=5; sindiv=rep(5, k) n=sum(class.sizes); p=2*(sum(sindiv) + scommon) beta.common=list(c(-.5, .5, .3, -.9, .1), c(-.3, .9, .1, -.1, .2), c(0.1, .2, -.1, .2, .3)) beta.indiv = lapply(1:k, function(i)  0.9 * beta.common[[i]])  out = binomial.example.data(k=k, class.sizes=class.sizes,                             scommon=scommon, sindiv=sindiv,                             n=n, p=p,                             beta.common=beta.common, beta.indiv=beta.indiv) x = out$x; y=out$y; groups = out$group  outtest = binomial.example.data(k=k, class.sizes=class.sizes,                                 scommon=scommon, sindiv=sindiv,                                 n=n, p=p,                                 beta.common=beta.common, beta.indiv=beta.indiv) xtest=outtest$x; ytest=outtest$y; groupstest=outtest$groups  fit = ptLasso(x, y, groups = groups, alpha = 0.5, family = \"binomial\", type.measure = \"auc\") plot(fit) # to see all of the cv.glmnet models trained  predict(fit, xtest, groupstest, ytest=ytest) #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, groupstest = groupstest,   #>     ytest = ytest)  #>  #>  #> alpha =  0.5  #>  #> Performance (AUC): #>  #>            allGroups   mean wtdMean group_1 group_2 group_3 #> Overall       0.5990 0.5963  0.5963  0.6006  0.6772  0.5112 #> Pretrain      0.6442 0.6631  0.6631  0.6965  0.7752  0.5177 #> Individual    0.6429 0.6580  0.6580  0.6948  0.7607  0.5186 #>  #> Support size: #>                                          #> Overall    8                             #> Pretrain   40 (3 common + 37 individual) #> Individual 40                             if (FALSE) { # \\dontrun{ ### Model fitting with parallel = TRUE require(doMC) registerDoMC(cores = 4) fit = ptLasso(x, y, groups = groups, family = \"gaussian\", type.measure = \"mse\", parallel=TRUE) } # }  # Multiresponse pretraining: # Now let's consider the case of a multiresponse outcome. We'll start by simulating data: set.seed(1234) n = 1000; ntrain = 500; p = 500 sigma = 2       x = matrix(rnorm(n*p), n, p) beta1 = c(rep(1, 5), rep(0.5, 5), rep(0, p - 10)) beta2 = c(rep(1, 5), rep(0, 5), rep(0.5, 5), rep(0, p - 15))  mu = cbind(x %*% beta1, x %*% beta2) y  = cbind(mu[, 1] + sigma * rnorm(n),             mu[, 2] + sigma * rnorm(n)) cat(\"SNR for the two tasks:\", round(diag(var(mu)/var(y-mu)), 2), fill=TRUE) #> SNR for the two tasks: 1.6 1.44 cat(\"Correlation between two tasks:\", cor(y[, 1], y[, 2]), fill=TRUE) #> Correlation between two tasks: 0.5164748  xtest = x[-(1:ntrain), ] ytest = y[-(1:ntrain), ]  x = x[1:ntrain, ] y = y[1:ntrain, ]  fit = ptLasso(x, y, type.measure = \"mse\", use.case = \"multiresponse\") plot(fit)  # to see all of the cv.glmnet models trained  predict(fit, xtest) # to predict with new data #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest)  #>  #>  #> alpha =  0.5  #>  #> Support size: #>                                           #> Overall    57                             #> Pretrain   29 (19 common + 10 individual) #> Individual 80                             predict(fit, xtest, ytest=ytest) # if ytest is included, we also measure performance #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, ytest = ytest)  #>  #>  #>  #> alpha =  0.5  #>  #> Performance (Mean squared error): #>  #>            allGroups  mean response_1 response_2 #> Overall        9.394 4.697      4.227      5.168 #> Pretrain       9.022 4.511      4.144      4.878 #> Individual     9.465 4.733      4.243      5.222 #>  #> Support size: #>                                           #> Overall    57                             #> Pretrain   29 (19 common + 10 individual) #> Individual 80                             # By default, we used lambda = \"lambda.min\" to measure performance. # We could instead use lambda = \"lambda.1se\": predict(fit, xtest, ytest=ytest, s=\"lambda.1se\") #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, ytest = ytest, s = \"lambda.1se\")  #>  #>  #>  #> alpha =  0.5  #>  #> Performance (Mean squared error): #>  #>            allGroups  mean response_1 response_2 #> Overall        9.879 4.940      4.387      5.492 #> Pretrain       9.481 4.740      4.210      5.271 #> Individual     9.965 4.983      4.341      5.624 #>  #> Support size: #>                                          #> Overall    19                            #> Pretrain   19 (19 common + 0 individual) #> Individual 27                             # We could also use the glmnet option relax = TRUE: fit = ptLasso(x, y, type.measure = \"mse\", relax = TRUE, use.case = \"multiresponse\")  # Time series pretraining # Now suppose we have time series data with a binomial outcome measured at 3 different time points. set.seed(1234) n = 600; ntrain = 300; p = 50 x = matrix(rnorm(n*p), n, p)  beta1 = c(rep(0.5, 10), rep(0, p-10)) beta2 = beta1 + c(rep(0, 10), runif(5, min = 0, max = 0.5), rep(0, p-15)) beta3 = beta1 + c(rep(0, 10), runif(5, min = 0, max = 0.5), rep(.5, 5), rep(0, p-20))  y1 = rbinom(n, 1, prob = 1/(1 + exp(-x %*% beta1))) y2 = rbinom(n, 1, prob = 1/(1 + exp(-x %*% beta2))) y3 = rbinom(n, 1, prob = 1/(1 + exp(-x %*% beta3))) y = cbind(y1, y2, y3)  xtest = x[-(1:ntrain), ] ytest = y[-(1:ntrain), ]  x = x[1:ntrain, ] y = y[1:ntrain, ]  fit =  ptLasso(x, y, use.case=\"timeSeries\", family=\"binomial\", type.measure = \"auc\") plot(fit)  predict(fit, xtest, ytest=ytest) #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, ytest = ytest)  #>  #>  #>  #> alpha =  0.5  #>  #> Performance (AUC): #>  #>              mean response_1 response_2 response_3 #> Pretrain   0.8185     0.8194     0.8060     0.8301 #> Individual 0.7997     0.8194     0.7691     0.8106 #>  #> Support size: #>                                           #> Pretrain   43 (26 common + 17 individual) #> Individual 47                              # The glmnet option relax = TRUE: fit = ptLasso(x, y, type.measure = \"auc\", family = \"binomial\", relax = TRUE,               use.case = \"timeSeries\") plot(fit)  predict(fit, xtest, ytest=ytest) #>  #> Call:   #> predict.ptLasso(object = fit, xtest = xtest, ytest = ytest)  #>  #>  #>  #> alpha =  0.5  #>  #> Performance (AUC): #>  #>              mean response_1 response_2 response_3 #> Pretrain   0.8154     0.8298     0.7956     0.8207 #> Individual 0.8018     0.8298     0.7647     0.8110 #>  #> Support size: #>                                           #> Pretrain   26 (12 common + 14 individual) #> Individual 29"}]
